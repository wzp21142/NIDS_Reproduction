{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import compat\n",
    "\n",
    "class DAE(object):\n",
    "\n",
    "\t\"\"\"\n",
    "\tDenoising autoencoder. Gaussian noise is added. The scale and standard deviation\n",
    "\tof it are noise_scale and noise_std, respectively.\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, n_feature, n_hidden, noise_scale, noise_std, reg_lamda = 0.01):\n",
    "\n",
    "\t\tself.n_hidden = n_hidden\n",
    "\t\tself.n_feature = n_feature\n",
    "\t\tself.reg_lamda = reg_lamda\n",
    "\t\tself.noise_scale = noise_scale\n",
    "\t\tself.noise_std = noise_std\n",
    "\n",
    "\t\tself.data = compat.v1.placeholder(shape = [None, n_feature],\n",
    "\t\t                           dtype = compat.v1.float64)\n",
    "\t\tself.noise = self.noise_scale * compat.v1.random_normal([n_feature], dtype = compat.v1.float64,\n",
    "\t\t                                               stddev = self.noise_std)\n",
    "\t\tdata_with_noise = self.data + self.noise\n",
    "\n",
    "\t\tself.weight_encoder = compat.v1.get_variable(name = 'weight_encoder',\n",
    "\t\t\t                    shape = [self.n_feature, self.n_hidden],\n",
    "\t\t\t                    dtype = compat.v1.float64)\n",
    "\t\tself.bias_encoder = compat.v1.Variable(compat.v1.zeros([self.n_hidden],\n",
    "\t\t\t                                      dtype = compat.v1.float64),\n",
    "\t\t                                name = 'bias_encoder')\n",
    "\n",
    "\t\tweight_decoder = compat.v1.get_variable(name = 'weight_decoder',\n",
    "\t\t\t                    shape = [self.n_hidden, self.n_feature],\n",
    "\t\t\t                    dtype = compat.v1.float64)\n",
    "\t\tbias_decoder = compat.v1.Variable(compat.v1.zeros([self.n_feature], dtype = compat.v1.float64),\n",
    "\t\t\t                           name = 'bias_decoder')\n",
    "\n",
    "\t\twith compat.v1.name_scope('Encoder'):\n",
    "\t\t\tdata_encoded = compat.v1.add(compat.v1.matmul(data_with_noise, self.weight_encoder),\n",
    "\t\t\t\t                  self.bias_encoder)\n",
    "\t\t\tdata_encoded = compat.v1.nn.tanh(data_encoded)\n",
    "\n",
    "\t\twith compat.v1.name_scope('Decoder'):\n",
    "\t\t\tdata_recons = compat.v1.add(compat.v1.matmul(data_encoded, weight_decoder),\n",
    "\t\t\t\t                 bias_decoder)\n",
    "\t\t\tself.data_recons = compat.v1.tanh(data_recons)\n",
    "\n",
    "\t\twith compat.v1.name_scope('Loss'):\n",
    "\t\t\tdiff = self.data_recons - self.data\n",
    "\t\t\tself.loss_mse = 0.5 * compat.v1.reduce_mean(compat.v1.reduce_sum(diff**2, axis = 1))\n",
    "\t\t\tloss_reg = compat.v1.reduce_sum(compat.v1.sqrt(compat.v1.reduce_sum(self.weight_encoder ** 2, axis = 1)))\n",
    "\t\t\tself.loss_reg = self.reg_lamda * loss_reg\n",
    "\t\t\tself.l2_loss = compat.v1.nn.l2_loss(weight_decoder) * 1E-3\n",
    "\n",
    "\t\t\tself.loss = self.loss_mse + self.loss_reg + self.l2_loss\n",
    "\n",
    "\t\twith compat.v1.name_scope('weight_vector'):\n",
    "\t\t\tself.weight_vector = compat.v1.reduce_sum(self.weight_encoder ** 2, axis = 1)\n",
    "\n",
    "\n",
    "class unbalanced_DAE(object):\n",
    "\n",
    "\t\"\"\"\n",
    "\tAn unbalanced version of DAE. the differences is that a weight pos_weight is added\n",
    "\tto the MSE reconstruction loss for positive examples. For this purpose, the labels\n",
    "\tof the examples are used.\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, n_feature, n_hidden, noise_scale, noise_std,\n",
    "\t             posi_weight = 1.0, reg_lamda = 0.00):\n",
    "\n",
    "\t\tself.n_hidden = n_hidden\n",
    "\t\tself.n_feature = n_feature\n",
    "\t\tself.reg_lamda = reg_lamda\n",
    "\t\tself.noise_scale = noise_scale\n",
    "\t\tself.noise_std = noise_std\n",
    "\t\tself.posi_weight = posi_weight\n",
    "\n",
    "\t\tself.data = compat.v1.placeholder(shape = [None, n_feature], dtype = compat.v1.float64)\n",
    "\t\tself.label = compat.v1.placeholder(shape = [None, 1], dtype = compat.v1.float64)\n",
    "\t\tself.noise = self.noise_scale * compat.v1.random_normal([n_feature], dtype = compat.v1.float64,\n",
    "\t\t                                                  stddev = self.noise_std)\n",
    "\t\tdata_with_noise = self.data + self.noise\n",
    "\n",
    "\t\tself.weight_encoder = compat.v1.get_variable(name = 'weight_encoder',\n",
    "\t\t\t                    shape = [self.n_feature, self.n_hidden],\n",
    "\t\t\t                    dtype = compat.v1.float64)\n",
    "\t\tself.bias_encoder = compat.v1.Variable(compat.v1.zeros([self.n_hidden],\n",
    "\t\t\t                                      dtype = compat.v1.float64),\n",
    "\t\t                                name = 'bias_encoder')\n",
    "\n",
    "\t\tweight_decoder = compat.v1.get_variable(name = 'weight_decoder',\n",
    "\t\t\t                    shape = [self.n_hidden, self.n_feature],\n",
    "\t\t\t                    dtype = compat.v1.float64)\n",
    "\t\tbias_decoder = compat.v1.Variable(compat.v1.zeros([self.n_feature], dtype = compat.v1.float64),\n",
    "\t\t\t                           name = 'bias_decoder')\n",
    "\n",
    "\t\twith compat.v1.name_scope('Encoder'):\n",
    "\t\t\tdata_encoded = compat.v1.add(compat.v1.matmul(data_with_noise, self.weight_encoder),\n",
    "\t\t\t\t                  self.bias_encoder)\n",
    "\t\t\tdata_encoded = compat.v1.nn.sigmoid(data_encoded)\n",
    "\n",
    "\t\twith compat.v1.name_scope('Decoder'):\n",
    "\t\t\tdata_recons = compat.v1.add(compat.v1.matmul(data_encoded, weight_decoder),\n",
    "\t\t\t\t                 bias_decoder)\n",
    "\t\t\tself.data_recons = compat.v1.nn.sigmoid(data_recons)\n",
    "\n",
    "\t\twith compat.v1.name_scope('Loss'):\n",
    "\t\t\tdiff = self.data_recons - self.data\n",
    "\t\t\tweights = self.label * (posi_weight -1) + 1\n",
    "\t\t\tweights = compat.v1.reshape(weights, shape = [-1])\n",
    "\t\t\tself.loss_mse = 0.5 * compat.v1.reduce_mean(compat.v1.reduce_sum(diff**2, axis = 1) * weights)\n",
    "\t\t\tloss_reg = compat.v1.reduce_sum(compat.v1.sqrt(compat.v1.reduce_sum(self.weight_encoder ** 2, axis = 1)))\n",
    "\t\t\tself.loss_reg = self.reg_lamda * loss_reg\n",
    "\t\t\tself.l2_loss = compat.v1.nn.l2_loss(weight_decoder) * 1E-3\n",
    "\n",
    "\t\t\tself.loss = self.loss_mse + self.loss_reg + self.l2_loss\n",
    "\n",
    "\t\twith compat.v1.name_scope('weight_vector'):\n",
    "\t\t\tself.weight_vector = compat.v1.reduce_sum(self.weight_encoder ** 2, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\nif __name__ == '__main__':\\n\\n    num_train = 125973\\n    num_test = 22543\\n    file_folder = 'normalized/'\\n\\n    num_classes = 2\\n    trans_dataset(file_folder + 'train+.tfrecords', file_folder + 'train2.txt', num_train, num_classes)\\n    trans_dataset(file_folder + 'test+.tfrecords', file_folder + 'test2.txt', num_test, num_classes)\\n\\n    num_classes = 5\\n    trans_dataset(file_folder + 'train5.tfrecords', file_folder + 'train5.txt', num_train, num_classes)\\n    trans_dataset(file_folder + 'test5.tfrecords', file_folder + 'test5.txt', num_test, num_classes)\\n\\n    file_folder_new = file_folder + 'cross_validation_5/'\\n\\n    file_train = file_folder + 'train5.txt'\\n    file_test = file_folder +'test5.txt'\\n    split_dataset(file_train, file_test, k =10, file_folder_new = file_folder_new)\\n\\n    dataset = np.loadtxt(file_folder +'train_new.txt')\\n    #dataset = [file_folder + str(x + 1) + '_train.csv' for x in range(4)]\\n    posi, neg = parse_pos_neg(dataset)\\n\\n    np.savetxt(file_folder + 'train_posi.txt',posi )\\n    np.savetxt(file_folder + 'train_neg.txt', neg)\\n    \""
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import compat\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def P_R_F1(confusion_matrix):\n",
    "\n",
    "    category = confusion_matrix.shape[0]\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    for i in range (category):\n",
    "        TP = confusion_matrix[i,i]\n",
    "\n",
    "        precsion_temp = TP/np.sum(confusion_matrix[:,i])\n",
    "        recall_temp = TP/np.sum(confusion_matrix[i,:])\n",
    "        f1_temp = 2*precsion_temp*recall_temp/(precsion_temp + recall_temp)\n",
    "\n",
    "        precision.append(precsion_temp)\n",
    "        recall.append(recall_temp)\n",
    "        f1.append(f1_temp)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "# shaping labels to one-hot vectors for trainning\n",
    "def label_coding(label, batch_size, category):\n",
    "    new_label = compat.v1.cast(label, dtype = compat.v1.int32)\n",
    "    new_label = compat.v1.reshape(new_label, [batch_size, 1])\n",
    "    new_label = compat.v1.one_hot(new_label, depth = category)\n",
    "    return compat.v1.reshape(new_label, [batch_size, category])\n",
    "\n",
    "# get next batch of data and label\n",
    "def next_batch(filename, batch_size, conf, buffer_size = 0):\n",
    "\n",
    "    len_feature = conf.len_feature\n",
    "    len_label = conf.len_label\n",
    "    num_classes = conf.num_classes\n",
    "    one_hot_encoding = conf.one_hot_encoding\n",
    "\n",
    "    def read_data(examples):\n",
    "        features = {\"features\": compat.v1.FixedLenFeature([len_feature], compat.v1.float32),\n",
    "                    \"label_2\": compat.v1.FixedLenFeature([len_label], compat.v1.float32),\n",
    "                    \"label_10\": compat.v1.FixedLenFeature([len_label], compat.v1.float32)}\n",
    "        parsed_features = compat.v1.parse_single_example(examples, features)\n",
    "        return parsed_features['features'], parsed_features['label_2'], \\\n",
    "               parsed_features['label_10']\n",
    "\n",
    "    data = compat.v1.data.TFRecordDataset(filename)\n",
    "    data = data.map(read_data)\n",
    "    if buffer_size != 0:\n",
    "        data = data.shuffle(buffer_size = buffer_size)\n",
    "    data = data.repeat()\n",
    "    data = data.batch(batch_size)\n",
    "    iterator = data.make_one_shot_iterator()\n",
    "    next_data, next_label_2, next_label_10 = iterator.get_next()\n",
    "\n",
    "    if one_hot_encoding == True:\n",
    "        if num_classes == 2:\n",
    "            next_label_2 = label_coding(next_label_2, batch_size,\n",
    "                                        num_classes)\n",
    "        else:\n",
    "            next_label_10 = label_coding(next_label_10, batch_size,\n",
    "                                         num_classes)\n",
    "\n",
    "    return next_data, next_label_2, next_label_10\n",
    "\n",
    "\n",
    "def trans_dataset(file_tfr, file_txt, num_examples, num_classes):\n",
    "    with compat.v1.Session() as sess:\n",
    "        all_data, all_label = next_batch(file_tfr, num_examples)\n",
    "        all_label = label_coding(all_label, num_examples, num_classes)\n",
    "\n",
    "        record = np.concatenate([sess.run(all_data), sess.run(all_label)], axis = 1)\n",
    "        np.savetxt(file_txt, record, fmt = '%.6e')\n",
    "\n",
    "def split_dataset(file_train, file_test, k, file_folder_new): # k is refer to k_fold\n",
    "\n",
    "    trainset = np.loadtxt(file_train)\n",
    "    testset = np.loadtxt(file_test)\n",
    "    dataset = np.concatenate((trainset, testset))\n",
    "\n",
    "    for i in range(k - 1):\n",
    "        trainset, testset = train_test_split(dataset, test_size = 1/(k - i))\n",
    "        dataset = trainset\n",
    "        np.savetxt(file_folder_new + str(i) + '.txt', testset)\n",
    "\n",
    "    np.savetxt(file_folder_new + str(k - 1) + '.txt', trainset)\n",
    "\n",
    "def get_dataset(file_folder, index_test, indices_train):\n",
    "\n",
    "    testset = np.loadtxt(file_folder + str(index_test) + '.txt')\n",
    "\n",
    "    count = 0\n",
    "    for other in indices_train:\n",
    "        temp = np.loadtxt(file_folder + str(other) + '.txt')\n",
    "\n",
    "        if count == 0:\n",
    "            trainset = temp\n",
    "        else:\n",
    "            trainset = np.concatenate((trainset, temp))\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    return trainset, testset\n",
    "\n",
    "def parse_pos_neg(dataset):\n",
    "\n",
    "    label = dataset[:, -1]\n",
    "\n",
    "    record_posi = []\n",
    "    record_neg = []\n",
    "\n",
    "    records_len = dataset.shape[-1]\n",
    "    records_num = dataset.shape[0]\n",
    "\n",
    "    for index in range(records_num):\n",
    "        record = dataset[index, :]\n",
    "        record = np.reshape(record, (1, records_len))\n",
    "        if label[index] == 0.:\n",
    "            record_posi.append(record)\n",
    "        else:\n",
    "            record_neg.append(record)\n",
    "\n",
    "    posi = np.concatenate(record_posi)\n",
    "    neg = np.concatenate(record_neg)\n",
    "\n",
    "    return posi, neg\n",
    "\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    num_train = 125973\n",
    "    num_test = 22543\n",
    "    file_folder = 'normalized/'\n",
    "\n",
    "    num_classes = 2\n",
    "    trans_dataset(file_folder + 'train+.tfrecords', file_folder + 'train2.txt', num_train, num_classes)\n",
    "    trans_dataset(file_folder + 'test+.tfrecords', file_folder + 'test2.txt', num_test, num_classes)\n",
    "\n",
    "    num_classes = 5\n",
    "    trans_dataset(file_folder + 'train5.tfrecords', file_folder + 'train5.txt', num_train, num_classes)\n",
    "    trans_dataset(file_folder + 'test5.tfrecords', file_folder + 'test5.txt', num_test, num_classes)\n",
    "\n",
    "    file_folder_new = file_folder + 'cross_validation_5/'\n",
    "\n",
    "    file_train = file_folder + 'train5.txt'\n",
    "    file_test = file_folder +'test5.txt'\n",
    "    split_dataset(file_train, file_test, k =10, file_folder_new = file_folder_new)\n",
    "\n",
    "    dataset = np.loadtxt(file_folder +'train_new.txt')\n",
    "    #dataset = [file_folder + str(x + 1) + '_train.csv' for x in range(4)]\n",
    "    posi, neg = parse_pos_neg(dataset)\n",
    "\n",
    "    np.savetxt(file_folder + 'train_posi.txt',posi )\n",
    "    np.savetxt(file_folder + 'train_neg.txt', neg)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-41-3c9b798bb3eb>:53: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through `tf.compat.v1`. In all other situations -- namely, eager mode and inside `tf.function` -- you can consume dataset elements using `for elem in dataset: ...` or by explicitly creating iterator via `iterator = iter(dataset)` and fetching its elements via `values = next(iterator)`. Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)` to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\n",
      "Epoch: 1, Loss on Train: 26.171487 mse on Train: 25.979220 reg on Train: 0.077158 Loss on Test: 22.507186 mse on Test: 22.232535 reg on Test: 0.090983\n",
      "75\n",
      "13\n",
      "Epoch: 2, Loss on Train: 21.624054 mse on Train: 21.286636 reg on Train: 0.099693 Loss on Test: 21.681653 mse on Test: 21.295011 reg on Test: 0.105837\n",
      "63\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tensorflow import compat\n",
    "import numpy as np\n",
    "#from autoencoders import DAE, unbalanced_DAE\n",
    "#from utils import next_batch, P_R_F1\n",
    "\n",
    "compat.v1.reset_default_graph()\n",
    "\n",
    "# system parameters\n",
    "\n",
    "class Configures(object):\n",
    "\n",
    "\tdef __init__(self):\n",
    "        # parameter of records\n",
    "\t\tself.len_feature = 202\n",
    "\t\tself.len_label = 1\n",
    "\t\t#self.num_classes = 2\n",
    "\t\tself.num_classes = 10\n",
    "\t\tself.one_hot_encoding = False\n",
    "\t\tself.num_records_train = 1625628\n",
    "\t\tself.num_records_test = 508012\n",
    "\n",
    "        # parameters for training\n",
    "\t\tself.batch_size = 256\n",
    "\t\tself.batch_size_test = 2048\n",
    "\t\tself.training_epochs = 2\n",
    "\t\tself.learn_rate_start = 1E-4\n",
    "\n",
    "\t\tself.batch_train = self.num_records_train//self.batch_size\n",
    "\t\tself.batch_test = self.num_records_test//self.batch_size_test\n",
    "\n",
    "n_hidden = 64\n",
    "noise_scale = 0.\n",
    "noise_std = 0.1\n",
    "conf = Configures()\n",
    "\n",
    "# training op\n",
    "with compat.v1.Session() as sess:\n",
    "\n",
    "\t#AE = DAE(conf.len_feature, n_hidden, noise_scale, noise_std, reg_lamda = 0.001)\n",
    "\tAE = unbalanced_DAE(conf.len_feature, n_hidden, noise_scale, noise_std, posi_weight = 3.5,\n",
    "\t\t                reg_lamda = 0.001)\n",
    "\n",
    "\tglobal_step = compat.v1.Variable(0, name = 'training_steps', trainable = False)\n",
    "\tlearn_rate = compat.v1.train.exponential_decay(conf.learn_rate_start, global_step, 2000, 0.96, staircase=True)\n",
    "\tupdate_ops = compat.v1.get_collection(compat.v1.GraphKeys.UPDATE_OPS)\n",
    "\twith compat.v1.control_dependencies(update_ops):\n",
    "\t\toptimizer = compat.v1.train.AdamOptimizer(conf.learn_rate_start)\n",
    "\t\tgrads_and_vars = optimizer.compute_gradients(AE.loss)\n",
    "\t\tgrads_clipped = [(compat.v1.clip_by_value(grad, -5., 5.), var) for grad, var in grads_and_vars]\n",
    "\t\ttrain_op = optimizer.apply_gradients(grads_clipped, global_step = global_step)\n",
    "\n",
    "\tsess.run(compat.v1.global_variables_initializer())\n",
    "\n",
    "\t# Reading data\n",
    "\tfile_train = ['../dataset/SGM-CNN/normalized/train.tfrecords', '../dataset/SGM-CNN/normalized/validation.tfrecords']\n",
    "\tfile_test = '../dataset/SGM-CNN/normalized/test.tfrecords'\n",
    "\ttrain_data, train_label, _ = next_batch(file_train, conf.batch_size, conf, 150)\n",
    "\ttest_data, test_label, _ = next_batch(file_test, conf.batch_size_test, conf)\n",
    "\n",
    "\tmin_loss = 100.\n",
    "\tfor epoch in range(conf.training_epochs):\n",
    "\t\ttime_start = time.time()\n",
    "\t\ttotal_mse_loss = 0.\n",
    "\t\ttotal_loss = 0.\n",
    "\t\ttotal_reg_loss = 0.\n",
    "\n",
    "\t\tfor step in range(conf.batch_train):\n",
    "\t\t\tdata, label = sess.run([train_data, train_label])\n",
    "\n",
    "\t\t\tfeed_dict = {AE.data: data, AE.label: label}\n",
    "\t\t\t_, loss, loss_mse, loss_reg = sess.run([train_op, AE.loss, AE.loss_mse, AE.loss_reg],\n",
    "\t\t\t                                        feed_dict = feed_dict)\n",
    "\n",
    "\t\t\ttotal_loss += loss\n",
    "\t\t\ttotal_mse_loss += loss_mse\n",
    "\t\t\ttotal_reg_loss += loss_reg\n",
    "\n",
    "\t\ttime_train_end = time.time()\n",
    "\t\ttest_loss = 0.\n",
    "\t\ttest_loss_reg = 0.\n",
    "\t\ttest_loss_mse = 0.\n",
    "\n",
    "\t\tfor step in range(conf.batch_test):\n",
    "\t\t\tdata, label = sess.run([test_data, test_label])\n",
    "\n",
    "\t\t\tfeed_dict = {AE.data: data, AE.label: label}\n",
    "\t\t\tweights, loss, loss_mse, loss_reg = sess.run([AE.weight_vector, AE.loss, AE.loss_mse, AE.loss_reg],\n",
    "\t\t\t                                              feed_dict = feed_dict)\n",
    "\t\t\ttest_loss += loss/conf.batch_test\n",
    "\t\t\ttest_loss_mse += loss_mse/conf.batch_test\n",
    "\t\t\ttest_loss_reg += loss_reg/conf.batch_test\n",
    "\n",
    "\t\ttime_test_end = time.time()\n",
    "\n",
    "\t\ttime_duration_train = int(time_train_end - time_start)\n",
    "\t\ttime_duration_test = int(time_test_end - time_train_end)\n",
    "\n",
    "\n",
    "\t\tif test_loss < min_loss:\n",
    "\t\t\tmin_loss = test_loss\n",
    "\t\t\tnp.savetxt('../dataset/SGM-CNN/normalized/weights_new_3.5.txt', weights, fmt = \"%.6E\")\n",
    "\n",
    "\t\tprint(\"Epoch:\", \"%d,\" % (epoch + 1),\n",
    "\t\t\t  \"Loss on Train:\", \"{:.6f}\".format(total_loss/(conf.batch_train)),\n",
    "\t\t\t  \"mse on Train:\", \"{:.6f}\".format(total_mse_loss/conf.batch_train),\n",
    "\t\t\t  \"reg on Train:\", \"{:.6f}\".format(total_reg_loss/conf.batch_train),\n",
    "\t\t\t  \"Loss on Test:\", \"{:.6f}\".format(test_loss),\n",
    "\t\t\t  \"mse on Test:\", \"{:.6f}\".format(test_loss_mse),\n",
    "\t\t\t  \"reg on Test:\", \"{:.6f}\".format(test_loss_reg))\n",
    "\n",
    "\t\tprint(time_duration_train)\n",
    "\t\tprint(time_duration_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 14, 173, 8, 175, 7, 16, 15, 3, 17, 177, 30]\n",
      "stcpb\n",
      "dtcpb\n",
      "service_-\n",
      "dload\n",
      "service_dns\n",
      "sload\n",
      "dmeansz\n",
      "smeansz\n",
      "sttl\n",
      "trans_depth\n",
      "service_ftp-data\n",
      "ct_ftp\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xd5 in position 92: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-45-13b91d07cde5>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    144\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    145\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 146\u001B[1;33m         \u001B[0mselect_feature\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfile_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_examples_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindices\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    147\u001B[0m         \u001B[0mselect_feature\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfile_valid\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_examples_validation\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindices\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    148\u001B[0m         \u001B[0mselect_feature\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfile_test\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_examples_test\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindices\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-45-13b91d07cde5>\u001B[0m in \u001B[0;36mselect_feature\u001B[1;34m(file, num_examples, indices)\u001B[0m\n\u001B[0;32m     88\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     89\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mcompat\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mv1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSession\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0msess\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 90\u001B[1;33m                 \u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabel_2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabel_10\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msess\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnext_batch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfile\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_examples\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     91\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     92\u001B[0m         \u001B[0mdata_select\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mselection\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindices\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36mrun\u001B[1;34m(self, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[0;32m    955\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    956\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 957\u001B[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001B[0m\u001B[0;32m    958\u001B[0m                          run_metadata_ptr)\n\u001B[0;32m    959\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[0mrun_metadata\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_run\u001B[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[0;32m   1178\u001B[0m     \u001B[1;31m# or if the call is a partial run that specifies feeds.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1179\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mfinal_fetches\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mfinal_targets\u001B[0m \u001B[1;32mor\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mhandle\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mfeed_dict_tensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1180\u001B[1;33m       results = self._do_run(handle, final_targets, final_fetches,\n\u001B[0m\u001B[0;32m   1181\u001B[0m                              feed_dict_tensor, options, run_metadata)\n\u001B[0;32m   1182\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_do_run\u001B[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001B[0m\n\u001B[0;32m   1356\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1357\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mhandle\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1358\u001B[1;33m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001B[0m\u001B[0;32m   1359\u001B[0m                            run_metadata)\n\u001B[0;32m   1360\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_do_call\u001B[1;34m(self, fn, *args)\u001B[0m\n\u001B[0;32m   1363\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_do_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1364\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1365\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1366\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0merrors\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mOpError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1367\u001B[0m       \u001B[0mmessage\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcompat\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mas_text\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmessage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_run_fn\u001B[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001B[0m\n\u001B[0;32m   1347\u001B[0m       \u001B[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1348\u001B[0m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_extend_graph\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1349\u001B[1;33m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001B[0m\u001B[0;32m   1350\u001B[0m                                       target_list, run_metadata)\n\u001B[0;32m   1351\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_call_tf_sessionrun\u001B[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001B[0m\n\u001B[0;32m   1439\u001B[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001B[0;32m   1440\u001B[0m                           run_metadata):\n\u001B[1;32m-> 1441\u001B[1;33m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001B[0m\u001B[0;32m   1442\u001B[0m                                             \u001B[0mfetch_list\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget_list\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1443\u001B[0m                                             run_metadata)\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m: 'utf-8' codec can't decode byte 0xd5 in position 92: invalid continuation byte"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import heapq\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow import compat\n",
    "from pandas import read_csv\n",
    "\n",
    "def get_indices(num_select, num_feature, file_weights):\n",
    "\t\"\"\"\n",
    "\tThis function is to select maximum k features according to their\n",
    "\tweights.\n",
    "\n",
    "\tPram:\n",
    "\t\tnum_select: An interger, the number of the selected features\n",
    "\t\tnum_feature: An interger, the number of the original features\n",
    "\t\tfile_weights: A txt file storing a numpy array. Each row of the\n",
    "\t\t              array is the weight for a feature\n",
    "\tReturn:\n",
    "\t\ta list containing the indices of selected features\n",
    "\t\"\"\"\n",
    "\n",
    "\tx = np.arange(1, num_feature + 1)\n",
    "\ty = np.loadtxt(file_weights)\n",
    "\tindices = heapq.nlargest(num_select, range(len(y)), y.take)\n",
    "\tplt.scatter(x, y)\n",
    "\tplt.show()\n",
    "\tplt.savefig('weights_dis.eps', format = 'eps')\n",
    "\tprint(indices)\n",
    "\n",
    "\treturn indices\n",
    "\n",
    "\n",
    "def read_data(examples):\n",
    "    features = {\"features\": compat.v1.FixedLenFeature([num_feature], compat.v1.float32),\n",
    "                \"label_2\": compat.v1.FixedLenFeature([len_label], compat.v1.float32),\n",
    "                \"label_10\": compat.v1.FixedLenFeature([len_label], compat.v1.float32)}\n",
    "    parsed_features = compat.v1.parse_single_example(examples, features)\n",
    "    return parsed_features['features'], parsed_features['label_2'], \\\n",
    "           parsed_features['label_10']\n",
    "\n",
    "\n",
    "# get next batch of data and label\n",
    "def next_batch(filename, num_examples):\n",
    "\n",
    "    data = compat.v1.data.TFRecordDataset(filename)\n",
    "    data = data.map(read_data)\n",
    "    data = data.batch(num_examples)\n",
    "    iterator = data.make_one_shot_iterator()\n",
    "    next_data, next_label_2, next_label_10 = iterator.get_next()\n",
    "    return next_data, next_label_2, next_label_10\n",
    "\n",
    "\n",
    "def make_tfrecords(dataset, file_to_save):\n",
    "\t[features, label_2, label_10] = dataset\n",
    "\n",
    "\twith compat.v1.python_io.TFRecordWriter(file_to_save) as writer:\n",
    "\t\tfor index in range(features.shape[0]):\n",
    "\t\t\tfeature = {'features': compat.v1.train.Feature(float_list = compat.v1.train.FloatList(value = features[index, :])),\n",
    "\t\t\t           'label_2': compat.v1.train.Feature(float_list = compat.v1.train.FloatList(value = label_2[index, :])),\n",
    "\t\t\t           'label_10': compat.v1.train.Feature(float_list = compat.v1.train.FloatList(value = label_10[index, :]))}\n",
    "\t\t\texample = compat.v1.train.Example(features = compat.v1.train.Features(feature = feature))\n",
    "\t\t\twriter.write(example.SerializeToString())\n",
    "\n",
    "\n",
    "def selection(data, indices):\n",
    "\t\"\"\"\n",
    "\tselect the columns (indicating the features) according to the indices\n",
    "\t\"\"\"\n",
    "\n",
    "\treturn data[:, indices]\n",
    "\n",
    "\n",
    "def select_feature(file, num_examples, indices):\n",
    "\t\"\"\"\n",
    "\tThe main function of feature selection.\n",
    "\n",
    "\tParams:\n",
    "\t  file: The .tfrecords file containing original data.包含原始数据的.tfrecords文件\n",
    "\t  num_examples: The number of examples in the file  文件中的记录数\n",
    "\t  indices: The indices of features to be selected  被选择特征的索引\n",
    "\n",
    "\tReturn:\n",
    "\t  None\n",
    "\t  In the function, a new .tfrecords file with tail of 'selected'\n",
    "\t  will be created in the same folder with the original data\n",
    "\t\"\"\"\n",
    "\n",
    "\twith compat.v1.Session() as sess:\n",
    "\t\tdata, label_2, label_10 = sess.run(next_batch(file, num_examples))\n",
    "\n",
    "\tdata_select = selection(data, indices)\n",
    "\n",
    "\tfile_name = file.split('\\\\')[-1] \n",
    "\tfile_tail = len('.tfrecords')\n",
    "\tfile_to_save = file_name[:-1*file_tail] + '_select_' + str(len(indices)) + '.tfrecords'\n",
    "\t\n",
    "\n",
    "\tmake_tfrecords([data_select, label_2, label_10], file_to_save)\n",
    "\n",
    "\n",
    "def show_feature_name(indices): \n",
    "\t\"\"\" \n",
    "\tThe function to convert indices to feature names\n",
    "\n",
    "\tParams:\n",
    "\t  indices:the indices of the features\n",
    "\tReturn:\n",
    "\t  None. \n",
    "\t  The name of features will be print\n",
    "\t\"\"\"\n",
    "\n",
    "\t#dirname = os.path.dirname(os.getcwd())\n",
    "\t#file = os.path.join(dirname, 'normalized/', '1_test.csv')\n",
    "\tfile = os.path.join( 'normalized/', '1_test.csv')\n",
    "\tdata = read_csv(file, index_col = 0)\n",
    "\tcols = data.columns\n",
    "\n",
    "\tfor x in indices:\n",
    "\t\tprint(cols[x])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\tnum_select = 12   #选择出来的特征数。The number of selected features\n",
    "\tnum_feature = 202\n",
    "\tlen_label = 1\n",
    "\tfile_weights = '../dataset/SGM-CNN/normalized/weights_new_3.5.txt'\n",
    "\n",
    "\tindices = get_indices(num_select, num_feature, file_weights)\n",
    "\t#print(indices)\n",
    "\tshow_feature_name(indices)\n",
    "\n",
    "\tdirname = os.path.dirname(os.getcwd())\n",
    "\tfile_folder = os.path.join(dirname, 'normalized/')\n",
    "\t\n",
    "\tfile_train = file_folder + 'train.tfrecords'\n",
    "\tfile_valid = file_folder + 'validation.tfrecords'\n",
    "\tnum_examples_train = 1778030\n",
    "\tnum_examples_validation = 254005\n",
    "\n",
    "\tfile_test = file_folder + 'test.tfrecords'\n",
    "\tnum_examples_test = 508012\n",
    "\n",
    "\t\n",
    "\tselect_feature(file_train, num_examples_train, indices)\n",
    "\tselect_feature(file_valid, num_examples_validation, indices)\n",
    "\tselect_feature(file_test, num_examples_test, indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 13, 173, 8, 16, 175, 15, 7, 17, 3, 177, 30]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[14, 13, 173, 8, 16, 175, 15, 7, 17, 3, 177, 30]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_indices(num_select, num_feature, file_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-8bba76b",
   "language": "python",
   "display_name": "PyCharm (SGM-CNN)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}