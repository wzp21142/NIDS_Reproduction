{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "# Mark the nominal columns and consolidate the data (extract all the nominal columns)\n",
    "# 标出名词性列，整合数据（把名词性的列都提取出来）\n",
    "def combine_dataset(files, col_names, processed=False):\n",
    "    dtypes = {}\n",
    "    if processed == False:\n",
    "        for col_name in col_names:\n",
    "            nominal_names = set(['srcip', 'sport', 'dstip', 'dsport', 'proto', 'state',\n",
    "                                 'service', 'ct_ftp', 'label_10'])  # Nominal column\n",
    "            if col_name in nominal_names:\n",
    "                dtypes[col_name] = str\n",
    "            else:\n",
    "                dtypes[col_name] = np.float32\n",
    "    else:\n",
    "        for col_name in col_names:\n",
    "            dtypes[col_name] = np.float32\n",
    "\n",
    "    records = []\n",
    "    for file in files:\n",
    "        data = pd.read_csv(file, header=None, names=col_names, dtype=dtypes)\n",
    "        records.append(data)\n",
    "\n",
    "    records_all = pd.concat(records)  # 当没有索引时、concat不管列名，直接加到一起\n",
    "    # When there is no index, concat adds them together regardless of the column names,\n",
    "\n",
    "    return records_all\n",
    "\n",
    "\n",
    "## Make new col names for categorical features after one-hot encoding\n",
    "# 为one-hot编码之后的列起个新列名\n",
    "def get_nominal_names(dataset, cols_nominal):\n",
    "    data_nominal = dataset[cols_nominal]\n",
    "\n",
    "    new_col_names = []\n",
    "    for col_name in cols_nominal:\n",
    "        name_unique = sorted(dataset[col_name].unique())  # 名词性列的不同的值。Different values for noun columns\n",
    "        new_col_name = [col_name + '_' + x for x in name_unique]\n",
    "        new_col_names.extend(new_col_name)\n",
    "\n",
    "    return new_col_names\n",
    "\n",
    "\n",
    "# Remove the unimportant feature, one-hot encoding, and convert the attack class to numeric\n",
    "# 删除不重要的特征，one-hot编码，将攻击类别转换为数值型\n",
    "def select_feature_and_encoding(dataset, cols_to_drop, cols_nominal, cols_nominal_all):\n",
    "    # Drop the features has no meaning such as src ip. 删除不重要的特征\n",
    "    for cols in cols_to_drop:\n",
    "        dataset.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "    # Save the label and then drop it from dataset 保留标签然后将它从数据集中删除（提取出标签列）\n",
    "    label_10 = dataset['label_10']\n",
    "    label_2 = dataset['label_2']\n",
    "    dataset.drop('label_2', axis=1, inplace=True)\n",
    "    dataset.drop('label_10', axis=1, inplace=True)\n",
    "\n",
    "    # replace the label with specific code  将标签数值化\n",
    "    replace_dict = {np.nan: 0, 'Analysis': 1, 'Backdoors': 2, 'Backdoor': 2, 'DoS': 3,\n",
    "                    'Exploits': 4, ' Fuzzers': 5, ' Fuzzers ': 5, 'Generic': 6,\n",
    "                    'Reconnaissance': 7, ' Shellcode ': 8, 'Shellcode': 8,\n",
    "                    'Worms': 9, ' Reconnaissance ': 7, }\n",
    "    new_label_10 = label_10.replace(replace_dict)\n",
    "    new_label_10.to_frame()\n",
    "    label_2.to_frame\n",
    "    del label_10\n",
    "\n",
    "    # replace the lost values  用0替换缺失值\n",
    "    replace_dict = {np.nan: 0, ' ': 0}\n",
    "    for cols in ['ct_ftp', 'ct_flw', 'is_ftp']:\n",
    "        dataset[cols] = dataset[cols].replace(replace_dict)\n",
    "\n",
    "    # 'is_ftp' column is wrong, correct it(I found that the value of it is\n",
    "    # all the same with ct_ftp_cmd, so if the value is not 0, is_ftp should\n",
    "    # be 1)\n",
    "    for x in dataset['is_ftp']:\n",
    "        if x != 0:\n",
    "            x = 1\n",
    "\n",
    "    # select and process the categorical features 选择并处理分类特征\n",
    "    data_nominal = dataset[cols_nominal]  # cols_nominal是名词性列的列名，提取出名词性列的数据\n",
    "    data_temp_1 = data_nominal.apply(LabelEncoder().fit_transform)  # 将名词性列进行编号\n",
    "    del data_nominal\n",
    "\n",
    "    new_col_names = []\n",
    "    for col_name in cols_nominal:\n",
    "        name_unique = sorted(dataset[col_name].unique())\n",
    "        new_col_name = [col_name + '_' + x for x in name_unique]\n",
    "\n",
    "        new_col_names.extend(new_col_name)\n",
    "        dataset.drop(col_name, axis=1, inplace=True)\n",
    "\n",
    "    # one-hot\n",
    "    enc = OneHotEncoder()\n",
    "    data_temp_2 = enc.fit_transform(data_temp_1)\n",
    "    del data_temp_1\n",
    "\n",
    "    data_encoded = pd.DataFrame(data_temp_2.toarray(), columns=new_col_names)\n",
    "    del data_temp_2\n",
    "\n",
    "    # complement the nominal columns 补充名词性列\n",
    "    diff = set(cols_nominal_all) - set(new_col_names)\n",
    "\n",
    "    if diff:\n",
    "        for cols in diff:\n",
    "            data_encoded[cols] = 0.\n",
    "        data_encoded = data_encoded[cols_nominal_all]\n",
    "\n",
    "    dataset = dataset.join(data_encoded)\n",
    "    del data_encoded\n",
    "\n",
    "    dataset = dataset.join(new_label_10)\n",
    "    dataset = dataset.join(label_2)\n",
    "\n",
    "    return dataset  # Complete data set (including data and labels)\n",
    "    # 完整的数据集（包括数据和标签）"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the training set and test set and save the file as a CSV file\n",
    "# 分裂训练集和测试集,并将文件保存成CSV文件\n",
    "def split_dataset(dataset, file_train, file_test):\n",
    "    cols = dataset.columns\n",
    "    # trainset, testset = train_test_split(dataset, test_size = 0.2)\n",
    "    trainset, testset = train_test_split(dataset, test_size=0.2, random_state=40, stratify=dataset['label_10'])\n",
    "    train = pd.DataFrame(trainset, columns=cols)\n",
    "    test = pd.DataFrame(testset, columns=cols)\n",
    "\n",
    "    train.to_csv(file_train)\n",
    "    test.to_csv(file_test)\n",
    "\n",
    "\n",
    "# Standardize, and save the file in CSV and tf formats\n",
    "# 标准化，并将文件保存成csv格式和tf格式\n",
    "def scaling(files_train, files_test, col_names_scaling, scaling_type):\n",
    "    if scaling_type == 'min_max':\n",
    "        scaler = MinMaxScaler()\n",
    "        file_folder = 'min_max/'\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "        file_folder = 'normalized/'\n",
    "\n",
    "    if not os.path.exists(file_folder):\n",
    "        os.mkdir(file_folder)\n",
    "    cols = []\n",
    "    for file in files_train:\n",
    "        # col 0 is the index in the file\n",
    "        trainset = pd.read_csv(file, index_col=0, dtype=np.float32)\n",
    "        if len(cols) == 0:\n",
    "            cols = trainset.columns\n",
    "        scaler.partial_fit(trainset[col_names_scaling])\n",
    "\n",
    "    del trainset\n",
    "    cols_keep = list(set(cols) - set(col_names_scaling))\n",
    "\n",
    "    for file in files_train:\n",
    "        trainset = pd.read_csv(file, dtype=np.float32)\n",
    "        train_scaled = scaler.transform(trainset[col_names_scaling])\n",
    "        train_changed = pd.DataFrame(train_scaled, columns=col_names_scaling)\n",
    "        train_unchanged = trainset[cols_keep]\n",
    "        trainset_final = pd.concat((train_changed, train_unchanged),\n",
    "                                   axis=1)\n",
    "        trainset_final = trainset_final[cols]\n",
    "        print(\"train:\", trainset_final.shape)  # trainset shape\n",
    "        file_csv = file_folder + file\n",
    "        trainset.to_csv(file_csv, index=False)\n",
    "        len_tail = len('.csv')\n",
    "        file_tfr = file_folder + file[:-1 * len_tail] + '.tfrecords'\n",
    "        make_tfrecords(trainset_final, file_tfr)\n",
    "\n",
    "    for file in files_test:\n",
    "        testset = pd.read_csv(file, dtype=np.float32)\n",
    "        test_scaled = scaler.transform(testset[col_names_scaling])\n",
    "        test_changed = pd.DataFrame(test_scaled, columns=col_names_scaling)\n",
    "        test_unchanged = testset[cols_keep]\n",
    "        testset_final = pd.concat((test_changed, test_unchanged), axis=1)\n",
    "        testset_final = testset_final[cols]\n",
    "        print(\"test:\", testset_final.shape)\n",
    "        file_csv = file_folder + file\n",
    "        testset.to_csv(file_csv, index=False)\n",
    "        len_tail = len('.csv')\n",
    "        file_tfr = file_folder + file[:-1 * len_tail] + '.tfrecords'\n",
    "        make_tfrecords(testset_final, file_tfr)\n",
    "\n",
    "\n",
    "# Save the file in tf format\n",
    "# 将文件保存成tf格式\n",
    "def make_tfrecords(dataset, file_to_save):\n",
    "    try:\n",
    "        data = dataset.values\n",
    "    except:\n",
    "        data = dataset\n",
    "    with tf.compat.v1.python_io.TFRecordWriter(file_to_save) as writer:  # python_io在tfv1中\n",
    "        for rows in data:\n",
    "            features, label_10, label_2 = rows[:-2], rows[-2], rows[-1]\n",
    "            feature = {'features': tf.train.Feature(float_list=tf.train.FloatList(value=features)),\n",
    "                       'label_2': tf.train.Feature(float_list=tf.train.FloatList(value=[label_2])),\n",
    "                       'label_10': tf.train.Feature(float_list=tf.train.FloatList(value=[label_10]))}\n",
    "            example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "            writer.write(example.SerializeToString())\n",
    "\n",
    "\n",
    "def next_batch(filename, batch_size):\n",
    "    len_feature = 202  # 特征数（不包含标签）。 Number of features (not including tags)\n",
    "    len_label = 1  # 标签长度。 The length of the label\n",
    "\n",
    "    def read_data(examples):\n",
    "        features = {\"features\": tf.io.FixedLenFeature([len_feature], tf.float32),\n",
    "                    \"label_2\": tf.io.FixedLenFeature([len_label], tf.float32),\n",
    "                    \"label_10\": tf.io.FixedLenFeature([len_label], tf.float32)}\n",
    "        parsed_features = tf.io.parse_single_example(examples, features)\n",
    "        return parsed_features['features'], parsed_features['label_2'], \\\n",
    "               parsed_features['label_10']\n",
    "\n",
    "    data = tf.data.TFRecordDataset(filename)\n",
    "    data = data.map(read_data)\n",
    "    data = data.batch(batch_size)\n",
    "    iterator = tf.compat.v1.data.make_one_shot_iterator(data)\n",
    "    next_data, next_label_2, next_label_10 = iterator.get_next()\n",
    "\n",
    "    return next_data, next_label_10, next_label_2\n",
    "\n",
    "\n",
    "# Integrate the four separate data sets\n",
    "# 将分开的4个数据集整合到一起\n",
    "def make_whole_datasets(tfrecords_train, num_train_example, tfrecords_test,\n",
    "                        num_test_example):\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        data_test, label_10_test, label_2_test = next_batch(tfrecords_test, num_test_example)\n",
    "        data, label_10, label_2 = sess.run([data_test, label_10_test, label_2_test])\n",
    "    dataset = np.concatenate([data, label_10, label_2], axis=1)\n",
    "    print(\"test:\", dataset.shape)\n",
    "    make_tfrecords(dataset, 'normalized/test.tfrecords')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    file_folder = 'F:/数据挖掘/SGM-CNN/UNSW_NB15/'  # 读取的原始文件所在的位置。 The location where the original file was read\n",
    "    col_names = ['srcip', 'sport', 'dstip', 'dsport', 'proto', 'state', 'dur',\n",
    "                 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss', 'dloss',\n",
    "                 'service', 'sload', 'dload', 'spkts', 'dpkts', 'swin', 'dwin',\n",
    "                 'stcpb', 'dtcpb', 'smeansz', 'dmeansz', 'trans_depth',\n",
    "                 'res_bdy_len', 'sjit', 'djit', 'stime', 'ltime', 'sintpkt',\n",
    "                 'dintpkt', 'tcprtt', 'synack', 'ackdat', 'is_sm_ips',\n",
    "                 'ct_state_ttl', 'ct_flw', 'is_ftp', 'ct_ftp', 'ct_srv_src',\n",
    "                 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport',\n",
    "                 'ct_dst_sport', 'ct_dst_src', 'label_10', 'label_2']  # 特证名（列名）。 listed name\n",
    "\n",
    "    cols_to_drop = ['srcip', 'dstip', 'stime', 'ltime', 'sport', 'dsport']\n",
    "    cols_nominal = ['proto', 'service', 'state']  # 名词性特征。Nominal features\n",
    "\n",
    "    files = [file_folder + 'UNSW-NB15_' + str(i + 1) + '.csv' for i in range(4)]\n",
    "    # dataset = combine_dataset(files, col_names)\n",
    "    file_folder = 'normalized/'  # 数据标准化后存放的文件夹。A folder where data is stored after standardization\n",
    "    files_train = [file_folder + str(x + 1) + '_train.tfrecords' for x in range(4)]\n",
    "    files_test = [file_folder + str(x + 1) + '_test.tfrecords' for x in range(4)]\n",
    "    num_train_example = 2032035  # trainset size\n",
    "    num_test_example = 508012  # testset size\n",
    "    make_whole_datasets(files_train, num_train_example, files_test, num_test_example)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import heapq\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import compat\n",
    "\n",
    "\n",
    "class DAE(object):\n",
    "    \"\"\"\n",
    "\tDenoising autoencoder. Gaussian noise is added. The scale and standard deviation\n",
    "\tof it are noise_scale and noise_std, respectively.\n",
    "\t\"\"\"\n",
    "\n",
    "    def __init__(self, n_feature, n_hidden, noise_scale, noise_std, reg_lamda=0.01):\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_feature = n_feature\n",
    "        self.reg_lamda = reg_lamda\n",
    "        self.noise_scale = noise_scale\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        self.data = compat.v1.placeholder(shape=[None, n_feature],\n",
    "                                          dtype=compat.v1.float64)\n",
    "        self.noise = self.noise_scale * compat.v1.random_normal([n_feature], dtype=compat.v1.float64,\n",
    "                                                                stddev=self.noise_std)\n",
    "        data_with_noise = self.data + self.noise\n",
    "\n",
    "        self.weight_encoder = compat.v1.get_variable(name='weight_encoder',\n",
    "                                                     shape=[self.n_feature, self.n_hidden],\n",
    "                                                     dtype=compat.v1.float64)\n",
    "        self.bias_encoder = compat.v1.Variable(compat.v1.zeros([self.n_hidden],\n",
    "                                                               dtype=compat.v1.float64),\n",
    "                                               name='bias_encoder')\n",
    "\n",
    "        weight_decoder = compat.v1.get_variable(name='weight_decoder',\n",
    "                                                shape=[self.n_hidden, self.n_feature],\n",
    "                                                dtype=compat.v1.float64)\n",
    "        bias_decoder = compat.v1.Variable(compat.v1.zeros([self.n_feature], dtype=compat.v1.float64),\n",
    "                                          name='bias_decoder')\n",
    "\n",
    "        with compat.v1.name_scope('Encoder'):\n",
    "            data_encoded = compat.v1.add(compat.v1.matmul(data_with_noise, self.weight_encoder),\n",
    "                                         self.bias_encoder)\n",
    "            data_encoded = compat.v1.nn.tanh(data_encoded)\n",
    "\n",
    "        with compat.v1.name_scope('Decoder'):\n",
    "            data_recons = compat.v1.add(compat.v1.matmul(data_encoded, weight_decoder),\n",
    "                                        bias_decoder)\n",
    "            self.data_recons = compat.v1.tanh(data_recons)\n",
    "\n",
    "        with compat.v1.name_scope('Loss'):\n",
    "            diff = self.data_recons - self.data\n",
    "            self.loss_mse = 0.5 * compat.v1.reduce_mean(compat.v1.reduce_sum(diff ** 2, axis=1))\n",
    "            loss_reg = compat.v1.reduce_sum(compat.v1.sqrt(compat.v1.reduce_sum(self.weight_encoder ** 2, axis=1)))\n",
    "            self.loss_reg = self.reg_lamda * loss_reg\n",
    "            self.l2_loss = compat.v1.nn.l2_loss(weight_decoder) * 1E-3\n",
    "\n",
    "            self.loss = self.loss_mse + self.loss_reg + self.l2_loss\n",
    "\n",
    "        with compat.v1.name_scope('weight_vector'):\n",
    "            self.weight_vector = compat.v1.reduce_sum(self.weight_encoder ** 2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class unbalanced_DAE(object):\n",
    "    \"\"\"\n",
    "\tAn unbalanced version of DAE. the differences is that a weight pos_weight is added\n",
    "\tto the MSE reconstruction loss for positive examples. For this purpose, the labels\n",
    "\tof the examples are used.\n",
    "\t\"\"\"\n",
    "\n",
    "    def __init__(self, n_feature, n_hidden, noise_scale, noise_std,\n",
    "                 posi_weight=1.0, reg_lamda=0.00):\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_feature = n_feature\n",
    "        self.reg_lamda = reg_lamda\n",
    "        self.noise_scale = noise_scale\n",
    "        self.noise_std = noise_std\n",
    "        self.posi_weight = posi_weight\n",
    "\n",
    "        self.data = compat.v1.placeholder(shape=[None, n_feature], dtype=compat.v1.float64)\n",
    "        self.label = compat.v1.placeholder(shape=[None, 1], dtype=compat.v1.float64)\n",
    "        self.noise = self.noise_scale * compat.v1.random_normal([n_feature], dtype=compat.v1.float64,\n",
    "                                                                stddev=self.noise_std)\n",
    "        data_with_noise = self.data + self.noise\n",
    "\n",
    "        self.weight_encoder = compat.v1.get_variable(name='weight_encoder',\n",
    "                                                     shape=[self.n_feature, self.n_hidden],\n",
    "                                                     dtype=compat.v1.float64)\n",
    "        self.bias_encoder = compat.v1.Variable(compat.v1.zeros([self.n_hidden],\n",
    "                                                               dtype=compat.v1.float64),\n",
    "                                               name='bias_encoder')\n",
    "\n",
    "        weight_decoder = compat.v1.get_variable(name='weight_decoder',\n",
    "                                                shape=[self.n_hidden, self.n_feature],\n",
    "                                                dtype=compat.v1.float64)\n",
    "        bias_decoder = compat.v1.Variable(compat.v1.zeros([self.n_feature], dtype=compat.v1.float64),\n",
    "                                          name='bias_decoder')\n",
    "\n",
    "        with compat.v1.name_scope('Encoder'):\n",
    "            data_encoded = compat.v1.add(compat.v1.matmul(data_with_noise, self.weight_encoder),\n",
    "                                         self.bias_encoder)\n",
    "            data_encoded = compat.v1.nn.sigmoid(data_encoded)\n",
    "\n",
    "        with compat.v1.name_scope('Decoder'):\n",
    "            data_recons = compat.v1.add(compat.v1.matmul(data_encoded, weight_decoder),\n",
    "                                        bias_decoder)\n",
    "            self.data_recons = compat.v1.nn.sigmoid(data_recons)\n",
    "\n",
    "        with compat.v1.name_scope('Loss'):\n",
    "            diff = self.data_recons - self.data\n",
    "            weights = self.label * (posi_weight - 1) + 1\n",
    "            weights = compat.v1.reshape(weights, shape=[-1])\n",
    "            self.loss_mse = 0.5 * compat.v1.reduce_mean(compat.v1.reduce_sum(diff ** 2, axis=1) * weights)\n",
    "            loss_reg = compat.v1.reduce_sum(compat.v1.sqrt(compat.v1.reduce_sum(self.weight_encoder ** 2, axis=1)))\n",
    "            self.loss_reg = self.reg_lamda * loss_reg\n",
    "            self.l2_loss = compat.v1.nn.l2_loss(weight_decoder) * 1E-3\n",
    "\n",
    "            self.loss = self.loss_mse + self.loss_reg + self.l2_loss\n",
    "\n",
    "        with compat.v1.name_scope('weight_vector'):\n",
    "            self.weight_vector = compat.v1.reduce_sum(self.weight_encoder ** 2, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def P_R_F1(confusion_matrix):\n",
    "    category = confusion_matrix.shape[0]\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    for i in range(category):\n",
    "        TP = confusion_matrix[i, i]\n",
    "\n",
    "        precsion_temp = TP / np.sum(confusion_matrix[:, i])\n",
    "        recall_temp = TP / np.sum(confusion_matrix[i, :])\n",
    "        f1_temp = 2 * precsion_temp * recall_temp / (precsion_temp + recall_temp)\n",
    "\n",
    "        precision.append(precsion_temp)\n",
    "        recall.append(recall_temp)\n",
    "        f1.append(f1_temp)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "# shaping labels to one-hot vectors for trainning\n",
    "def label_coding(label, batch_size, category):\n",
    "    new_label = compat.v1.cast(label, dtype=compat.v1.int32)\n",
    "    new_label = compat.v1.reshape(new_label, [batch_size, 1])\n",
    "    new_label = compat.v1.one_hot(new_label, depth=category)\n",
    "    return compat.v1.reshape(new_label, [batch_size, category])\n",
    "\n",
    "\n",
    "# get next batch of data and label\n",
    "def next_batch(filename, batch_size, conf, buffer_size=0):\n",
    "    len_feature = conf.len_feature\n",
    "    len_label = conf.len_label\n",
    "    num_classes = conf.num_classes\n",
    "    one_hot_encoding = conf.one_hot_encoding\n",
    "\n",
    "    def read_data(examples):\n",
    "        features = {\"features\": compat.v1.FixedLenFeature([len_feature], compat.v1.float32),\n",
    "                    \"label_2\": compat.v1.FixedLenFeature([len_label], compat.v1.float32),\n",
    "                    \"label_10\": compat.v1.FixedLenFeature([len_label], compat.v1.float32)}\n",
    "        parsed_features = compat.v1.parse_single_example(examples, features)\n",
    "        return parsed_features['features'], parsed_features['label_2'], \\\n",
    "               parsed_features['label_10']\n",
    "\n",
    "    data = compat.v1.data.TFRecordDataset(filename)\n",
    "    data = data.map(read_data)\n",
    "    if buffer_size != 0:\n",
    "        data = data.shuffle(buffer_size=buffer_size)\n",
    "    data = data.repeat()\n",
    "    data = data.batch(batch_size)\n",
    "    iterator = data.make_one_shot_iterator()\n",
    "    next_data, next_label_2, next_label_10 = iterator.get_next()\n",
    "\n",
    "    if one_hot_encoding == True:\n",
    "        if num_classes == 2:\n",
    "            next_label_2 = label_coding(next_label_2, batch_size,\n",
    "                                        num_classes)\n",
    "        else:\n",
    "            next_label_10 = label_coding(next_label_10, batch_size,\n",
    "                                         num_classes)\n",
    "\n",
    "    return next_data, next_label_2, next_label_10\n",
    "\n",
    "\n",
    "def trans_dataset(file_tfr, file_txt, num_examples, num_classes):\n",
    "    with compat.v1.Session() as sess:\n",
    "        all_data, all_label = next_batch(file_tfr, num_examples)\n",
    "        all_label = label_coding(all_label, num_examples, num_classes)\n",
    "\n",
    "        record = np.concatenate([sess.run(all_data), sess.run(all_label)], axis=1)\n",
    "        np.savetxt(file_txt, record, fmt='%.6e')\n",
    "\n",
    "\n",
    "def split_dataset(file_train, file_test, k, file_folder_new):  # k is refer to k_fold\n",
    "\n",
    "    trainset = np.loadtxt(file_train)\n",
    "    testset = np.loadtxt(file_test)\n",
    "    dataset = np.concatenate((trainset, testset))\n",
    "\n",
    "    for i in range(k - 1):\n",
    "        trainset, testset = train_test_split(dataset, test_size=1 / (k - i))\n",
    "        dataset = trainset\n",
    "        np.savetxt(file_folder_new + str(i) + '.txt', testset)\n",
    "\n",
    "    np.savetxt(file_folder_new + str(k - 1) + '.txt', trainset)\n",
    "\n",
    "\n",
    "def get_dataset(file_folder, index_test, indices_train):\n",
    "    testset = np.loadtxt(file_folder + str(index_test) + '.txt')\n",
    "\n",
    "    count = 0\n",
    "    for other in indices_train:\n",
    "        temp = np.loadtxt(file_folder + str(other) + '.txt')\n",
    "\n",
    "        if count == 0:\n",
    "            trainset = temp\n",
    "        else:\n",
    "            trainset = np.concatenate((trainset, temp))\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    return trainset, testset\n",
    "\n",
    "\n",
    "def parse_pos_neg(dataset):\n",
    "    label = dataset[:, -1]\n",
    "\n",
    "    record_posi = []\n",
    "    record_neg = []\n",
    "\n",
    "    records_len = dataset.shape[-1]\n",
    "    records_num = dataset.shape[0]\n",
    "\n",
    "    for index in range(records_num):\n",
    "        record = dataset[index, :]\n",
    "        record = np.reshape(record, (1, records_len))\n",
    "        if label[index] == 0.:\n",
    "            record_posi.append(record)\n",
    "        else:\n",
    "            record_neg.append(record)\n",
    "\n",
    "    posi = np.concatenate(record_posi)\n",
    "    neg = np.concatenate(record_neg)\n",
    "\n",
    "    return posi, neg\n",
    "\n",
    "\n",
    "compat.v1.reset_default_graph()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# system parameters\n",
    "\n",
    "class Configures(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        # parameter of records\n",
    "        self.len_feature = 202\n",
    "        self.len_label = 1\n",
    "        # self.num_classes = 2\n",
    "        self.num_classes = 10\n",
    "        self.one_hot_encoding = False\n",
    "        self.num_records_train = 1625628\n",
    "        self.num_records_test = 508012\n",
    "\n",
    "        # parameters for training\n",
    "        self.batch_size = 256\n",
    "        self.batch_size_test = 2048\n",
    "        self.training_epochs = 2\n",
    "        self.learn_rate_start = 1E-4\n",
    "\n",
    "        self.batch_train = self.num_records_train // self.batch_size\n",
    "        self.batch_test = self.num_records_test // self.batch_size_test\n",
    "\n",
    "\n",
    "n_hidden = 64\n",
    "noise_scale = 0.\n",
    "noise_std = 0.1\n",
    "conf = Configures()\n",
    "\n",
    "\n",
    "# training op\n",
    "\n",
    "\n",
    "def get_indices(num_select, num_feature, file_weights):\n",
    "    \"\"\"\n",
    "    This function is to select maximum k features according to their\n",
    "    weights.\n",
    "\n",
    "    Pram:\n",
    "        num_select: An interger, the number of the selected features\n",
    "        num_feature: An interger, the number of the original features\n",
    "        file_weights: A txt file storing a numpy array. Each row of the\n",
    "                      array is the weight for a feature\n",
    "    Return:\n",
    "        a list containing the indices of selected features\n",
    "    \"\"\"\n",
    "\n",
    "    x = np.arange(1, num_feature + 1)\n",
    "    y = np.loadtxt(file_weights)\n",
    "    indices = heapq.nlargest(num_select, range(len(y)), y.take)\n",
    "    plt.scatter(x, y)\n",
    "    plt.show()\n",
    "    plt.savefig('weights_dis.eps', format='eps')\n",
    "    print(indices)\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "def read_data(examples):\n",
    "    features = {\"features\": compat.v1.FixedLenFeature([num_feature], compat.v1.float32),\n",
    "                \"label_2\": compat.v1.FixedLenFeature([len_label], compat.v1.float32),\n",
    "                \"label_10\": compat.v1.FixedLenFeature([len_label], compat.v1.float32)}\n",
    "    parsed_features = compat.v1.parse_single_example(examples, features)\n",
    "    return parsed_features['features'], parsed_features['label_2'], \\\n",
    "           parsed_features['label_10']\n",
    "\n",
    "\n",
    "# get next batch of data and label\n",
    "def next_batch(filename, num_examples):\n",
    "    data = compat.v1.data.TFRecordDataset(filename)\n",
    "    data = data.map(read_data)\n",
    "    data = data.batch(num_examples)\n",
    "    iterator = data.make_one_shot_iterator()\n",
    "    next_data, next_label_2, next_label_10 = iterator.get_next()\n",
    "    return next_data, next_label_2, next_label_10\n",
    "\n",
    "\n",
    "def make_tfrecords(dataset, file_to_save):\n",
    "    [features, label_2, label_10] = dataset\n",
    "\n",
    "    with compat.v1.python_io.TFRecordWriter(file_to_save) as writer:\n",
    "        for index in range(features.shape[0]):\n",
    "            feature = {\n",
    "                'features': compat.v1.train.Feature(float_list=compat.v1.train.FloatList(value=features[index, :])),\n",
    "                'label_2': compat.v1.train.Feature(float_list=compat.v1.train.FloatList(value=label_2[index, :])),\n",
    "                'label_10': compat.v1.train.Feature(float_list=compat.v1.train.FloatList(value=label_10[index, :]))}\n",
    "            example = compat.v1.train.Example(features=compat.v1.train.Features(feature=feature))\n",
    "            writer.write(example.SerializeToString())\n",
    "\n",
    "\n",
    "def selection(data, indices):\n",
    "    \"\"\"\n",
    "    select the columns (indicating the features) according to the indices\n",
    "    \"\"\"\n",
    "\n",
    "    return data[:, indices]\n",
    "\n",
    "\n",
    "def select_feature(file, num_examples, indices):\n",
    "    \"\"\"\n",
    "    The main function of feature selection.\n",
    "\n",
    "    Params:\n",
    "      file: The .tfrecords file containing original data.包含原始数据的.tfrecords文件\n",
    "      num_examples: The number of examples in the file  文件中的记录数\n",
    "      indices: The indices of features to be selected  被选择特征的索引\n",
    "\n",
    "    Return:\n",
    "      None\n",
    "      In the function, a new .tfrecords file with tail of 'selected'\n",
    "      will be created in the same folder with the original data\n",
    "    \"\"\"\n",
    "\n",
    "    with compat.v1.Session() as sess:\n",
    "        data, label_2, label_10 = sess.run(next_batch(file, num_examples))\n",
    "\n",
    "    data_select = selection(data, indices)\n",
    "\n",
    "    file_name = file.split('\\\\')[-1]\n",
    "    file_tail = len('.npy')\n",
    "    file_to_save = file_name[:-1 * file_tail] + '_select_' + str(len(indices)) + '_data_.npy'\n",
    "    np.save(file_to_save,data_select)\n",
    "    file_to_save = file_name[:-1 * file_tail] + '_select_' + str(len(indices)) + '_label_2_.npy'\n",
    "    np.save(file_to_save,label_2)\n",
    "    file_to_save = file_name[:-1 * file_tail] + '_select_' + str(len(indices)) + '_label_10_.npy'\n",
    "    np.save(file_to_save,label_10)\n",
    "    # make_tfrecords([data_select, label_2, label_10], file_to_save)\n",
    "\n",
    "\n",
    "def show_feature_name(indices):\n",
    "    \"\"\"\n",
    "    The function to convert indices to feature names\n",
    "\n",
    "    Params:\n",
    "      indices:the indices of the features\n",
    "    Return:\n",
    "      None.\n",
    "      The name of features will be print\n",
    "    \"\"\"\n",
    "\n",
    "    file = os.path.join('normalized/', '1_test.csv')\n",
    "    data = read_csv(file, index_col=0)\n",
    "    cols = data.columns\n",
    "\n",
    "    for x in indices:\n",
    "        print(cols[x])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    num_select = 12  # 选择出来的特征数。The number of selected features\n",
    "    num_feature = 202\n",
    "    len_label = 1\n",
    "    file_weights = 'F:/数据挖掘/SGM-CNN/normalized/weights_new_3.5.txt'\n",
    "\n",
    "    indices = get_indices(num_select, num_feature, file_weights)\n",
    "    # print(indices)\n",
    "    show_feature_name(indices)\n",
    "\n",
    "    file_train = 'F:/数据挖掘/SGM-CNN/normalized/train.tfrecords'\n",
    "    file_valid = 'F:/数据挖掘/SGM-CNN/normalized/validation.tfrecords'\n",
    "    num_examples_train = 1778030\n",
    "    num_examples_validation = 254005\n",
    "    file_test = 'F:/数据挖掘/SGM-CNN/normalized/test.tfrecords'\n",
    "    num_examples_test = 508012\n",
    "\n",
    "    select_feature(file_train, num_examples_train, indices)\n",
    "    select_feature(file_valid, num_examples_validation, indices)\n",
    "    #select_feature(file_test, num_examples_test, indices)\n",
    "\n",
    "get_indices(num_select, num_feature, file_weights)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SMOTE all minority classes to (number of data set samples / classes)\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "data = np.load('F:/数据挖掘/SGM-CNN/新建文件夹/train_select_12_data.npy')\n",
    "label = np.load('F:/数据挖掘/SGM-CNN/新建文件夹/train_select_12_label_2.npy')\n",
    "\n",
    "X = np.array(data)\n",
    "b = np.array(label)\n",
    "bb = b.reshape(b.shape[0], )\n",
    "y2 = np.int32(bb)\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "sorted(Counter(y2).items())\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import time\n",
    "\n",
    "time_start = time.time()\n",
    "a = 889015\n",
    "# [X[0].size/2]\n",
    "\n",
    "smo = SMOTE(sampling_strategy={1:a}, random_state=42)\n",
    "\n",
    "X_smo, y_smo = smo.fit_sample(X, y2)\n",
    "print(sorted(Counter(y_smo).items()))\n",
    "\n",
    "time_end = time.time()\n",
    "time = time_end - time_start\n",
    "print(\"time:\", time)\n",
    "\n",
    "print(X_smo.shape[0])\n",
    "\n",
    "# Extract Majority class of data\n",
    "\n",
    "list0 = []\n",
    "list1 = []\n",
    "list2 = []\n",
    "\n",
    "for i in range(X_smo.shape[0]):\n",
    "    if y_smo[i] == 0:\n",
    "        list0.append(X_smo[i])  # 正常流量\n",
    "    else:\n",
    "        list1.append(X_smo[i])\n",
    "        list2.append(y_smo[i])\n",
    "\n",
    "data0 = np.array(list0)\n",
    "data1 = np.array(list1)\n",
    "label1 = np.array(list2)\n",
    "\n",
    "label11 = label1.reshape(label1.shape[0], )\n",
    "\n",
    "print(\"Normal class data shape：\", data0.shape)\n",
    "print(\"Attack class data shape：\", data1.shape)\n",
    "print(\"Attack class label shape：\", label11.shape)\n",
    "\n",
    "# Cluster majority data into  C (total number of classes)\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import time\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "estimator = GaussianMixture(n_components=10)\n",
    "estimator.fit(data0)\n",
    "\n",
    "time_end = time.time()\n",
    "time = time_end - time_start\n",
    "print(\"time:\", time)\n",
    "\n",
    "label_pred = estimator.predict(data0)\n",
    "\n",
    "sorted(Counter(label_pred).items())\n",
    "\n",
    "# Select a certain amount of data from each cluster to form a new majority data\n",
    "\n",
    "\n",
    "c0 = []\n",
    "c1 = []\n",
    "s0 = s1 = 0\n",
    "\n",
    "for i in range(data0.shape[0]):\n",
    "    if label_pred[i] == 0:\n",
    "        c0.append(data0[i])\n",
    "        s0 = s0 + 1\n",
    "    elif label_pred[i] == 1:\n",
    "        c1.append(data0[i])\n",
    "        s1 = s1 + 1\n",
    "a = 444507\n",
    "# [a/2]\n",
    "del c1[a:len(c1)]\n",
    "c00 = np.array(c0)\n",
    "c11 = np.array(c1)\n",
    "\n",
    "q = np.concatenate((c00, c11), axis=0)\n",
    "\n",
    "label_zc = np.zeros((q.shape[0],), dtype=int)\n",
    "\n",
    "data_end = np.concatenate((q, data1), axis=0)\n",
    "label_end = np.concatenate((label_zc, label1), axis=0)\n",
    "\n",
    "\n",
    "sorted(Counter(label_end).items())\n",
    "\n",
    "label_end = label_end.reshape(label_end.shape[0], 1)\n",
    "\n",
    "np.save(\"F:/数据挖掘/SGM-CNN/新建文件夹/SGM_data_train.npy\", data_end)\n",
    "np.save(\"F:/数据挖掘/SGM-CNN/新建文件夹/SGM_label2_train.npy\", label_end)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SMOTE all minority classes to (number of data set samples / classes)\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import time\n",
    "\n",
    "data = np.load('F:/数据挖掘/SGM-CNN/新建文件夹/train_select_12_data.npy')\n",
    "label = np.load('F:/数据挖掘/SGM-CNN/新建文件夹/train_select_12_label_10.npy')\n",
    "\n",
    "X = np.array(data)\n",
    "b = np.array(label)\n",
    "bb = b.reshape(b.shape[0], )\n",
    "y10 = np.int32(bb)\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "sorted(Counter(y10).items())\n",
    "\n",
    "\n",
    "time_start = time.time()\n",
    "a = 177803\n",
    "# [X[0].size/10]\n",
    "\n",
    "smo = SMOTE(sampling_strategy={1:a, 2: a, 3: a, 4: a, 5: a, 6: a, 7: a, 8: a, 9: a}, random_state=42)\n",
    "\n",
    "X_smo, y_smo = smo.fit_sample(X, y10)\n",
    "print(sorted(Counter(y_smo).items()))\n",
    "\n",
    "time_end = time.time()\n",
    "time = time_end - time_start\n",
    "print(\"time:\", time)\n",
    "\n",
    "\n",
    "print(X_smo.shape[0])\n",
    "\n",
    "# Extract Majority class of data\n",
    "\n",
    "list0 = []\n",
    "list1 = []\n",
    "list2 = []\n",
    "\n",
    "for i in range(X_smo.shape[0]):\n",
    "    if y_smo[i] == 0:\n",
    "        list0.append(X_smo[i])  # 正常流量\n",
    "    else:\n",
    "        list1.append(X_smo[i])\n",
    "        list2.append(y_smo[i])\n",
    "\n",
    "data0 = np.array(list0)\n",
    "data1 = np.array(list1)\n",
    "label1 = np.array(list2)\n",
    "\n",
    "label11 = label1.reshape(label1.shape[0], )\n",
    "\n",
    "print(\"Normal class data shape：\", data0.shape)\n",
    "print(\"Attack class data shape：\", data1.shape)\n",
    "print(\"Attack class label shape：\", label11.shape)\n",
    "\n",
    "# Cluster majority data into  C (total number of classes)\n",
    "\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "estimator = GaussianMixture(n_components=10)\n",
    "estimator.fit(data0)\n",
    "\n",
    "time_end = time.time()\n",
    "time = time_end - time_start\n",
    "print(\"time:\", time)\n",
    "\n",
    "label_pred = estimator.predict(data0)\n",
    "sorted(Counter(label_pred).items())\n",
    "\n",
    "# Select a certain amount of data from each cluster to form a new majority data\n",
    "\n",
    "\n",
    "c0 = []\n",
    "c1 = []\n",
    "c2 = []\n",
    "c3 = []\n",
    "c4 = []\n",
    "c5 = []\n",
    "c6 = []\n",
    "c7 = []\n",
    "c8 = []\n",
    "c9 = []\n",
    "s0 = s1 = s2 = s3 = s4 = s5 = s6 = s7 = s8 = s9 = 0\n",
    "\n",
    "for i in range(data0.shape[0]):\n",
    "    if label_pred[i] == 0:\n",
    "        c0.append(data0[i])\n",
    "        s0 = s0 + 1\n",
    "    elif label_pred[i] == 1:\n",
    "        c1.append(data0[i])\n",
    "        s1 = s1 + 1\n",
    "    elif label_pred[i] == 2:\n",
    "        c2.append(data0[i])\n",
    "        s2 = s2 + 1\n",
    "    elif label_pred[i] == 3:\n",
    "        c3.append(data0[i])\n",
    "        s3 = s3 + 1\n",
    "    elif label_pred[i] == 4:\n",
    "        c4.append(data0[i])\n",
    "        s4 = s4 + 1\n",
    "    elif label_pred[i] == 5:\n",
    "        c5.append(data0[i])\n",
    "        s5 = s5 + 1\n",
    "    elif label_pred[i] == 6:\n",
    "        c6.append(data0[i])\n",
    "        s6 = s6 + 1\n",
    "    elif label_pred[i] == 7:\n",
    "        c7.append(data0[i])\n",
    "        s7 = s7 + 1\n",
    "    elif label_pred[i] == 8:\n",
    "        c8.append(data0[i])\n",
    "        s8 = s8 + 1\n",
    "    elif label_pred[i] == 9:\n",
    "        c9.append(data0[i])\n",
    "        s9 = s9 + 1\n",
    "\n",
    "a = 17780\n",
    "# [a/10]\n",
    "\n",
    "del c1[a:len(c1)]\n",
    "del c2[a:len(c2)]\n",
    "del c3[a:len(c3)]\n",
    "del c4[a:len(c4)]\n",
    "del c5[a:len(c5)]\n",
    "del c6[a:len(c6)]\n",
    "del c7[a:len(c7)]\n",
    "del c8[a:len(c8)]\n",
    "del c9[a:len(c9)]\n",
    "\n",
    "c00 = np.array(c0)\n",
    "c11 = np.array(c1)\n",
    "c22 = np.array(c2)\n",
    "c33 = np.array(c3)\n",
    "c44 = np.array(c4)\n",
    "c55 = np.array(c5)\n",
    "c66 = np.array(c6)\n",
    "c77 = np.array(c7)\n",
    "c88 = np.array(c8)\n",
    "c99 = np.array(c9)\n",
    "\n",
    "q = np.concatenate((c00, c11, c22, c33, c44, c55, c66, c77, c88, c99), axis=0)\n",
    "label_zc = np.zeros((q.shape[0],), dtype=int)\n",
    "data_end = np.concatenate((q, data1), axis=0)\n",
    "label_end = np.concatenate((label_zc, label1), axis=0)\n",
    "\n",
    "sorted(Counter(label_end).items())\n",
    "\n",
    "label_end = label_end.reshape(label_end.shape[0], 1)\n",
    "\n",
    "np.save(\"F:/数据挖掘/SGM-CNN/新建文件夹/SGM_data_train.npy\", data_end)\n",
    "np.save(\"F:/数据挖掘/SGM-CNN/新建文件夹/SGM_label10_train.npy\", label_end)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}