{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function, division\n",
    "import downhill\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import scipy.spatial\n",
    "import numpy\n",
    "from warnings import warn\n",
    "from scipy.stats import scoreatpercentile\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors.base import NeighborsBase\n",
    "from sklearn.neighbors.base import KNeighborsMixin\n",
    "from sklearn.neighbors.base import UnsupervisedMixin\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.utils import check_array\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from __future__ import print_function\n",
    "\n",
    "__docformat__ = 'restructedtext en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This tutorial introduces the multilayer perceptron using Theano.\n",
    "\n",
    " A multilayer perceptron is a logistic regressor where\n",
    "instead of feeding the input to the logistic regression you insert a\n",
    "intermediate layer, called the hidden layer, that has a nonlinear\n",
    "activation function (usually tanh or sigmoid) . One can use many such\n",
    "hidden layers making the architecture deep. The tutorial will also tackle\n",
    "the problem of MNIST digit classification.\n",
    "\n",
    ".. math::\n",
    "\n",
    "    f(x) = G( b^{(2)} + W^{(2)}( s( b^{(1)} + W^{(1)} x))),\n",
    "\n",
    "References:\n",
    "\n",
    "    - textbooks: \"Pattern Recognition and Machine Learning\" -\n",
    "                 Christopher M. Bishop, section 5\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# start-snippet-1\n",
    "class HiddenLayer(object):\n",
    "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
    "                 activation=None):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: units are fully-connected and have\n",
    "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "\n",
    "        NOTE : The nonlinearity used here is tanh\n",
    "\n",
    "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "\n",
    "        :type rng: numpy.random.RandomState\n",
    "        :param rng: a random number generator used to initialize weights\n",
    "\n",
    "        :type input: theano.tensor.dmatrix\n",
    "        :param input: a symbolic tensor of shape (n_examples, n_in)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "\n",
    "        :type activation: theano.Op or function\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        # end-snippet-1\n",
    "\n",
    "        # `W` is initialized with `W_values` which is uniformely sampled\n",
    "        # from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden))\n",
    "        # for tanh activation function\n",
    "        # the output of uniform if converted using asarray to dtype\n",
    "        # theano.config.floatX so that the code is runable on GPU\n",
    "        # Note : optimal initialization of weights is dependent on the\n",
    "        #        activation function used (among other things).\n",
    "        #        For example, results presented in [Xavier10] suggest that you\n",
    "        #        should use 4 times larger initial weights for sigmoid\n",
    "        #        compared to tanh\n",
    "        #        We have no info for other function, so we use the same as\n",
    "        #        tanh.\n",
    "        if W is None:\n",
    "            W_values = numpy.asarray(\n",
    "                rng.uniform(\n",
    "                    low=-numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    high=numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    size=(n_in, n_out)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            if activation == theano.tensor.nnet.sigmoid:\n",
    "                W_values *= 4\n",
    "\n",
    "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
    "\n",
    "        if b is None:\n",
    "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
    "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
    "\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "\n",
    "        lin_output = T.dot(input, self.W) + self.b\n",
    "        self.output = (lin_output if activation is None\n",
    "                                    else activation(lin_output))\n",
    "\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "\n",
    "\n",
    "# start-snippet-2\n",
    "#class MLP(object):\n",
    "#    \"\"\"Multi-Layer Perceptron Class\n",
    "#\n",
    "#    A multilayer perceptron is a feedforward artificial neural network model\n",
    "#    that has one layer or more of hidden units and nonlinear activations.\n",
    "#    Intermediate layers usually have as activation function tanh or the\n",
    "#    sigmoid function (defined here by a ``HiddenLayer`` class)  while the\n",
    "#    top layer is a softmax layer (defined here by a ``LogisticRegression``\n",
    "#    class).\n",
    "#    \"\"\"\n",
    "#\n",
    "#    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
    "#        \"\"\"Initialize the parameters for the multilayer perceptron\n",
    "#\n",
    "#        :type rng: numpy.random.RandomState\n",
    "#        :param rng: a random number generator used to initialize weights\n",
    "#\n",
    "#        :type input: theano.tensor.TensorType\n",
    "#        :param input: symbolic variable that describes the input of the\n",
    "#        architecture (one minibatch)\n",
    "#\n",
    "#        :type n_in: int\n",
    "#        :param n_in: number of input units, the dimension of the space in\n",
    "#        which the datapoints lie\n",
    "#\n",
    "#        :type n_hidden: int\n",
    "#        :param n_hidden: number of hidden units\n",
    "#\n",
    "#        :type n_out: int\n",
    "#        :param n_out: number of output units, the dimension of the space in\n",
    "#        which the labels lie\n",
    "#\n",
    "#        \"\"\"\n",
    "#\n",
    "#        # Since we are dealing with a one hidden layer MLP, this will translate\n",
    "#        # into a HiddenLayer with a tanh activation function connected to the\n",
    "#        # LogisticRegression layer; the activation function can be replaced by\n",
    "#        # sigmoid or any other nonlinear function\n",
    "#        self.hiddenLayer = HiddenLayer(\n",
    "#            rng=rng,\n",
    "#            input=input,\n",
    "#            n_in=n_in,\n",
    "#            n_out=n_hidden,\n",
    "#            activation=T.tanh\n",
    "#        )\n",
    "#\n",
    "#        # The logistic regression layer gets as input the hidden units\n",
    "#        # of the hidden layer\n",
    "#        self.logRegressionLayer = LogisticRegression(\n",
    "#            input=self.hiddenLayer.output,\n",
    "#            n_in=n_hidden,\n",
    "#            n_out=n_out\n",
    "#        )\n",
    "#        # end-snippet-2 start-snippet-3\n",
    "#        # L1 norm ; one regularization option is to enforce L1 norm to\n",
    "#        # be small\n",
    "#        self.L1 = (\n",
    "#            abs(self.hiddenLayer.W).sum()\n",
    "#            + abs(self.logRegressionLayer.W).sum()\n",
    "#        )\n",
    "#\n",
    "#        # square of L2 norm ; one regularization option is to enforce\n",
    "#        # square of L2 norm to be small\n",
    "#        self.L2_sqr = (\n",
    "#            (self.hiddenLayer.W ** 2).sum()\n",
    "#            + (self.logRegressionLayer.W ** 2).sum()\n",
    "#        )\n",
    "#\n",
    "#        # negative log likelihood of the MLP is given by the negative\n",
    "#        # log likelihood of the output of the model, computed in the\n",
    "#        # logistic regression layer\n",
    "#        self.negative_log_likelihood = (\n",
    "#            self.logRegressionLayer.negative_log_likelihood\n",
    "#        )\n",
    "#        # same holds for the function computing the number of errors\n",
    "#        self.errors = self.logRegressionLayer.errors\n",
    "#\n",
    "#        # the parameters of the model are the parameters of the two layer it is\n",
    "#        # made out of\n",
    "#        self.params = self.hiddenLayer.params + self.logRegressionLayer.params\n",
    "#        # end-snippet-3\n",
    "#\n",
    "#        # keep track of model input\n",
    "#        self.input = input\n",
    "\n",
    "#\n",
    "#def test_mlp(learning_rate=0.01, L1_reg=0.00, L2_reg=0.0001, n_epochs=1000,\n",
    "#             dataset='mnist.pkl.gz', batch_size=20, n_hidden=500):\n",
    "#    \"\"\"\n",
    "#    Demonstrate stochastic gradient descent optimization for a multilayer\n",
    "#    perceptron\n",
    "#\n",
    "#    This is demonstrated on MNIST.\n",
    "#\n",
    "#    :type learning_rate: float\n",
    "#    :param learning_rate: learning rate used (factor for the stochastic\n",
    "#    gradient\n",
    "#\n",
    "#    :type L1_reg: float\n",
    "#    :param L1_reg: L1-norm's weight when added to the cost (see\n",
    "#    regularization)\n",
    "#\n",
    "#    :type L2_reg: float\n",
    "#    :param L2_reg: L2-norm's weight when added to the cost (see\n",
    "#    regularization)\n",
    "#\n",
    "#    :type n_epochs: int\n",
    "#    :param n_epochs: maximal number of epochs to run the optimizer\n",
    "#\n",
    "#    :type dataset: string\n",
    "#    :param dataset: the path of the MNIST dataset file from\n",
    "#                 http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\n",
    "#\n",
    "#\n",
    "#   \"\"\"\n",
    "#    datasets = load_data(dataset)\n",
    "#\n",
    "#    train_set_x, train_set_y = datasets[0]\n",
    "#    valid_set_x, valid_set_y = datasets[1]\n",
    "#    test_set_x, test_set_y = datasets[2]\n",
    "#\n",
    "#    # compute number of minibatches for training, validation and testing\n",
    "#    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "#    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "#    n_test_batches = test_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "#\n",
    "#    ######################\n",
    "#    # BUILD ACTUAL MODEL #\n",
    "#    ######################\n",
    "#    print('... building the model')\n",
    "#\n",
    "#    # allocate symbolic variables for the data\n",
    "#    index = T.lscalar()  # index to a [mini]batch\n",
    "#    x = T.matrix('x')  # the data is presented as rasterized images\n",
    "#    y = T.ivector('y')  # the labels are presented as 1D vector of\n",
    "#                        # [int] labels\n",
    "#\n",
    "#    rng = numpy.random.RandomState(1234)\n",
    "#\n",
    "#    # construct the MLP class\n",
    "#    classifier = MLP(\n",
    "#        rng=rng,\n",
    "#        input=x,\n",
    "#        n_in=28 * 28,\n",
    "#        n_hidden=n_hidden,\n",
    "#        n_out=10\n",
    "#    )\n",
    "#\n",
    "#    # start-snippet-4\n",
    "#    # the cost we minimize during training is the negative log likelihood of\n",
    "#    # the model plus the regularization terms (L1 and L2); cost is expressed\n",
    "#    # here symbolically\n",
    "#    cost = (\n",
    "#        classifier.negative_log_likelihood(y)\n",
    "#        + L1_reg * classifier.L1\n",
    "#        + L2_reg * classifier.L2_sqr\n",
    "#    )\n",
    "#    # end-snippet-4\n",
    "#\n",
    "#    # compiling a Theano function that computes the mistakes that are made\n",
    "#    # by the model on a minibatch\n",
    "#    test_model = theano.function(\n",
    "#        inputs=[index],\n",
    "#        outputs=classifier.errors(y),\n",
    "#        givens={\n",
    "#            x: test_set_x[index * batch_size:(index + 1) * batch_size],\n",
    "#            y: test_set_y[index * batch_size:(index + 1) * batch_size]\n",
    "#        }\n",
    "#    )\n",
    "#\n",
    "#    validate_model = theano.function(\n",
    "#        inputs=[index],\n",
    "#        outputs=classifier.errors(y),\n",
    "#        givens={\n",
    "#            x: valid_set_x[index * batch_size:(index + 1) * batch_size],\n",
    "#            y: valid_set_y[index * batch_size:(index + 1) * batch_size]\n",
    "#        }\n",
    "#    )\n",
    "#\n",
    "#    # start-snippet-5\n",
    "#    # compute the gradient of cost with respect to theta (sorted in params)\n",
    "#    # the resulting gradients will be stored in a list gparams\n",
    "#    gparams = [T.grad(cost, param) for param in classifier.params]\n",
    "#\n",
    "#    # specify how to update the parameters of the model as a list of\n",
    "#    # (variable, update expression) pairs\n",
    "#\n",
    "#    # given two lists of the same length, A = [a1, a2, a3, a4] and\n",
    "#    # B = [b1, b2, b3, b4], zip generates a list C of same size, where each\n",
    "#    # element is a pair formed from the two lists :\n",
    "#    #    C = [(a1, b1), (a2, b2), (a3, b3), (a4, b4)]\n",
    "#    updates = [\n",
    "#        (param, param - learning_rate * gparam)\n",
    "#        for param, gparam in zip(classifier.params, gparams)\n",
    "#    ]\n",
    "#\n",
    "#    # compiling a Theano function `train_model` that returns the cost, but\n",
    "#    # in the same time updates the parameter of the model based on the rules\n",
    "#    # defined in `updates`\n",
    "#    train_model = theano.function(\n",
    "#        inputs=[index],\n",
    "#        outputs=cost,\n",
    "#        updates=updates,\n",
    "#        givens={\n",
    "#            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "#            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "#        }\n",
    "#    )\n",
    "#    # end-snippet-5\n",
    "#\n",
    "#    ###############\n",
    "#    # TRAIN MODEL #\n",
    "#    ###############\n",
    "#    print('... training')\n",
    "#\n",
    "#    # early-stopping parameters\n",
    "#    patience = 10000  # look as this many examples regardless\n",
    "#    patience_increase = 2  # wait this much longer when a new best is\n",
    "#                           # found\n",
    "#    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "#                                   # considered significant\n",
    "#    validation_frequency = min(n_train_batches, patience // 2)\n",
    "#                                  # go through this many\n",
    "#                                  # minibatche before checking the network\n",
    "#                                  # on the validation set; in this case we\n",
    "#                                  # check every epoch\n",
    "#\n",
    "#    best_validation_loss = numpy.inf\n",
    "#    best_iter = 0\n",
    "#    test_score = 0.\n",
    "#    start_time = timeit.default_timer()\n",
    "#\n",
    "#    epoch = 0\n",
    "#    done_looping = False\n",
    "#\n",
    "#    while (epoch < n_epochs) and (not done_looping):\n",
    "#        epoch = epoch + 1\n",
    "#        for minibatch_index in range(n_train_batches):\n",
    "#\n",
    "#            minibatch_avg_cost = train_model(minibatch_index)\n",
    "#            # iteration number\n",
    "#            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "#\n",
    "#            if (iter + 1) % validation_frequency == 0:\n",
    "#                # compute zero-one loss on validation set\n",
    "#                validation_losses = [validate_model(i) for i\n",
    "#                                     in range(n_valid_batches)]\n",
    "#                this_validation_loss = numpy.mean(validation_losses)\n",
    "#\n",
    "#                print(\n",
    "#                    'epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "#                    (\n",
    "#                        epoch,\n",
    "#                        minibatch_index + 1,\n",
    "#                        n_train_batches,\n",
    "#                        this_validation_loss * 100.\n",
    "#                    )\n",
    "#                )\n",
    "#\n",
    "#                # if we got the best validation score until now\n",
    "#                if this_validation_loss < best_validation_loss:\n",
    "#                    #improve patience if loss improvement is good enough\n",
    "#                    if (\n",
    "#                        this_validation_loss < best_validation_loss *\n",
    "#                        improvement_threshold\n",
    "#                    ):\n",
    "#                        patience = max(patience, iter * patience_increase)\n",
    "#\n",
    "#                    best_validation_loss = this_validation_loss\n",
    "#                    best_iter = iter\n",
    "#\n",
    "#                    # test it on the test set\n",
    "#                    test_losses = [test_model(i) for i\n",
    "#                                   in range(n_test_batches)]\n",
    "#                    test_score = numpy.mean(test_losses)\n",
    "#\n",
    "#                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "#                           'best model %f %%') %\n",
    "#                          (epoch, minibatch_index + 1, n_train_batches,\n",
    "#                           test_score * 100.))\n",
    "#\n",
    "#            if patience <= iter:\n",
    "#                done_looping = True\n",
    "#                break\n",
    "#\n",
    "#    end_time = timeit.default_timer()\n",
    "#    print(('Optimization complete. Best validation score of %f %% '\n",
    "#           'obtained at iteration %i, with test performance %f %%') %\n",
    "#          (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n",
    "#    print(('The code for file ' +\n",
    "#           os.path.split(__file__)[1] +\n",
    "#           ' ran for %.2fm' % ((end_time - start_time) / 60.)), file=sys.stderr)\n",
    "#\n",
    "#\n",
    "#if __name__ == '__main__':\n",
    "#    test_mlp()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Aug 17 15:26:18 2017\n",
    "\n",
    "@author: VANLOI\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"calculate batch_size on different size of dataset\"\n",
    "def stopping_para_vae(train_size):\n",
    "    n_batch  = 0\n",
    "    b_size = 0\n",
    "    max_stop = 2000\n",
    "    if (train_size <= 2000):\n",
    "        n_batch  = 20\n",
    "        b_size = int(train_size/n_batch)\n",
    "    else:\n",
    "        b_size = 100\n",
    "        n_batch  = int(train_size/b_size)\n",
    "\n",
    "    patience = round(max_stop/n_batch)\n",
    "    every_valid = 1\n",
    "    return patience, every_valid, b_size, n_batch\n",
    "\n",
    "\n",
    "def stopping_para_shrink(train_size):\n",
    "    n_batch  = 0\n",
    "    b_size = 0\n",
    "    max_stop = 2000.0\n",
    "    if (train_size <= 2000):\n",
    "        n_batch  = 20\n",
    "        b_size = int(train_size/n_batch)\n",
    "    else:\n",
    "        b_size = 100\n",
    "        n_batch  = int(train_size/b_size)\n",
    "\n",
    "    every_valid = 5\n",
    "    \"update times after validation = every_valid x n_batch\"\n",
    "    patience =  round(max_stop/(every_valid*n_batch))\n",
    "\n",
    "    return patience, every_valid, b_size, n_batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Apr 06 12:59:05 2017\n",
    "\n",
    "@author: VANLOI\n",
    "\"\"\"\n",
    "NSLKDD = [\"Probe\", \"DoS\", \"R2L\", \"U2R\", \"NSLKDD\"]\n",
    "UNSW   = [\"Fuzzers\", \"Analysis\", \"Backdoor\", \"DoS_UNSW\", \"Exploits\", \"Generic\",\\\n",
    "            \"Reconnaissance\", \"Shellcode\", \"Worms\", \"UNSW\"]\n",
    "CTU13  = [\"CTU13_08\", \"CTU13_13\", \"CTU13_10\", \"CTU13_09\"]\n",
    "\n",
    "def hyper_parameters(data):\n",
    "    if (data == \"C-heart\"):\n",
    "        h_sizes = [10, 7, 4]\n",
    "\n",
    "    elif (data == \"ACA\"):\n",
    "        h_sizes = [11, 7, 4]\n",
    "\n",
    "    elif (data == \"WBC\"):\n",
    "        h_sizes = [7, 6, 4]\n",
    "\n",
    "    elif (data == \"WDBC\"):\n",
    "        h_sizes = [22, 14, 6]\n",
    "\n",
    "    elif (data in NSLKDD):\n",
    "        h_sizes = [85, 49, 12]\n",
    "\n",
    "    elif (data in UNSW):\n",
    "        h_sizes = [136, 75, 15]\n",
    "\n",
    "    #Table - 1\n",
    "    elif (data == \"GLASS\"):\n",
    "        h_sizes = [7, 6, 4]\n",
    "\n",
    "    elif (data == \"Ionosphere\"):\n",
    "        h_sizes = [23, 15, 6]\n",
    "\n",
    "\n",
    "    elif (data == \"PenDigits\"):\n",
    "        h_sizes = [12, 9, 5]\n",
    "\n",
    "    elif (data == \"Shuttle\"):\n",
    "        h_sizes = [7, 6, 4]\n",
    "\n",
    "    elif (data == \"WPBC\"):\n",
    "        h_sizes = [23, 15, 6]\n",
    "\n",
    "    #Table - 2\n",
    "    elif (data == \"Annthyroid\"):\n",
    "        h_sizes = [16, 10, 5]\n",
    "\n",
    "    elif (data == \"Arrhythmia\"):\n",
    "        h_sizes = [178, 98, 17]\n",
    "        #h_sizes = [138, 17]\n",
    "        #h_sizes = [50, 10, 2]\n",
    "\n",
    "    elif (data == \"Cardiotocography\"):\n",
    "        h_sizes = [16, 10, 5]\n",
    "\n",
    "    elif (data == \"Heartdisease\"):\n",
    "        h_sizes = [10, 7, 4]\n",
    "\n",
    "    elif (data == \"Hepatitis\"):\n",
    "        h_sizes = [14, 10, 5]\n",
    "\n",
    "    elif (data == \"InternetAds\"):\n",
    "        h_sizes = [1052, 546, 40]\n",
    "\n",
    "    elif (data == \"PageBlocks\"):\n",
    "        h_sizes = [8, 6, 4]\n",
    "\n",
    "    elif (data == \"Parkinson\"):\n",
    "        h_sizes = [16, 11, 5]\n",
    "\n",
    "    elif (data == \"Pima\"):\n",
    "        h_sizes = [6, 5, 3]\n",
    "\n",
    "    elif (data == \"Spambase\"):\n",
    "        h_sizes = [41, 24, 8]\n",
    "\n",
    "    elif (data == \"Wilt\"):\n",
    "        h_sizes = [4, 4, 3]\n",
    "\n",
    "\n",
    "    elif (data == \"waveform\"):\n",
    "        h_sizes = [14, 7, 5]\n",
    "\n",
    "    elif (data == \"CTU13_08\"):\n",
    "        h_sizes = [29, 18, 7]\n",
    "        #h_sizes = [27, 15, 2]\n",
    "\n",
    "    elif (data == \"CTU13_09\"):\n",
    "        h_sizes = [30, 18, 7]\n",
    "        #h_sizes = [28, 15, 2]\n",
    "\n",
    "    elif (data == \"CTU13_10\"):\n",
    "        h_sizes = [28, 17, 7]\n",
    "        #h_sizes = [26, 14, 2]\n",
    "\n",
    "    elif (data == \"CTU13_13\"):\n",
    "        h_sizes = [29, 18, 7]\n",
    "        #h_sizes = [27, 15, 2]\n",
    "\n",
    "    elif (data == \"CTU13_06\"):\n",
    "        h_sizes = [28, 17, 7]\n",
    "\n",
    "    elif (data == \"CTU13_07\"):\n",
    "        h_sizes = [25, 15, 6]\n",
    "\n",
    "    elif (data == \"CTU13_12\"):\n",
    "        h_sizes = [26, 17, 7]\n",
    "\n",
    "    return h_sizes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Mar 06 17:29:51 2017\n",
    "\n",
    "@author: VANLOI\n",
    "\"\"\"\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Mar 06 17:16:02 2017\n",
    "\n",
    "@author: VANLOI\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "__all__ = [\"LocalOutlierFactor\"]\n",
    "\n",
    "\n",
    "class LocalOutlierFactor(NeighborsBase, KNeighborsMixin, UnsupervisedMixin):\n",
    "    \"\"\"Unsupervised Outlier Detection using Local Outlier Factor (LOF)\n",
    "    The anomaly score of each sample is called Local Outlier Factor.\n",
    "    It measures the local deviation of density of a given sample with\n",
    "    respect to its neighbors.\n",
    "    It is local in that the anomaly score depends on how isolated the object\n",
    "    is with respect to the surrounding neighborhood.\n",
    "    More precisely, locality is given by k-nearest neighbors, whose distance\n",
    "    is used to estimate the local density.\n",
    "    By comparing the local density of a sample to the local densities of\n",
    "    its neighbors, one can identify samples that have a substantially lower\n",
    "    density than their neighbors. These are considered outliers.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_neighbors : int, optional (default=20)\n",
    "        Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
    "        If n_neighbors is larger than the number of samples provided,\n",
    "        all samples will be used.\n",
    "    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n",
    "        Algorithm used to compute the nearest neighbors:\n",
    "        - 'ball_tree' will use :class:`BallTree`\n",
    "        - 'kd_tree' will use :class:`KDTree`\n",
    "        - 'brute' will use a brute-force search.\n",
    "        - 'auto' will attempt to decide the most appropriate algorithm\n",
    "          based on the values passed to :meth:`fit` method.\n",
    "        Note: fitting on sparse input will override the setting of\n",
    "        this parameter, using brute force.\n",
    "    leaf_size : int, optional (default=30)\n",
    "        Leaf size passed to :class:`BallTree` or :class:`KDTree`. This can\n",
    "        affect the speed of the construction and query, as well as the memory\n",
    "        required to store the tree. The optimal value depends on the\n",
    "        nature of the problem.\n",
    "    p : integer, optional (default=2)\n",
    "        Parameter for the Minkowski metric from\n",
    "        :ref:`sklearn.metrics.pairwise.pairwise_distances`. When p = 1, this is\n",
    "        equivalent to using manhattan_distance (l1), and euclidean_distance\n",
    "        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
    "    metric : string or callable, default 'minkowski'\n",
    "        metric used for the distance computation. Any metric from scikit-learn\n",
    "        or scipy.spatial.distance can be used.\n",
    "        If 'precomputed', the training input X is expected to be a distance\n",
    "        matrix.\n",
    "        If metric is a callable function, it is called on each\n",
    "        pair of instances (rows) and the resulting value recorded. The callable\n",
    "        should take two arrays as input and return one value indicating the\n",
    "        distance between them. This works for Scipy's metrics, but is less\n",
    "        efficient than passing the metric name as a string.\n",
    "        Valid values for metric are:\n",
    "        - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
    "          'manhattan']\n",
    "        - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
    "          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
    "          'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto',\n",
    "          'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath',\n",
    "          'sqeuclidean', 'yule']\n",
    "        See the documentation for scipy.spatial.distance for details on these\n",
    "        metrics:\n",
    "        http://docs.scipy.org/doc/scipy/reference/spatial.distance.html\n",
    "    metric_params : dict, optional (default=None)\n",
    "        Additional keyword arguments for the metric function.\n",
    "    contamination : float in (0., 0.5), optional (default=0.1)\n",
    "        The amount of contamination of the data set, i.e. the proportion\n",
    "        of outliers in the data set. When fitting this is used to define the\n",
    "        threshold on the decision function.\n",
    "    n_jobs : int, optional (default=1)\n",
    "        The number of parallel jobs to run for neighbors search.\n",
    "        If ``-1``, then the number of jobs is set to the number of CPU cores.\n",
    "        Affects only :meth:`kneighbors` and :meth:`kneighbors_graph` methods.\n",
    "    Attributes\n",
    "    ----------\n",
    "    negative_outlier_factor_ : numpy array, shape (n_samples,)\n",
    "        The opposite LOF of the training samples. The lower, the more normal.\n",
    "        Inliers tend to have a LOF score close to 1, while outliers tend\n",
    "        to have a larger LOF score.\n",
    "        The local outlier factor (LOF) of a sample captures its\n",
    "        supposed 'degree of abnormality'.\n",
    "        It is the average of the ratio of the local reachability density of\n",
    "        a sample and those of its k-nearest neighbors.\n",
    "    n_neighbors_ : integer\n",
    "        The actual number of neighbors used for :meth:`kneighbors` queries.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Breunig, M. M., Kriegel, H. P., Ng, R. T., & Sander, J. (2000, May).\n",
    "           LOF: identifying density-based local outliers. In ACM sigmod record.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_neighbors=20, algorithm='auto', leaf_size=30,\n",
    "                 metric='minkowski', p=2, metric_params=None,\n",
    "                 contamination=0.1, n_jobs=1):\n",
    "                    self.n_neighbors=n_neighbors\n",
    "                    self.algorithm=algorithm\n",
    "                    self.leaf_size=leaf_size\n",
    "                    self.metric=metric\n",
    "                    self.p=p\n",
    "                    self.metric_params=metric_params\n",
    "                    self.n_jobs=n_jobs\n",
    "\n",
    "                    self.contamination = contamination\n",
    "\n",
    "    def fit_predict(self, X, y=None):\n",
    "        \"\"\"\"Fits the model to the training set X and returns the labels\n",
    "        (1 inlier, -1 outlier) on the training set according to the LOF score\n",
    "        and the contamination parameter.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features), default=None\n",
    "            The query sample or samples to compute the Local Outlier Factor\n",
    "            w.r.t. to the training samples.\n",
    "        Returns\n",
    "        -------\n",
    "        is_inlier : array, shape (n_samples,)\n",
    "            Returns -1 for anomalies/outliers and 1 for inliers.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.fit(X)._predict()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the model using X as training data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix, BallTree, KDTree}\n",
    "            Training data. If array or matrix, shape [n_samples, n_features],\n",
    "            or [n_samples, n_samples] if metric='precomputed'.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        if not (0. < self.contamination <= .5):\n",
    "            raise ValueError(\"contamination must be in (0, 0.5]\")\n",
    "\n",
    "        super(LocalOutlierFactor, self).fit(X)\n",
    "\n",
    "        n_samples = self._fit_X.shape[0]\n",
    "        if self.n_neighbors > n_samples:\n",
    "            warn(\"n_neighbors (%s) is greater than the \"\n",
    "                 \"total number of samples (%s). n_neighbors \"\n",
    "                 \"will be set to (n_samples - 1) for estimation.\"\n",
    "                 % (self.n_neighbors, n_samples))\n",
    "        self.n_neighbors_ = max(1, min(self.n_neighbors, n_samples - 1))\n",
    "\n",
    "        self._distances_fit_X_, _neighbors_indices_fit_X_ = (\n",
    "            self.kneighbors(None, n_neighbors=self.n_neighbors_))\n",
    "\n",
    "        self._lrd = self._local_reachability_density(\n",
    "            self._distances_fit_X_, _neighbors_indices_fit_X_)\n",
    "\n",
    "        # Compute lof score over training samples to define threshold_:\n",
    "        lrd_ratios_array = (self._lrd[_neighbors_indices_fit_X_] /\n",
    "                            self._lrd[:, np.newaxis])\n",
    "\n",
    "        self.negative_outlier_factor_ = -np.mean(lrd_ratios_array, axis=1)\n",
    "\n",
    "        self.threshold_ = -scoreatpercentile(\n",
    "            -self.negative_outlier_factor_, 100. * (1. - self.contamination))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _predict(self, X=None):\n",
    "        \"\"\"Predict the labels (1 inlier, -1 outlier) of X according to LOF.\n",
    "        If X is None, returns the same as fit_predict(X_train).\n",
    "        This method allows to generalize prediction to new observations (not\n",
    "        in the training set). As LOF originally does not deal with new data,\n",
    "        this method is kept private.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features), default=None\n",
    "            The query sample or samples to compute the Local Outlier Factor\n",
    "            w.r.t. to the training samples. If None, makes prediction on the\n",
    "            training data without considering them as their own neighbors.\n",
    "        Returns\n",
    "        -------\n",
    "        is_inlier : array, shape (n_samples,)\n",
    "            Returns -1 for anomalies/outliers and +1 for inliers.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, [\"threshold_\", \"negative_outlier_factor_\",\n",
    "                               \"n_neighbors_\", \"_distances_fit_X_\"])\n",
    "\n",
    "        if X is not None:\n",
    "            X = check_array(X, accept_sparse='csr')\n",
    "            is_inlier = np.ones(X.shape[0], dtype=int)\n",
    "            is_inlier[self._decision_function(X) <= self.threshold_] = -1\n",
    "        else:\n",
    "            is_inlier = np.ones(self._fit_X.shape[0], dtype=int)\n",
    "            is_inlier[self.negative_outlier_factor_ <= self.threshold_] = -1\n",
    "\n",
    "        return is_inlier\n",
    "\n",
    "    def _decision_function(self, X):\n",
    "        \"\"\"Opposite of the Local Outlier Factor of X (as bigger is better,\n",
    "        i.e. large values correspond to inliers).\n",
    "        The argument X is supposed to contain *new data*: if X contains a\n",
    "        point from training, it consider the later in its own neighborhood.\n",
    "        Also, the samples in X are not considered in the neighborhood of any\n",
    "        point.\n",
    "        The decision function on training data is available by considering the\n",
    "        opposite of the negative_outlier_factor_ attribute.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The query sample or samples to compute the Local Outlier Factor\n",
    "            w.r.t. the training samples.\n",
    "        Returns\n",
    "        -------\n",
    "        opposite_lof_scores : array, shape (n_samples,)\n",
    "            The opposite of the Local Outlier Factor of each input samples.\n",
    "            The lower, the more abnormal.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, [\"threshold_\", \"negative_outlier_factor_\",\n",
    "                               \"_distances_fit_X_\"])\n",
    "\n",
    "        X = check_array(X, accept_sparse='csr')\n",
    "\n",
    "        distances_X, neighbors_indices_X = (\n",
    "            self.kneighbors(X, n_neighbors=self.n_neighbors_))\n",
    "        X_lrd = self._local_reachability_density(distances_X,\n",
    "                                                 neighbors_indices_X)\n",
    "\n",
    "        lrd_ratios_array = (self._lrd[neighbors_indices_X] /\n",
    "                            X_lrd[:, np.newaxis])\n",
    "\n",
    "        # as bigger is better:\n",
    "        return -np.mean(lrd_ratios_array, axis=1)\n",
    "\n",
    "    def _local_reachability_density(self, distances_X, neighbors_indices):\n",
    "        \"\"\"The local reachability density (LRD)\n",
    "        The LRD of a sample is the inverse of the average reachability\n",
    "        distance of its k-nearest neighbors.\n",
    "        Parameters\n",
    "        ----------\n",
    "        distances_X : array, shape (n_query, self.n_neighbors)\n",
    "            Distances to the neighbors (in the training samples `self._fit_X`)\n",
    "            of each query point to compute the LRD.\n",
    "        neighbors_indices : array, shape (n_query, self.n_neighbors)\n",
    "            Neighbors indices (of each query point) among training samples\n",
    "            self._fit_X.\n",
    "        Returns\n",
    "        -------\n",
    "        local_reachability_density : array, shape (n_samples,)\n",
    "            The local reachability density of each sample.\n",
    "        \"\"\"\n",
    "        dist_k = self._distances_fit_X_[neighbors_indices,\n",
    "                                        self.n_neighbors_ - 1]\n",
    "        reach_dist_array = np.maximum(distances_X, dist_k)\n",
    "\n",
    "        #  1e-10 to avoid `nan' when when nb of duplicates > n_neighbors_:\n",
    "        return 1. / (np.mean(reach_dist_array, axis=1) + 1e-10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Feb 25 17:47:44 2016\n",
    "\n",
    "@author: caoloi\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "seed = 0\n",
    "\n",
    "\"Normalize training and testing sets\"\n",
    "def normalize_data(train_X, test_X, scale):\n",
    "    if ((scale == \"standard\") | (scale == \"maxabs\") | (scale == \"minmax\")):\n",
    "        if (scale == \"standard\"):\n",
    "            scaler = preprocessing.StandardScaler()\n",
    "        elif (scale == \"maxabs\"):\n",
    "            scaler = preprocessing.MaxAbsScaler()\n",
    "        elif (scale == \"minmax\"):\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "        scaler.fit(train_X)\n",
    "        train_X = scaler.transform(train_X)\n",
    "        test_X  = scaler.transform(test_X)\n",
    "    else:\n",
    "        print (\"No scaler\")\n",
    "    return train_X, test_X\n",
    "\n",
    "\"***********************Pre-processing NSL-KDD dataset************************\"\n",
    "def process_KDD(group_attack):\n",
    "    d0 = np.genfromtxt(\"Data/NSLKDD/NSLKDD_Train.csv\", delimiter=\",\")\n",
    "    d1 = np.genfromtxt(\"Data/NSLKDD/NSLKDD_Test.csv\", delimiter=\",\")\n",
    "    # Train: 67343, Test normal: 9711, Dos: 7458, R2L   2887, U2R: 67, Probe 2422\n",
    "\n",
    "    n_train = 6734\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(d0)\n",
    "    np.random.shuffle(d1)\n",
    "\n",
    "    \"Pre-processing training set\"\n",
    "    dy = d0[:,-1]                       # put labels(the last column) to dy\n",
    "    train_X = d0[(dy==0)]               # Normal(class 0) to train_X\n",
    "    train_X = train_X[:,0:-1]           # discard the last column (labels)\n",
    "    train_X = train_X[:n_train]         # downsample training set\n",
    "\n",
    "    \"Pre-processing Testing set\"\n",
    "    dy = d1[:,-1]                       # put labels to dy\n",
    "    dX = d1[:,0:-1]                     # put data to dX without last column (labels)\n",
    "\n",
    "    dX0 = dX[(dy==0)]                   # Normal, class 0\n",
    "    if (group_attack == \"Probe\"):\n",
    "        dX1 = dX[(dy==1)]\n",
    "    elif (group_attack == \"DoS\"):\n",
    "        dX1 = dX[(dy==2)]\n",
    "    elif (group_attack == \"R2L\"):\n",
    "        dX1 = dX[(dy==3)]\n",
    "    elif (group_attack == \"U2R\"):\n",
    "        dX1 = dX[(dy==4)]\n",
    "    elif (group_attack == \"NSLKDD\"):\n",
    "        dX1 = dX[(dy>0)]\n",
    "    else:\n",
    "        print (\"No group of attack\")\n",
    "\n",
    "    test_X0 = dX0                       #normal test\n",
    "    test_X1 = dX1                       #anomaly test\n",
    "    #normal and anomaly test\n",
    "    test_X = np.concatenate((test_X0, test_X1))\n",
    "\n",
    "    #Create label for normal and anomaly test examples, and then combine two sets\n",
    "    test_y0 = np.full((len(test_X0)), False, dtype=bool)\n",
    "    test_y1 = np.full((len(test_X1)), True,  dtype=bool)\n",
    "    test_y =  np.concatenate((test_y0, test_y1))\n",
    "    #create binary label (1-normal, 0-anomaly) for compute AUC later\n",
    "    actual = (~test_y).astype(np.int)\n",
    "\n",
    "    return train_X, test_X, actual\n",
    "\n",
    "\"***************************Processing UNSW Dataset***************************\"\n",
    "def process_UNSW(group_attack):\n",
    "\n",
    "    \"\"\"group_attack: {'Fuzzers': 1, 'Exploits': 5, 'Normal': 0, 'Generic': 6, 'Worms': 9,\n",
    "    'Analysis': 2, 'Backdoor': 3, 'DoS': 4, 'Reconnaissance': 7, 'Shellcode': 8}\"\"\"\n",
    "    #DoS --> DoS_UNSW to avoid the conflict with DoS in NSL-KDD\n",
    "    d0 = np.genfromtxt(\"Data/UNSW/UNSW_Train.csv\", delimiter=\",\")\n",
    "    d1 = np.genfromtxt(\"Data/UNSW/UNSW_Test.csv\", delimiter=\",\")\n",
    "    #Training set: 175341 (56000 normal, 119341 anomaly)\n",
    "    #Testing set:   82332 (37000 normal,  45332 anomaly)\n",
    "\n",
    "    n_train = 5600         #downsample training set 10%\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(d0)\n",
    "    np.random.shuffle(d1)\n",
    "\n",
    "    \"Pre-processing training set\"\n",
    "    dy = d0[:,-1]                       # put labels(the last column) to dy\n",
    "    train_X = d0[(dy==0)]               # Normal(class 0) to train_X\n",
    "    train_X = train_X[:,0:-1]           # discard the last column (labels)\n",
    "    train_X = train_X[:n_train]         # downsample training set\n",
    "\n",
    "    \"Pre-processing Testing set\"\n",
    "    dy = d1[:,-1]                       # put labels to dy\n",
    "    dX = d1[:,0:-1]                     # put data to dX without last column (labels)\n",
    "\n",
    "\n",
    "    \"\"\"group_attack: {'Fuzzers': 1, 'Exploits': 5, 'Normal': 0, 'Generic': 6, 'Worms': 9,\n",
    "    'Analysis': 2, 'Backdoor': 3, 'DoS': 4, 'Reconnaissance': 7, 'Shellcode': 8}\"\"\"\n",
    "\n",
    "    dX0 = dX[(dy==0)]                   # Normal, class 0\n",
    "\n",
    "    if (group_attack == \"Fuzzers\"):\n",
    "        dX1 = dX[(dy==1)]\n",
    "    elif (group_attack == \"Analysis\"):\n",
    "        dX1 = dX[(dy==2)]\n",
    "    elif (group_attack == \"Backdoor\"):\n",
    "        dX1 = dX[(dy==3)]\n",
    "    elif (group_attack == \"DoS_UNSW\"):\n",
    "        dX1 = dX[(dy==4)]\n",
    "    elif (group_attack == \"Exploits\"):\n",
    "        dX1 = dX[(dy==5)]\n",
    "    elif (group_attack == \"Generic\"):\n",
    "        dX1 = dX[(dy==6)]\n",
    "    elif (group_attack == \"Reconnaissance\"):\n",
    "        dX1 = dX[(dy==7)]\n",
    "    elif (group_attack == \"Shellcode\"):\n",
    "        dX1 = dX[(dy==8)]\n",
    "    elif (group_attack == \"Worms\"):\n",
    "        dX1 = dX[(dy==9)]\n",
    "    elif (group_attack == \"UNSW\"):\n",
    "        dX1 = dX[(dy>0)]\n",
    "    else:\n",
    "        print (\"No group of attack\")\n",
    "\n",
    "    test_X0 = dX0                       #normal test\n",
    "    test_X1 = dX1                       #anomaly test\n",
    "    #normal and anomaly test\n",
    "    test_X = np.concatenate((test_X0, test_X1))\n",
    "\n",
    "    #Create label for normal and anomaly test examples, and then combine two sets\n",
    "    test_y0 = np.full((len(test_X0)), False, dtype=bool)\n",
    "    test_y1 = np.full((len(test_X1)), True,  dtype=bool)\n",
    "    test_y =  np.concatenate((test_y0, test_y1))\n",
    "    #create binary label (1-normal, 0-anomaly) for compute AUC later\n",
    "    actual = (~test_y).astype(np.int)\n",
    "\n",
    "    return train_X, test_X, actual\n",
    "\n",
    "\"*************************Read data from CSV file*****************************\"\n",
    "def process_PenDigits():\n",
    "    #16 real-values + 1 class (normal: class 0, anomaly: class 2 or 1-9)\n",
    "    #each class is equaly to each others in train and test set\n",
    "    d0 = np.genfromtxt(\"Data/pendigits_train.csv\", delimiter=\",\")\n",
    "    d1 = np.genfromtxt(\"Data/pendigits_test.csv\", delimiter=\",\")\n",
    "\n",
    "    # shuffle\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(d0)\n",
    "    np.random.shuffle(d1)\n",
    "\n",
    "    \"Pre-processing training set\"\n",
    "    dy = d0[:,-1]                       # put labels(the last column) to dy\n",
    "    train_X = d0[(dy==0)]               # Normal(class 0) to train_X\n",
    "    train_X = train_X[:,0:-1]           # discard the last column (labels)\n",
    "\n",
    "    \"Pre-processing Testing set\"\n",
    "    dy = d1[:,-1]                       # put labels to dy\n",
    "    dX = d1[:,0:-1]                     # put data to dX without last column (labels)\n",
    "\n",
    "    dX0 = dX[(dy == 0)]                  # Normal, class 0\n",
    "    dX1 = dX[(dy == 2)]                   # Anomaly, class 2 (or 1-9)\n",
    "\n",
    "    test_X0 = dX0                       #normal test\n",
    "    test_X1 = dX1                       #anomaly test\n",
    "    #normal and anomaly test\n",
    "    test_X = np.concatenate((test_X0, test_X1))\n",
    "\n",
    "    #Create label for normal and anomaly test examples, and then combine two sets\n",
    "    test_y0 = np.full((len(test_X0)), False, dtype=bool)\n",
    "    test_y1 = np.full((len(test_X1)), True,  dtype=bool)\n",
    "    test_y =  np.concatenate((test_y0, test_y1))\n",
    "    #create binary label (1-normal, 0-anomaly) for compute AUC later\n",
    "    actual = (~test_y).astype(np.int)\n",
    "\n",
    "    return train_X, test_X, actual\n",
    "\n",
    "\n",
    "\"*************************Read Shuttle data*****************************\"\n",
    "def process_Shuttle():\n",
    "    #9 attributes + 1 class (1-7), train_set: 43500(34108 normal), test_set: 14500\n",
    "    #Consider normal: class 1, anomaly: classes 2 - 7\n",
    "    d0 = np.genfromtxt(\"Data/shuttle_train.csv\", delimiter=\",\")\n",
    "    d1 = np.genfromtxt(\"Data/shuttle_test.csv\", delimiter=\",\")\n",
    "    n_train = 3410  #downsample 10%\n",
    "    # shuffle\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(d0)\n",
    "    np.random.shuffle(d1)\n",
    "\n",
    "    \"Pre-processing training set\"\n",
    "    dy = d0[:,-1]                       # put labels(the last column) to dy\n",
    "    train_X = d0[(dy==1)]               # Normal(class 1) to train_X\n",
    "    train_X = train_X[:,0:-1]           # discard the last column (labels)\n",
    "\n",
    "    train_X = train_X[:n_train]\n",
    "\n",
    "    \"Pre-processing Testing set\"\n",
    "    dy = d1[:,-1]                       # put labels to dy\n",
    "    dX = d1[:,0:-1]                     # put data to dX without last column (labels)\n",
    "\n",
    "    dX0 = dX[(dy==1)]                   # Normal, class 1\n",
    "    dX1 = dX[(dy>1)]                    # Anomaly, class 2-7\n",
    "\n",
    "    test_X0 = dX0                       #normal test\n",
    "    test_X1 = dX1                       #anomaly test\n",
    "    #normal and anomaly test\n",
    "    test_X = np.concatenate((test_X0, test_X1))\n",
    "\n",
    "    #Create label for normal and anomaly test examples, and then combine two sets\n",
    "    test_y0 = np.full((len(test_X0)), False, dtype=bool)\n",
    "    test_y1 = np.full((len(test_X1)), True,  dtype=bool)\n",
    "    test_y =  np.concatenate((test_y0, test_y1))\n",
    "    actual = (~test_y).astype(np.int)\n",
    "\n",
    "    return train_X, test_X, actual\n",
    "\n",
    "\n",
    "\"*************************Read Annthyroid data*****************************\"\n",
    "def process_Annthyroid():\n",
    "    #21 attributes + 1 class (3 - healthy, 1-2 - hypothyroidism)\n",
    "    #Train_set (3488 1-heathy, 284 hypo), Test_set(3178-heathy, 250-hypo)\n",
    "    # 3 - normal, 1 - hyperfunction and 2 - subnormal functioning\n",
    "    d0 = np.genfromtxt(\"Data/annthyroid_train.csv\", delimiter=\",\")\n",
    "    d1 = np.genfromtxt(\"Data/annthyroid_test.csv\", delimiter=\",\")\n",
    "    n_train = 3488\n",
    "    # shuffle\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(d0)\n",
    "    np.random.shuffle(d1)\n",
    "\n",
    "    \"Pre-processing training set\"\n",
    "    dy = d0[:,-1]                       # put labels(the last column) to dy\n",
    "    train_X = d0[(dy==3)]               # Normal(class 3) to train_X\n",
    "    train_X = train_X[:,0:-1]           # discard the last column (labels)\n",
    "    train_X = train_X[:n_train]\n",
    "\n",
    "    \"Pre-processing Testing set\"\n",
    "    dy = d1[:,-1]                       # put labels to dy\n",
    "    dX = d1[:,0:-1]                     # put data to dX without last column (labels)\n",
    "\n",
    "    dX0 = dX[(dy==3)]                   # Normal, class 1\n",
    "    dX1 = dX[(dy<3)]                    # Anomaly, class 1, 2. better if choosing only class 1\n",
    "\n",
    "    test_X0 = dX0                       #normal test\n",
    "    test_X1 = dX1                       #anomaly test\n",
    "    #normal and anomaly test\n",
    "    test_X = np.concatenate((test_X0, test_X1))\n",
    "\n",
    "    #Create label for normal and anomaly test examples, and then combine two sets\n",
    "    test_y0 = np.full((len(test_X0)), False, dtype=bool)\n",
    "    test_y1 = np.full((len(test_X1)), True,  dtype=bool)\n",
    "    test_y =  np.concatenate((test_y0, test_y1))\n",
    "    actual = (~test_y).astype(np.int)\n",
    "\n",
    "    return train_X, test_X, actual\n",
    "\n",
    "\"*************************Read wilt data*****************************\"\n",
    "def process_wilt():\n",
    "    #5 attributes + 1 class(0,1), Train_set(4265 cover land, 74 diseased tree)\n",
    "    #Test_set (313 cover land, 187 diseased tree)\n",
    "    d0 = np.genfromtxt(\"Data/wilt_train.csv\", delimiter=\",\")\n",
    "    d1 = np.genfromtxt(\"Data/wilt_test.csv\", delimiter=\",\")\n",
    "    n_train = 4265\n",
    "    # shuffle\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(d0)\n",
    "    np.random.shuffle(d1)\n",
    "\n",
    "    \"Pre-processing training set\"\n",
    "    dy = d0[:,-1]                       # put labels(the last column) to dy\n",
    "    train_X = d0[(dy==0)]               # Normal(class 1) to train_X\n",
    "    train_X = train_X[:,0:-1]           # discard the last column (labels)\n",
    "    train_X = train_X[:n_train]         #downsample\\\n",
    "\n",
    "\n",
    "    \"Pre-processing Testing set\"\n",
    "    dX = d1[:,0:-1]                     # put data to dX without last column (labels)\n",
    "    dy = d1[:,-1]                       # put labels to dy\n",
    "\n",
    "    dX0 = dX[(dy==0)]                   # Normal, class 0 (cover land)\n",
    "    dX1 = dX[(dy==1)]                   # Anomaly, class 1 (diseased tree)\n",
    "\n",
    "    test_X0 = dX0                       #normal test\n",
    "    test_X1 = dX1                       #anomaly test\n",
    "    #normal and anomaly test\n",
    "    test_X = np.concatenate((test_X0, test_X1))\n",
    "\n",
    "    #Create label for normal and anomaly test examples, and then combine two sets\n",
    "    test_y0 = np.full((len(test_X0)), False, dtype=bool)\n",
    "    test_y1 = np.full((len(test_X1)), True,  dtype=bool)\n",
    "    test_y =  np.concatenate((test_y0, test_y1))\n",
    "    actual = (~test_y).astype(np.int)\n",
    "\n",
    "    return train_X, test_X, actual\n",
    "\n",
    "\"*****************************Load dataset*****************************\"\n",
    "def load_data(dataset):\n",
    "    NSLKDD = [\"Probe\", \"DoS\", \"R2L\", \"U2R\", \"NSLKDD\"]\n",
    "    UNSW   = [\"Fuzzers\", \"Analysis\", \"Backdoor\", \"DoS_UNSW\", \"Exploits\", \"Generic\",\\\n",
    "            \"Reconnaissance\", \"Shellcode\", \"Worms\", \"UNSW\"]\n",
    "    CTU13  = [\"CTU13_06\",\"CTU13_07\",\"CTU13_08\",\"CTU13_09\",\"CTU13_10\",\"CTU13_12\",\"CTU13_13\"]\n",
    "\n",
    "    if (dataset == \"C-heart\"):\n",
    "        d = np.genfromtxt(\"Data/C-heart.csv\", delimiter=\",\")\n",
    "        label_threshold = 0\n",
    "        # 13 attributes + 1 class [0 - Level0(164); level 1,2,3,4 - (139), 6 missing)\n",
    "        #Some features may be CATEGORICAL, don't need to preprocessing\n",
    "\n",
    "    elif (dataset == \"ACA\"):\n",
    "        d = np.genfromtxt(\"Data/australian.csv\", delimiter=\",\")\n",
    "        label_threshold = 0\n",
    "        # 14 feature + 1 class [ 0 (383 normal), 1 (307 anomaly)]\n",
    "        # 8 CATEGORICAL FEATURES NEED TO BE PREPROCESSED\n",
    "\n",
    "    elif (dataset == \"WBC\"):\n",
    "        d = np.genfromtxt(\"Data/wbc.csv\", delimiter=\",\")\n",
    "        label_threshold = 2\n",
    "        #9 real-value + 1 class[2-benign(458); 4-malignant(241)], 16 missing\n",
    "\n",
    "    elif (dataset == \"WDBC\"):\n",
    "        d = np.genfromtxt(\"Data/wdbc.csv\", delimiter=\",\")\n",
    "        label_threshold = 2\n",
    "        # 30 real-value + 1 class [2 - benign(357); 4 - malignant(212)]\n",
    "\n",
    "    elif (dataset in NSLKDD):\n",
    "        train_X, test_X, actual = process_KDD(dataset)\n",
    "        return train_X, test_X, actual\n",
    "        #Tree CATEGORICAL FEATURES NEED TO BE PREPROCESSED\n",
    "\n",
    "    elif (dataset in UNSW):\n",
    "        train_X, test_X, actual = process_UNSW(dataset)\n",
    "        return train_X, test_X, actual\n",
    "\n",
    "    #**************************** TABLE - 1 **********************************\n",
    "    elif (dataset == \"GLASS\"):\n",
    "        d = np.genfromtxt(\"Data/glass.csv\", delimiter=\",\")\n",
    "        label_threshold = 4\n",
    "        # 9 attributes + 1 class [1-4 - 163 window glass (normal); 5-7 - 51 Non-window glass (anomaly)]\n",
    "\n",
    "    elif (dataset == \"Ionosphere\"):\n",
    "        d = np.genfromtxt(\"Data/ionosphere.csv\", delimiter=\",\")\n",
    "        label_threshold = 0\n",
    "        d = d[:,2:]\n",
    "        # Ignore the first and second features, using 32 features\n",
    "        # 34 attributes + 1 class [0 - 225 good (normal); 1 - 126 bad (anomaly)]\n",
    "\n",
    "    elif (dataset == \"PenDigits\"):\n",
    "        train_X, test_X, actual = process_PenDigits()\n",
    "        #16 real-value + 1 class attribute (0 as Normal - 2 ( or 1,2,3,4,5,6,7,8,9)\n",
    "        #Training 780 normal, testing 363 normal and 364 anomaly\n",
    "        return train_X, test_X, actual\n",
    "\n",
    "    elif (dataset == \"Shuttle\"):\n",
    "        train_X, test_X, actual = process_Shuttle()\n",
    "        #9 real-value + 1 class (1 as Normal -  class 2-7 as anomaly)\n",
    "        #train_set: 43500, test_set: 14500\n",
    "        return train_X, test_X, actual\n",
    "\n",
    "    elif (dataset == \"WPBC\"):\n",
    "        d = np.genfromtxt(\"Data/wpbc.csv\", delimiter=\",\")\n",
    "        label_threshold = 0\n",
    "        d = d[:,1:]   #remove ID feature\n",
    "        # 32 attributes + class [0 - 151 nonrecur (normal); 1 - 47 recur (anomaly)], 4 missing\n",
    "\n",
    "\n",
    "    #**************************** TABLE - 2 **********************************\n",
    "    elif (dataset == \"Annthyroid\"):\n",
    "        train_X, test_X, actual = process_Annthyroid()\n",
    "        #21 attributes + 1 class(Class 3 as Normal -  class 1, 2 as anomaly)\n",
    "        #Training 3488 normal, testing 3178 normal and 250 anomaly\n",
    "        return train_X, test_X, actual\n",
    "\n",
    "    elif (dataset ==\"Arrhythmia\"):\n",
    "        d = np.genfromtxt(\"Data/arrhythmia.csv\", delimiter=\",\")\n",
    "        # 452, 245 normal (class 1), 207 anomaly (classes 2 - 16)\n",
    "        # 279 attributes, 19 attributes have value 0, 1 attribute has many missing data.\n",
    "        label_threshold = 1\n",
    "\n",
    "    elif (dataset ==\"Cardiotocography\"):\n",
    "        d = np.genfromtxt(\"Data/Cardiotocography.csv\", delimiter=\",\")\n",
    "        # 21, 1655 normal (class 1), 471 anomaly (classes 2, 3)\n",
    "        label_threshold = 1\n",
    "\n",
    "    elif (dataset ==\"Heartdisease\"):\n",
    "        d = np.genfromtxt(\"Data/heartdisease.csv\", delimiter=\",\")\n",
    "        # 270 instances: 150 absence (1-normal) and 120 presence (2-anomaly)\n",
    "        #13 attributes including some: ORDERED and NOMINAL features\n",
    "        label_threshold = 1\n",
    "\n",
    "    elif (dataset ==\"Hepatitis\"):\n",
    "        d = np.genfromtxt(\"Data/hepatitis.csv\", delimiter=\",\")\n",
    "        # 155 instances, 19 attributes + class label( 2- Live 123, 1-die 32)\n",
    "        #(remove missing: remain class 2: 67, class 1: 13)\n",
    "        label_threshold = 2\n",
    "\n",
    "    elif (dataset ==\"InternetAds\"):\n",
    "        d = np.genfromtxt(\"Data/internet-ad.csv\", delimiter=\",\")\n",
    "        # 3264 instances, 1558 attributes + class(0: nonad, 1: Ads), many missing\n",
    "        label_threshold = 0\n",
    "\n",
    "    elif (dataset ==\"PageBlocks\"):\n",
    "        d = np.genfromtxt(\"Data/page-blocks.csv\", delimiter=\",\")\n",
    "        # 5473 instances, 10 attributes + class(1 (4913): text, 2-5: hiriz line (329), graphic (28),\n",
    "        # line (88) or picture (115)\n",
    "        label_threshold = 1\n",
    "\n",
    "    elif (dataset ==\"Parkinson\"):\n",
    "        d = np.genfromtxt(\"Data/parkinsons.csv\", delimiter=\",\")\n",
    "        # 195 instances: 48 Healthy (0), 147 Parkinson (1), 22 real-value\n",
    "        label_threshold = 0\n",
    "\n",
    "    elif (dataset ==\"Pima\"):\n",
    "        d = np.genfromtxt(\"Data/pima.csv\", delimiter=\",\")\n",
    "        # 768,  500 normal (class 0), 268 diabetes (class 1)\n",
    "        # 8 real, integer - values attributes\n",
    "        label_threshold = 0\n",
    "\n",
    "    elif (dataset ==\"Spambase\"):\n",
    "        d = np.genfromtxt(\"Data/spambase.csv\", delimiter=\",\")\n",
    "        # 4601,  2788 non-spam (normal, 0), 1813 spam (anomaly, 1), 57 real-values\n",
    "        label_threshold = 0\n",
    "\n",
    "    elif (dataset == \"Wilt\"):\n",
    "        train_X, test_X, actual = process_wilt()\n",
    "        #5 attributes + 1 class attribute, (0) Non-wilt as Normal - (1) wilt as anomaly\n",
    "        #Training 4388 ( 4265 - normal, 74 anomaly), testing 500 (313 - normal, 187 - anomaly)\n",
    "        return train_X, test_X, actual\n",
    "\n",
    "\n",
    "    elif (dataset ==\"CTU13_08\"):\n",
    "        d = np.genfromtxt(\"Data/CTU13/CTU13_08.csv\", delimiter=\",\")\n",
    "        label_threshold = 1\n",
    "\n",
    "    elif (dataset ==\"CTU13_09\"):\n",
    "        d = np.genfromtxt(\"Data/CTU13/CTU13_09.csv\", delimiter=\",\")\n",
    "        label_threshold = 1\n",
    "\n",
    "    elif (dataset ==\"CTU13_10\"):\n",
    "        d = np.genfromtxt(\"Data/CTU13/CTU13_10.csv\", delimiter=\",\")\n",
    "        label_threshold = 1\n",
    "\n",
    "    elif (dataset ==\"CTU13_13\"):\n",
    "        d = np.genfromtxt(\"Data/CTU13/CTU13_13.csv\", delimiter=\",\")\n",
    "        label_threshold = 1\n",
    "\n",
    "\n",
    "    elif (dataset ==\"CTU13_06\"):\n",
    "        d = np.genfromtxt(\"Data/CTU13/CTU13_06.csv\", delimiter=\",\")\n",
    "        label_threshold = 1\n",
    "\n",
    "    elif (dataset ==\"CTU13_07\"):\n",
    "        d = np.genfromtxt(\"Data/CTU13/CTU13_07.csv\", delimiter=\",\")\n",
    "        label_threshold = 1\n",
    "\n",
    "    elif (dataset ==\"CTU13_12\"):\n",
    "        d = np.genfromtxt(\"Data/CTU13/CTU13_12.csv\", delimiter=\",\")\n",
    "        label_threshold = 1\n",
    "\n",
    "    else:\n",
    "        print (\"Incorrect data\")\n",
    "\n",
    "    \"*************************Chosing dataset*********************************\"\n",
    "    d = d[~np.isnan(d).any(axis=1)]    #discard the '?' values\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(d)\n",
    "\n",
    "    dX = d[:,0:-1]              #put data to dX without the last column (labels)\n",
    "    dy = d[:,-1]                #put label to dy\n",
    "\n",
    "    if (dataset ==\"Hepatitis\"):\n",
    "        dy = dy < label_threshold\n",
    "    else:\n",
    "        dy = dy > label_threshold\n",
    "                                # dy=True with anomaly labels\n",
    "                                # separate into normal and anomaly\n",
    "    dX0 = dX[~dy]               # Normal data\n",
    "    dX1 = dX[dy]                # Anomaly data\n",
    "    dy0 = dy[~dy]               # Normal label\n",
    "    dy1 = dy[dy]                # Anomaly label\n",
    "\n",
    "    #print(\"Normal: %d Anomaly %d\" %(len(dX0), len(dX1)))\n",
    "    if (dataset in CTU13):\n",
    "        split = 0.4             #split 40% for training, 60% for testing\n",
    "    else:\n",
    "        split = 0.8             #split 80% for training, 20% for testing\n",
    "\n",
    "    idx0  = int(split * len(dX0))\n",
    "    idx1  = int(split * len(dX1))\n",
    "\n",
    "    train_X = dX0[:idx0]        # train_X is 80% of the normal class\n",
    "\n",
    "    # test set is the other half of the normal class and all of the anomaly class\n",
    "    test_X = np.concatenate((dX0[idx0:], dX1[idx1:]))  # 30% of normal and 30% of anomaly\n",
    "    test_y = np.concatenate((dy0[idx0:], dy1[idx1:]))  # 30% of normal and 30% of anomaly label\n",
    "    #conver test_y into 1 or 0 for computing AUC later\n",
    "    actual = (~test_y).astype(np.int)\n",
    "\n",
    "    return train_X, test_X, actual"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Feb 25 18:06:19 2016\n",
    "\n",
    "@author: caoloi\n",
    "\"\"\"\n",
    "\n",
    "\"******************************* CENTROID *************************************\"\n",
    "class CentroidBasedOneClassClassifier:\n",
    "    def __init__(self, threshold = 0.95, metric=\"euclidean\", scale = \"standard\"):\n",
    "\n",
    "        self.threshold = threshold\n",
    "        \"\"\"only CEN used StandardScaler because the centroid of training set need\n",
    "        to be move to origin\"\"\"\n",
    "        self.scaler = preprocessing.StandardScaler()\n",
    "        self.metric = metric\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.scaler.fit(X)\n",
    "        X = self.scaler.transform(X)\n",
    "        # because we are using StandardScaler, the centroid is a\n",
    "        # vector of zeros, but we save it in shape (1, n) to allow\n",
    "        # cdist to work happily later.\n",
    "        self.centroid = np.zeros((1, X.shape[1]))\n",
    "        # no need to scale again\n",
    "        dists = self.get_density(X, scale=False)\n",
    "        # transform relative threshold (eg 95%) to absolute\n",
    "        self.abs_threshold = np.percentile(dists, 100 * self.threshold)\n",
    "    #It is actually the mean of the distances from each points in training set\n",
    "    #to the centroid zero.\n",
    "    def get_density(self, X, scale=True):\n",
    "        if scale:\n",
    "            X = self.scaler.transform(X)\n",
    "        dists = scipy.spatial.distance.cdist(X, self.centroid, metric=self.metric)\n",
    "        dists = np.mean(dists, axis=1)\n",
    "        return dists\n",
    "\n",
    "    def predict(self, X):\n",
    "        dists = self.get_density(X)\n",
    "        return dists > self.abs_threshold\n",
    "\n",
    "\"****************** CENTROID WITHOUT STANDARD SCALER *************************\"\n",
    "class Centroid_Classifier:\n",
    "    def __init__(self, threshold = 0.95, metric=\"euclidean\"):\n",
    "\n",
    "        self.threshold = threshold\n",
    "        \"\"\"only CEN used StandardScaler because the centroid of training set need\n",
    "        to be move to origin\"\"\"\n",
    "        #self.scaler = preprocessing.StandardScaler()\n",
    "        self.metric = metric\n",
    "\n",
    "    def fit(self, X):\n",
    "        #self.scaler.fit(X)\n",
    "        #X = self.scaler.transform(X)\n",
    "        # because we are using StandardScaler, the centroid is a\n",
    "        # vector of zeros, but we save it in shape (1, n) to allow\n",
    "        # cdist to work happily later.\n",
    "        self.centroid = np.zeros((1, X.shape[1]))\n",
    "        # no need to scale again\n",
    "        dists = self.get_density(X)\n",
    "        # transform relative threshold (eg 95%) to absolute\n",
    "        self.abs_threshold = np.percentile(dists, 100 * self.threshold)\n",
    "    #It is actually the mean of the distances from each points in training set\n",
    "    #to the centroid zero.\n",
    "    def get_density(self, X):\n",
    "        #if scale:\n",
    "        #    X = self.scaler.transform(X)\n",
    "        dists = scipy.spatial.distance.cdist(X, self.centroid, metric=self.metric)\n",
    "        dists = np.mean(dists, axis=1)\n",
    "        return dists\n",
    "\n",
    "    def predict(self, X):\n",
    "        dists = self.get_density(X)\n",
    "        return dists > self.abs_threshold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"************************** NEGATIVE DISTANCE ********************************\"\n",
    "class NegativeMeanDistance:\n",
    "    def __init__(self, metric=\"euclidean\"):\n",
    "        self.metric = metric\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X = X\n",
    "\n",
    "    def score_samples(self, X):\n",
    "        dists = scipy.spatial.distance.cdist(X, self.X, metric=self.metric)\n",
    "        return -np.mean(dists, axis=1)\n",
    "\n",
    "\n",
    "\"*************************** DENSITY *****************************************\"\n",
    "class DensityBasedOneClassClassifier:\n",
    "    def __init__(self, threshold=0.95,\n",
    "                 kernel=\"gaussian\",\n",
    "                 bandwidth=1.0,\n",
    "                 metric=\"euclidean\",\n",
    "                 should_downsample=False,\n",
    "                 downsample_count=1000,\n",
    "                 scale = \"standard\"):\n",
    "\n",
    "        self.should_downsample = should_downsample\n",
    "        self.downsample_count = downsample_count\n",
    "        self.threshold = threshold\n",
    "\n",
    "        #Load dataset, standard or maxabs[-1,1], minmax[0,1], No\n",
    "        if (scale == \"standard\"):\n",
    "            self.scaler = preprocessing.StandardScaler()\n",
    "        elif (scale == \"minmax\"):\n",
    "            self.scaler = preprocessing.MinMaxScaler()\n",
    "        elif (scale == \"maxabs\"):\n",
    "            self.scaler = preprocessing.MaxAbsScaler()\n",
    "\n",
    "        if kernel == \"really_linear\":\n",
    "            self.dens = NegativeMeanDistance(metric=metric)\n",
    "        else:\n",
    "            self.dens = KernelDensity(bandwidth=bandwidth, kernel=kernel, metric=metric)\n",
    "\n",
    "    def fit(self, X):\n",
    "        # scale\n",
    "        self.scaler.fit(X)\n",
    "        self.X = self.scaler.transform(X)\n",
    "\n",
    "        # downsample?\n",
    "        if self.should_downsample:\n",
    "            self.X = self.downsample(self.X, self.downsample_count)\n",
    "\n",
    "        self.dens.fit(self.X)\n",
    "        # transform relative threshold (eg 95%) to absolute\n",
    "        dens = self.get_density(self.X, scale=False) # no need to scale again\n",
    "        self.abs_threshold = np.percentile(dens, 100 * (1 - self.threshold))\n",
    "\n",
    "    def get_density(self, X, scale=True):\n",
    "        if scale:\n",
    "            X = self.scaler.transform(X)\n",
    "        # in negative log-prob (for KDE), in negative distance (for NegativeMeanDistance)\n",
    "        return self.dens.score_samples(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        dens = self.get_density(X)\n",
    "        return dens < self.abs_threshold # in both KDE and NMD, lower values are more anomalous\n",
    "\n",
    "    def downsample(self, X, n):\n",
    "        # we've already fit()ted, but we're worried that our X is so\n",
    "        # large our classifier will be too slow in practice. we can\n",
    "        # downsample by running a kde on X and sampling from it (this\n",
    "        # will be slow, but happens only once), and then using those\n",
    "        # points as the new X.\n",
    "        if len(X) < n:\n",
    "            return X\n",
    "        kde = KernelDensity()\n",
    "        kde.fit(X)\n",
    "        return kde.sample(n)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Aug 14 10:48:37 2016\n",
    "\n",
    "@author: VANLOI\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def Plotting_AUC(name_dataset, path_result, training_size,\n",
    "                 FPR_auto, TPR_auto, auc_auto,\n",
    "                 FPR_cen, TPR_cen, auc_cen,\n",
    "                 FPR_kde, TPR_kde, auc_kde):\n",
    "    print (\"\\n*********************** Plot AUC *************************\")\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.title('The ROC curves - '+ name_dataset, fontsize=16)\n",
    "    plt.plot(FPR_auto, TPR_auto, 'g-^'  , label='OCAE      (AUC = %0.3f)'% auc_auto, markevery = 150 , markersize = 6)\n",
    "    plt.plot(FPR_cen, TPR_cen,   'b-o' ,  label='OCCEN    (AUC = %0.3f)'% auc_cen, markevery = 150 , markersize = 6)\n",
    "    plt.plot(FPR_kde, TPR_kde, 'r-x' , label='OCKDE    (AUC = %0.3f)'% auc_kde, markevery = 150 , markersize = 6)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.xlim([-0.1,1.1])\n",
    "    plt.ylim([-0.1,1.1])\n",
    "    plt.ylabel('True Positive Rate', fontsize=14)\n",
    "    plt.xlabel('False Positive Rate', fontsize=14)\n",
    "    plt.savefig(path_result + \"fig_\" + name_dataset +\"_\" + training_size + \"_Auc.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "def Plotting_End2End_RE(RE, epoch, ymin, ymax, data_name, path):\n",
    "    \"\"\"Plotting RE on train_set and validation_set of the End-to-End traing\n",
    "    process\"\"\"\n",
    "\n",
    "    plt.figure(figsize=(6,3))\n",
    "    #plt.title('End-to-End training RE on ' + data_name, fontsize=16)\n",
    "    plt.xlim([0.0, epoch + 1.0])\n",
    "    plt.ylim([ymin,ymax])\n",
    "\n",
    "    x  = RE[:,0]\n",
    "    y1 = RE[:,1]\n",
    "    y2 = RE[:,2]\n",
    "    plt.plot(x, y1,  'b', label = 'Validation set')\n",
    "    plt.plot(x, y2,  'r', label = 'Training set')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Error', fontsize=14)\n",
    "    plt.xlabel('Epochs', fontsize=14)\n",
    "    plt.savefig(path + data_name +\"_End2End_loss.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "def Plotting_Loss_Component(LOSS, RE, ymin, ymax, data_name, path):\n",
    "    \"\"\"Plotting RE on train_set and validation_set of the End-to-End traing\n",
    "    process\"\"\"\n",
    "\n",
    "    plt.figure(figsize=(6,3))\n",
    "    #plt.title('End-to-End training RE on ' + data_name, fontsize=16)\n",
    "\n",
    "    x  = LOSS[:,0]\n",
    "    y1 = LOSS[:,1]\n",
    "    y2 = LOSS[:,2]\n",
    "    y3 = RE[:,2]\n",
    "\n",
    "    plt.xlim([0.0, max(x) + 1.0])\n",
    "    plt.ylim([ymin,ymax])\n",
    "\n",
    "    plt.plot(x, y1,  'b', label = 'Recon error')\n",
    "    plt.plot(x, y2,  'g', label = 'KL-divergence')\n",
    "    plt.plot(x, y3,  'r', label = 'Training error')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Errors', fontsize=14)\n",
    "    plt.xlabel('Epochs', fontsize=14)\n",
    "    plt.savefig(path + data_name +\"_traing_errors.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\"*****************************************************************************\"\n",
    "def Plotting_Monitor(RE, ymin, ymax, data_name, path):\n",
    "    \"\"\"Plotting RE on train_set and validation_set of the End-to-End traing\n",
    "    process\"\"\"\n",
    "\n",
    "    plt.figure(figsize=(6,3))\n",
    "    #plt.title('Monitoring AUC every 5 epoch on ' + data_name, fontsize=16)\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "    x   = RE[:,0]\n",
    "    lof = RE[:,1]\n",
    "    cen = RE[:,2]\n",
    "    dis = RE[:,3]\n",
    "    kde = RE[:,4]\n",
    "    svm5 = RE[:,5]\n",
    "    svm1 = RE[:,6]\n",
    "    ae  = RE[:,7]\n",
    "\n",
    "    plt.xlim([0.0, max(x) + 1.0])\n",
    "    plt.ylim([ymin,ymax])\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "    plt.plot(x, lof,  'r-o', label = 'LOF', markevery = 20 , markersize = 6)\n",
    "    plt.plot(x, cen,  'b-x', label = 'CEN', markevery = 20 , markersize = 6)\n",
    "    plt.plot(x, dis,  'g-^', label = 'MDIS', markevery = 20 , markersize = 6)\n",
    "    plt.plot(x, kde,  'y-x', label = 'KDE', markevery = 20 , markersize = 6)\n",
    "    plt.plot(x, svm5,  'r-^', label = 'SVM05', markevery = 20 , markersize = 6)\n",
    "    plt.plot(x, svm1,  'g-o', label = 'SVM01', markevery = 20 , markersize = 6)\n",
    "    plt.plot(x, ae,   'b-^', label = 'AE' , markevery = 20 , markersize = 6)\n",
    "\n",
    "    ax.legend(bbox_to_anchor=(0.99, 0.28), ncol= 3, fontsize = 'medium')\n",
    "\n",
    "    #plt.legend(loc='upper right')\n",
    "    plt.ylabel('AUC', fontsize=14)\n",
    "    plt.xlabel('Epochs', fontsize=14)\n",
    "    plt.savefig(path + data_name + \"_Monitor_AUCs.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#def Plotting_Monitor(AUC_hidden, epoch, ymin, ymax, data_name, path):\n",
    "#    \"\"\"Plotting RE on train_set and validation_set of the End-to-End traing\n",
    "#    process\"\"\"\n",
    "#\n",
    "#    plt.figure(figsize=(8,4))\n",
    "#\n",
    "#    x   = AUC_hidden[:,0]\n",
    "#    lof = AUC_hidden[:,1]\n",
    "#    cen = AUC_hidden[:,2]\n",
    "#    dis = AUC_hidden[:,3]\n",
    "#    kde = AUC_hidden[:,4]\n",
    "#    svm5 = AUC_hidden[:,5]\n",
    "#    svm1 = AUC_hidden[:,6]\n",
    "#    ae  = AUC_hidden[:,7]\n",
    "#\n",
    "#    ax0 = plt.subplot2grid((1, 8), (0, 0), colspan=7)\n",
    "#    plt.xlim([0.0, epoch + 1.0])\n",
    "#    plt.ylim([ymin,ymax])\n",
    "#    ax0.plot(x, lof,  'r-o', label = 'LOF', markevery = 20 , markersize = 6)\n",
    "#    ax0.plot(x, cen,  'b-x', label = 'CEN', markevery = 20 , markersize = 6)\n",
    "#    ax0.plot(x, dis,  'g-^', label = 'DIS', markevery = 20 , markersize = 6)\n",
    "#    ax0.plot(x, kde,  'y-x', label = 'KDE', markevery = 20 , markersize = 6)\n",
    "#    ax0.plot(x, svm5, 'r-^', label = 'SVM05', markevery = 20 , markersize = 6)\n",
    "#    ax0.plot(x, svm1, 'g-o', label = 'SVM01', markevery = 20 , markersize = 6)\n",
    "#    ax0.plot(x, ae,   'b-^', label = 'AE' , markevery = 20 , markersize = 6)\n",
    "#\n",
    "#    ax0.legend(bbox_to_anchor=(0.99, 0.32), ncol= 3)\n",
    "#    #plt.legend(loc='upper right')\n",
    "#    plt.ylabel('AUC', fontsize=14)\n",
    "#    plt.xlabel('Epochs', fontsize=14)\n",
    "#\n",
    "#    ax1 = plt.subplot2grid((1, 8), (0, 7))\n",
    "#    plt.ylim([ymin,ymax])\n",
    "#    ax1.plot(x, lof,  'r-o', label = 'LOF', markevery = 20 , markersize = 6)\n",
    "#    ax1.plot(x, cen,  'b-x', label = 'CEN', markevery = 20 , markersize = 6)\n",
    "#    ax1.plot(x, dis,  'g-^', label = 'DIS', markevery = 20 , markersize = 6)\n",
    "#    ax1.plot(x, kde,  'y-x', label = 'KDE', markevery = 20 , markersize = 6)\n",
    "#    ax1.plot(x, svm5, 'r-^', label = 'SVM05', markevery = 20 , markersize = 6)\n",
    "#    ax1.plot(x, svm1, 'g-o', label = 'SVM01', markevery = 20 , markersize = 6)\n",
    "#    ax1.plot(x, ae,   'b-^', label = 'AE' , markevery = 20 , markersize = 6)\n",
    "#    ax1.axes.get_yaxis().set_visible(False)\n",
    "#    ax1.axes.get_xaxis().set_visible(False)\n",
    "#\n",
    "#    plt.tight_layout()\n",
    "#    plt.savefig(path + data_name + \"_Monitor_AUCs.pdf\")\n",
    "#    plt.show()\n",
    "\n",
    "def Plotting_Pre_RE(RE, n_layers, epoch, ymin, ymax, batch_size, data_name):\n",
    "    \"\"\"Plotting REs of each dAE in the pre-training process\"\"\"\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.title('Pre-training RE on' + data_name + '- Batch size = ' + str(batch_size), fontsize=16)\n",
    "    plt.xlim([0.0, epoch + 1.0])\n",
    "    plt.ylim([ymin,ymax])\n",
    "\n",
    "    color = ['b', 'g', 'r', 'y']\n",
    "    label = [\"layer 1\", \"layer 2\", \"layer 3\", \"layer 4\"]\n",
    "\n",
    "    ax = plt.subplot(111)\n",
    "    x  = RE[:,0]\n",
    "    for i in range(n_layers):\n",
    "        y = RE[:,i+1]\n",
    "        plt.plot(x, y,  color[i], label = label[i])\n",
    "\n",
    "    ax.legend(bbox_to_anchor=(0.99, 0.99), ncol=n_layers)\n",
    "    #plt.legend(loc='upper right')\n",
    "    plt.ylabel('Reconstruction errors', fontsize=14)\n",
    "    plt.xlabel('Epochs', fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "\"Plotting recontruction error from three autoencoders together\"\n",
    "def Plotting_Pre_RE1(re, stop_epoch, n_layers, ymin, ymax, batch_size, data_name, path):\n",
    "    \"\"\"Plotting REs of each dAE in the pre-training process\"\"\"\n",
    "    plt.figure(figsize=(8,4))\n",
    "    #plt.title('Pre-training RE on ' + data_name + ' - Batch size = ' + str(batch_size), fontsize=16)\n",
    "\n",
    "    max_epoch = np.max(stop_epoch)\n",
    "\n",
    "    plt.xlim([0.0, max_epoch + 1.0])\n",
    "    plt.ylim([ymin,ymax])\n",
    "\n",
    "    color = ['b', 'g', 'r', 'y']\n",
    "    label = [\"layer 1\", \"layer 2\", \"layer 3\", \"layer 4\"]\n",
    "\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        x = None\n",
    "        y = None\n",
    "        x = np.array(range(int(stop_epoch[i])))   #stop epoches of layer i\n",
    "        y = re[:,i]                               #pre-train errors of layer i\n",
    "        y = y[:len(x)]\n",
    "        plt.plot(x, y,  color[i], label = label[i]) #plot pre-train errors of each layer\n",
    "\n",
    "    ax.legend(bbox_to_anchor=(0.99, 0.99), ncol=n_layers)\n",
    "    #plt.legend(loc='upper right')\n",
    "    plt.ylabel('Reconstruction Error', fontsize=14)\n",
    "    plt.xlabel('Epochs', fontsize=14)\n",
    "    plt.savefig(path + data_name + \"_Pre_train.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "#\"Each subplot for ploting reconstruction error of each autoencoder\"\n",
    "#def Plotting_Pre_RE1(re, stop_epoch, n_layers, ymin, ymax, batch_size, data_name):\n",
    "#    \"\"\"Plotting REs of each dAE in the pre-training process\"\"\"\n",
    "#\n",
    "#    max_epoch = np.max(stop_epoch)\n",
    "#    color = ['b-', 'g-', 'r-']\n",
    "#    plt.subplots(ncols=3, nrows = 1, figsize=(8, 3))\n",
    "#\n",
    "#    for i in range(n_layers):\n",
    "#        x = np.array(range(int(stop_epoch[i])))   #stop epoches of layer i\n",
    "#        y = re[:,i]                               #pre-train errors of layer i\n",
    "#        y = y[:len(x)]\n",
    "#        fig = plt.subplot(1, 3, i+1)\n",
    "#\n",
    "#        plt.xlim([0.0, max_epoch + max_epoch/20])\n",
    "#        plt.ylim([ymin,ymax])\n",
    "#\n",
    "#        plt.plot(x, y, color[i])\n",
    "#        plt.legend(['Layer ' + str(i+1)])\n",
    "#        if (i==0):\n",
    "#            plt.ylabel('Reconstruction Error', fontsize=14)\n",
    "#        else:\n",
    "#            fig.axes.get_yaxis().set_visible(False)\n",
    "#\n",
    "#        plt.xlabel('Epochs', fontsize=14)\n",
    "#\n",
    "#        plt.yticks(fontsize=10)\n",
    "#        plt.xticks(rotation = 30, fontsize=10)\n",
    "#\n",
    "#        #hide the first zero in axes matplotlib\n",
    "#        ax = plt.gca()\n",
    "#        xticks = ax.xaxis.get_major_ticks()\n",
    "#        xticks[0].label1.set_visible(False)\n",
    "#\n",
    "#    plt.subplots_adjust(wspace=0.005, hspace=0)\n",
    "#    plt.savefig(path + data_name + \"_Pre_train.pdf\")\n",
    "#    plt.show()\n",
    "\n",
    "\n",
    "def Plotting_AUC_RE(AUC_RE, dataset, ymin, ymax, path):\n",
    "    \"\"\"Plotting AUC against training-RE when evaluting the model. This is aim to\n",
    "    do gridsearch over batch_sizes to choose the best performanced model.\n",
    "    Hopfully, the smaller training-RE the model produces, the higher accuracy\n",
    "    when evaluting the model on testing set\"\"\"\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.title('AUC against RE - '+ dataset, fontsize=16)\n",
    "\n",
    "    #Sorted AUC_RE by reconstruction error\n",
    "    AUC_RE = AUC_RE[np.argsort(AUC_RE[:,9])]\n",
    "\n",
    "    x = AUC_RE[:,9]\n",
    "    plt.xlim( x[0] - (x[-1]-x[0])/20 , x[-1] + (x[-1]-x[0])/20)\n",
    "    plt.ylim([ymin, ymax])\n",
    "\n",
    "    y01 = AUC_RE[:,2]  #AUC of LOF\n",
    "    y11 = AUC_RE[:,3]  #AUC of CEN\n",
    "    y21 = AUC_RE[:,4]  #AUC of NDIS\n",
    "    y31 = AUC_RE[:,5]  #AUC of KDE\n",
    "    y41 = AUC_RE[:,6]  #AUC of KDE\n",
    "    y51 = AUC_RE[:,8]  #AUC of AE\n",
    "\n",
    "\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "    plt.plot(x, y01,  'b-p', label = 'LOF', markersize = 6)\n",
    "    plt.plot(x, y11,  'r-p', label = 'CEN', markersize = 6)\n",
    "    plt.plot(x, y21,  'g-^', label = 'NDIS',markersize = 6)\n",
    "    plt.plot(x, y31,  'y-d', label = 'KDE',markersize = 6)\n",
    "    plt.plot(x, y41,  'r-s', label = 'SVM05',markersize = 6)\n",
    "    plt.plot(x, y51,  'b-s', label = 'AE',markersize = 6)\n",
    "    #b: blue | g: green | r: red | c: cyan | m: magenta | y: yellow | k: black | w: white\n",
    "\n",
    "    ax.legend(bbox_to_anchor=(0.99, 0.25), ncol=3)\n",
    "    plt.ylabel('AUC Value', fontsize=14)\n",
    "    plt.xlabel('Reconstruction Error x 100', fontsize=14)\n",
    "    plt.savefig(path + \"AUC_RE_\" + dataset +\".pdf\")\n",
    "    plt.show()\n",
    "\n",
    "def Plotting_AUC_Batch_Size(AUC_RE, dataset, ymin, ymax, path):\n",
    "    \"\"\"Plotting AUC against training-RE when evaluting the model. This is aim to\n",
    "    do gridsearch over batch_sizes to choose the best performanced model.\n",
    "    Hopfully, the smaller training-RE the model produces, the higher accuracy\n",
    "    when evaluting the model on testing set\"\"\"\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.title('AUC against RE - '+ dataset, fontsize=16)\n",
    "\n",
    "    #Sorted AUC_RE by reconstruction error\n",
    "    #AUC_RE = AUC_RE[np.argsort(AUC_RE[:,9])]\n",
    "\n",
    "    x = AUC_RE[:,0]\n",
    "    plt.xlim( x[0] - 1 , x[-1] + 1)\n",
    "    plt.ylim([ymin, ymax])\n",
    "\n",
    "    y01 = AUC_RE[:,2]  #AUC of LOF\n",
    "    y11 = AUC_RE[:,3]  #AUC of CEN\n",
    "    y21 = AUC_RE[:,4]  #AUC of NDIS\n",
    "    y31 = AUC_RE[:,5]  #AUC of KDE\n",
    "    y41 = AUC_RE[:,6]  #AUC of KDE\n",
    "    y51 = AUC_RE[:,8]  #AUC of AE\n",
    "\n",
    "\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "    plt.plot(x, y01,  'b-p', label = 'LOF', markersize = 6)\n",
    "    plt.plot(x, y11,  'r-p', label = 'CEN', markersize = 6)\n",
    "    plt.plot(x, y21,  'g-^', label = 'NDIS',markersize = 6)\n",
    "    plt.plot(x, y31,  'y-d', label = 'KDE',markersize = 6)\n",
    "    plt.plot(x, y41,  'r-s', label = 'SVM05',markersize = 6)\n",
    "    plt.plot(x, y51,  'b-s', label = 'AE',markersize = 6)\n",
    "    #b: blue | g: green | r: red | c: cyan | m: magenta | y: yellow | k: black | w: white\n",
    "\n",
    "    ax.legend(bbox_to_anchor=(0.99, 0.25), ncol=3)\n",
    "    plt.ylabel('AUC Value', fontsize=14)\n",
    "    plt.xlabel('Reconstruction Error x 100', fontsize=14)\n",
    "    plt.savefig(path + \"AUC_RE_\" + dataset +\".pdf\")\n",
    "    plt.show()\n",
    "\n",
    "def Plotting_AUC_BW(AUC_Hidden, dataset, xmax, ymin, ymax, training_size, path ):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.title('AUC against BW - '+ dataset, fontsize=16)\n",
    "    plt.xlim([0.0, xmax])\n",
    "    plt.ylim([ymin, ymax])\n",
    "\n",
    "    x   = AUC_Hidden[:,0]\n",
    "    y11 = AUC_Hidden[:,1]\n",
    "    y21 = AUC_Hidden[:,2]\n",
    "    y31 = AUC_Hidden[:,3]\n",
    "    y41 = AUC_Hidden[:,4]\n",
    "    y51 = AUC_Hidden[:,5]\n",
    "\n",
    "    plt.plot(x, y11,  'b-s', label = 'KDE      - Hidden',markersize = 6)\n",
    "    plt.plot(x, y21,  'r-p', label = 'Negative Distance', markersize = 6)\n",
    "    plt.plot(x, y31,  'g-^', label = 'SVM(0.5) - Hidden',markersize = 6)\n",
    "    plt.plot(x, y41,  'y-d', label = 'SVM(0.2) - Hidden',markersize = 6)\n",
    "    plt.plot(x, y51,  'm-s', label = 'SVM(0.1) - Hidden',markersize = 6)\n",
    "    #b: blue | g: green | r: red | c: cyan | m: magenta | y: yellow | k: black | w: white\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('AUC Value', fontsize=14)\n",
    "    plt.xlabel('Bandwidth', fontsize=14)\n",
    "    plt.savefig(path + \"AUC_BW_\" + dataset + \"_\"+ training_size +\".pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_auc_size_input(data, data_name, sizes, path):\n",
    "\n",
    "    # data to plot\n",
    "    n_groups = 5\n",
    "    LOF     = data[:,0:1]\n",
    "    CEN     = data[:,1:2]\n",
    "    DIS     = data[:,2:3]\n",
    "    KDE     = data[:,3:4]\n",
    "    SVM05   = data[:,4:5]\n",
    "    SVM01   = data[:,-1]\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "#    plt.title(data_name + ' Attack Group', fontsize=16)\n",
    "    plt.ylim([0.0, 1.0])\n",
    "#    plt.ylim([0.0, m + m/5])\n",
    "\n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.1\n",
    "    opacity = 1.0\n",
    "\n",
    "    plt.bar(index + bar_width, LOF, bar_width,\n",
    "                 alpha=opacity, color='b', label='LOF')\n",
    "\n",
    "    plt.bar(index + 2*bar_width, CEN, bar_width,\n",
    "                 alpha=opacity,color='g',label='CEN')\n",
    "\n",
    "    plt.bar(index + 3*bar_width , DIS, bar_width,\n",
    "                 alpha=opacity,color='r',label='DIS')\n",
    "\n",
    "    plt.bar(index + 4*bar_width, KDE, bar_width,\n",
    "                 alpha=opacity,color='y',label='KDE')\n",
    "\n",
    "    plt.bar(index + 5*bar_width, SVM05, bar_width,\n",
    "                 alpha=opacity,color='c',label='SVM05')\n",
    "\n",
    "    plt.bar(index + 6*bar_width, SVM01, bar_width,\n",
    "                 alpha=opacity,color='maroon',label='SVM01')\n",
    "\n",
    "    ax.legend(bbox_to_anchor=(1.04, 0.42), ncol=1, fontsize = 'small')\n",
    "\n",
    "    plt.xlabel('Size of training set', fontsize=16)\n",
    "    plt.ylabel('AUC', fontsize=16)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    ax.yaxis.grid(True)\n",
    "\n",
    "    plt.xticks(index + 2*bar_width, ('0.5%('+str(sizes[0])+')', '1%('+str(sizes[1])+')', '5%('+str(sizes[2])+')', '10%('+str(sizes[3])+')', '20%('+str(sizes[4])+')'),rotation=15,fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path + data_name + \"_auc_size.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"Plot AUC vs Size of training data - 1\"\n",
    "def plot_auc_size_1(data, data_name, sizes, path):\n",
    "    # data to plot\n",
    "    n_groups = 5\n",
    "    LOF     = data[:,0:1]\n",
    "    CEN     = data[:,1:2]\n",
    "    DIS     = data[:,2:3]\n",
    "    KDE     = data[:,3:4]\n",
    "    SVM05   = data[:,4:5]\n",
    "    SVM01   = data[:,5:6]\n",
    "#    RE      = data[:,-1]\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "#    plt.title(data_name + ' Attack Group', fontsize=16)\n",
    "    plt.ylim([0.0, 1.0])\n",
    "#    plt.ylim([0.0, m + m/5])\n",
    "\n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.1\n",
    "    opacity = 1.0\n",
    "\n",
    "    plt.bar(index + bar_width, LOF, bar_width,\n",
    "                 alpha=opacity, color='cyan', label='LOF')\n",
    "\n",
    "    plt.bar(index + 2*bar_width, CEN, bar_width,\n",
    "                 alpha=opacity,color='yellow',label='CEN')\n",
    "\n",
    "    plt.bar(index + 3*bar_width , DIS, bar_width,\n",
    "                 alpha=opacity,color='magenta',label='NDIS')\n",
    "\n",
    "    plt.bar(index + 4*bar_width, KDE, bar_width,\n",
    "                 alpha=opacity,color='blue',label='KDE')\n",
    "\n",
    "    plt.bar(index + 5*bar_width, SVM05, bar_width,\n",
    "                 alpha=opacity,color='lightblue',label=r'SVM$_{\\nu = 0.5}$')\n",
    "\n",
    "    plt.bar(index + 6*bar_width, SVM01, bar_width,\n",
    "                 alpha=opacity,color='plum',label=r'SVM$_{\\nu = 0.1}$')\n",
    "\n",
    "#    plt.bar(index + 7*bar_width, RE, bar_width,\n",
    "#                 alpha=opacity,color='springgreen',label='RE-Based')\n",
    "    #xx-small, x-small, small, medium, large, x-large, xx-large\n",
    "    ax.legend(bbox_to_anchor=(1.0, 0.30), ncol=2, fontsize = 'large')\n",
    "\n",
    "    plt.xlabel('Size of training set', fontsize=16)\n",
    "    plt.ylabel('AUC', fontsize=16)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    ax.yaxis.grid(True)\n",
    "\n",
    "    plt.xticks(index + 2*bar_width, (str(sizes[0]),\\\n",
    "                                     str(sizes[1]),\\\n",
    "                                     str(sizes[2]),\\\n",
    "                                     str(sizes[3]),\\\n",
    "                                     str(sizes[4])),\\\n",
    "                                     rotation=0,fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path + data_name + \"_auc_size.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "#from matplotlib import rcParams\n",
    "##rcParams['mathtext.default'] = 'regular'\n",
    "#rcParams['text.usetex']=True\n",
    "# Mathtext font size\n",
    "# \\tiny, \\small, \\normalsize, \\large, \\Large, \\LARGE, \\huge and \\Huge\n",
    "\"Plot AUC vs Size of training data - 2\"\n",
    "def plot_auc_size_2(data, data_name, name, sizes, method, path):\n",
    "    cl = [\"LOF\", \"CEN\", \"MDIS\", \"KDE\", r'OCSVM$_{\\nu=0.5}$', r'OCSVM$_{\\nu=0.1}$']\n",
    "    # data to plot\n",
    "    n_groups = 6\n",
    "    Z500    = data[:,0:1]\n",
    "    Z1000     = data[:,1:2]\n",
    "    Z2000     = data[:,2:3]\n",
    "    Z5000     = data[:,3:4]\n",
    "    Z10000      = data[:,-1]\n",
    "#    RE      = data[:,-1]\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "    plt.title(name , fontsize=16)\n",
    "    plt.ylim([0.5, 1.0])\n",
    "#    plt.ylim([0.0, m + m/5])\n",
    "\n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.125\n",
    "    space     = 0.0\n",
    "    opacity = 1.0\n",
    "\n",
    "    plt.bar(index + bar_width+space, Z500, bar_width,\n",
    "                 alpha=opacity, color='cyan', label='500')\n",
    "\n",
    "    plt.bar(index + 2*(bar_width+space), Z1000, bar_width,\n",
    "                 alpha=opacity,color='yellow',label='1000')\n",
    "\n",
    "    plt.bar(index + 3*(bar_width+space) , Z2000, bar_width,\n",
    "                 alpha=opacity,color='magenta',label='2000')\n",
    "\n",
    "    plt.bar(index + 4*(bar_width+space), Z5000, bar_width,\n",
    "                 alpha=opacity,color='blue',label='5000')\n",
    "\n",
    "    plt.bar(index + 5*(bar_width+space), Z10000, bar_width,\n",
    "                 alpha=opacity,color='lightblue',label='10000')\n",
    "\n",
    "\n",
    "#    plt.bar(index + 7*bar_width, RE, bar_width,\n",
    "#                 alpha=opacity,color='springgreen',label='RE-Based')\n",
    "    #xx-small, x-small, small, medium, large, x-large, xx-large\n",
    "    ax.legend(bbox_to_anchor=(1.01, 0.61), ncol=1, fontsize = 'x-large')\n",
    "\n",
    "#    plt.xlabel('Size of training set', fontsize=16)\n",
    "    plt.ylabel('AUC', fontsize=16)\n",
    "    plt.yticks(fontsize=14)\n",
    "\n",
    "    ax.yaxis.grid(True)\n",
    "\n",
    "    plt.xticks(index + 3*bar_width, (method + cl[0],\\\n",
    "                                     method + cl[1],\\\n",
    "                                     method + cl[2],\\\n",
    "                                     method + cl[3],\\\n",
    "                                     method + cl[4],\\\n",
    "                                     method + cl[5]),\\\n",
    "                                     rotation=20,fontsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path + data_name + \"_auc_size.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\"Visualize the hidden data in two dimension\"\n",
    "def visualize_hidden(train_set, test_set, actual, data_name, data, path):\n",
    "\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    scaler.fit(train_set)\n",
    "\n",
    "    train_set = scaler.transform(train_set)\n",
    "    test_set  = scaler.transform(test_set)\n",
    "\n",
    "    test_X0 = test_set[(actual==1)]\n",
    "    test_X1 = test_set[(actual==0)]\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = plt.subplot(111)\n",
    "    if data == \"train\":\n",
    "        plt.plot(train_set[:,0], train_set[:,1], 'bo', ms=5, mec=\"b\", label=\"Normal Train\")\n",
    "    elif data == \"normal\":\n",
    "        plt.plot(test_X0[:,0],   test_X0[:,1],   'go', ms=5, mec=\"g\", label=\"Normal Test\")\n",
    "    else:\n",
    "        plt.plot(test_X1[:,0],   test_X1[:,1],   'r^', ms=5, mec=\"r\", label= \"Anomaly Test\")\n",
    "\n",
    "    ax.legend(bbox_to_anchor=(1.0, 1.0), ncol=3 )\n",
    "\n",
    "    plt.axis('equal')\n",
    "    plt.ylim((-10.0, 10.0))\n",
    "    plt.xlim((-10.0, 10.0))\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(path + data_name + \"_v_hid_train_\" + dataset + \".pdf\")\n",
    "    plt.show()\n",
    "    plt.close\n",
    "\n",
    "\n",
    "\n",
    "\"Each subplot for ploting reconstruction error of each autoencoder\"\n",
    "def visualize_hidden1(train_set, test_set, actual, data_name, path):\n",
    "    \"\"\"Plotting REs of each dAE in the pre-training process\"\"\"\n",
    "\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    scaler.fit(train_set)\n",
    "\n",
    "    train_set = scaler.transform(train_set)\n",
    "    test_set  = scaler.transform(test_set)\n",
    "\n",
    "    test_X0 = test_set[(actual==1)]\n",
    "    test_X1 = test_set[(actual==0)]\n",
    "\n",
    "    plt.subplots(ncols=3, nrows = 1, figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.axis('equal')\n",
    "    plt.ylim((-10.0, 10.0))\n",
    "    plt.xlim((-10.0, 10.0))\n",
    "    plt.plot(train_set[:,0], train_set[:,1], 'bo', ms=5, mec=\"b\", label=\"Normal Train\")\n",
    "    plt.legend([\"Normal Train\"])\n",
    "\n",
    "    fig = plt.subplot(1, 3, 2)\n",
    "    plt.axis('equal')\n",
    "    plt.ylim((-10.0, 10.0))\n",
    "    plt.xlim((-10.0, 10.0))\n",
    "    plt.plot(test_X0[:,0],   test_X0[:,1],   'go', ms=5, mec=\"g\", label=\"Normal Test\")\n",
    "    fig.axes.get_yaxis().set_visible(False)\n",
    "    plt.legend([\"Normal Test\"])\n",
    "\n",
    "    fig = plt.subplot(1, 3, 3)\n",
    "    plt.axis('equal')\n",
    "    plt.ylim((-10.0, 10.0))\n",
    "    plt.xlim((-10.0, 10.0))\n",
    "    plt.plot(test_X1[:,0],   test_X1[:,1],   'r^', ms=5, mec=\"r\", label= \"Anomaly Test\")\n",
    "    fig.axes.get_yaxis().set_visible(False)\n",
    "    plt.legend([\"Anomaly Test\"])\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0)\n",
    "    plt.savefig(data_name + \"_Visualize.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\"Investigate Bandwidth/gramma parameters of SVM and KDE\"\n",
    "def Plot_AUC_Bandwidth(auc, data_name, X_max, n_features ,path):\n",
    "\n",
    "    bw    = auc[:,0]\n",
    "    kde   = auc[:,1]\n",
    "    svm05 = auc[:,2]\n",
    "    svm01 = auc[:,3]\n",
    "    default_bw = (n_features/2.0)**0.5\n",
    "\n",
    "    fig = plt.figure(figsize=(8,4))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "\n",
    "    plt.xlim([0.0, X_max])\n",
    "    plt.ylim([0.40,1.0])\n",
    "\n",
    "    plt.plot(bw, svm05,  'r-o', ms=6, mec=\"r\", label =r'$\\mathrm{SVM}_{\\nu = 0.5}$', markevery = 3)\n",
    "    plt.plot(bw, svm01,  'g-^', ms=6, mec=\"g\", label =r'$\\mathrm{SVM}_{\\nu = 0.1}$', markevery = 3)\n",
    "    plt.plot(bw, kde,    'b-x', ms=6, mec=\"b\", label= 'KDE', markevery = 3)\n",
    "    plt.legend(bbox_to_anchor=(1.01, 0.15), ncol=3)\n",
    "    ax1.set_ylabel('AUC', fontsize=14)\n",
    "    ax1.set_xlabel('Bandwidth', fontsize=14)\n",
    "\n",
    "\n",
    "    ax2 = ax1.twiny()\n",
    "    new_tick_locations = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "    def tick_function(bw):\n",
    "        gamma = 1.0/(2.0*bw*bw)\n",
    "        return [\"%.3f\" % z for z in gamma]\n",
    "\n",
    "    ax2.set_xlim(ax1.get_xlim())\n",
    "    ax2.set_xticks(new_tick_locations)\n",
    "    ax2.set_xticklabels(tick_function(new_tick_locations))\n",
    "    ax2.set_xlabel(r\"Gamma($\\gamma$) =  $1/(2*bandwidth^{2})$\", fontsize=14 )\n",
    "\n",
    "\n",
    "    sparse_data = [\"Arrhythmia\", \"Spambase\", \"UNSW\", \"NSLKDD\", \"InternetAds\"]\n",
    "    if (data_name in sparse_data):\n",
    "        x_text = 4.0\n",
    "    else:\n",
    "        x_text = 0.5\n",
    "    ax1.annotate('default value',\n",
    "            xy=(default_bw, 1.0), xytext=(x_text, 1.06),\n",
    "            arrowprops=dict(facecolor='green', arrowstyle=\"->\"))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path + \"Bandwith_auc/\" + data_name + \"_BW.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def plot_sparsity_auc_bar(data, improve_auc, spa_score, method, path):\n",
    "\n",
    "    #Using CTU13-13 to demonstrate for four CTU13 datasets, [6,7,8,9]\n",
    "    id_data = [0, 1, 2, 3, 4, 5, 6, 9, 10, 11, 12, 13]\n",
    "    improve_auc = improve_auc[id_data]\n",
    "    spa_score   = spa_score[id_data]        #sparsity score\n",
    "    labels      = data[id_data]\n",
    "\n",
    "    n_groups = len(id_data)\n",
    "    LOF     = improve_auc[:,2:3]\n",
    "    CEN     = improve_auc[:,3:4]\n",
    "    DIS     = improve_auc[:,4:5]\n",
    "    KDE     = improve_auc[:,5:6]\n",
    "    SVM05   = improve_auc[:,6:7]\n",
    "    SVM01   = improve_auc[:,7:8]\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "    #plt.title(data_name + ' Attack Group', fontsize=16)\n",
    "    plt.ylim([-0.45, 0.45])\n",
    "\n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.1\n",
    "    opacity = 1.0\n",
    "\n",
    "    plt.bar(index + 1*bar_width,   LOF,   bar_width, alpha=opacity, color='b', label='LOF')\n",
    "    plt.bar(index + 2*bar_width, CEN,   bar_width, alpha=opacity, color='g',label='CEN')\n",
    "    plt.bar(index + 3*bar_width, DIS,   bar_width, alpha=opacity, color='r',label='NDIS')\n",
    "    plt.bar(index + 4*bar_width, KDE,   bar_width, alpha=opacity, color='y',label='KDE')\n",
    "    plt.bar(index + 5*bar_width, SVM05, bar_width, alpha=opacity, color='c',label='SVM05')\n",
    "    plt.bar(index + 6*bar_width, SVM01, bar_width, alpha=opacity, color='maroon',label='SVM01')\n",
    "    #xx-small, x-small, small, medium, large, x-large, xx-large\n",
    "    ax.legend(bbox_to_anchor=(0.44, 1.0), ncol=3, fontsize = 'medium')\n",
    "\n",
    "    plt.xlabel('Sparsity of data', fontsize=14)\n",
    "    plt.ylabel('($\\mathrm{AUC}_{\\mathrm{hidden}}$' + '-' + '$\\mathrm{AUC}_{\\mathrm{input}}$)', fontsize=14)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    ax.yaxis.grid(True)\n",
    "    plt.xticks(index + 3*bar_width,(str(spa_score[i][1]) + '-' + str(labels[i]) for i in range(n_groups)),rotation=60,fontsize=11)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path + \"auc_sparsity_\" + method + \"_bar.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_sparsity_auc(data, improve_auc, spa_score, method, path):\n",
    "\n",
    "    #Using CTU13-13 to demonstrate for four CTU13 datasets, [6,7,8,9]\n",
    "    id_data = [0, 1, 2, 3, 4, 5, 9, 10, 11, 12, 13]\n",
    "    improve_auc = improve_auc[id_data]\n",
    "    spa_score   = spa_score[id_data]        #sparsity score\n",
    "    labels = data[id_data]                  #name of datasets\n",
    "\n",
    "    LOF     = improve_auc[:,2:3]\n",
    "    CEN     = improve_auc[:,3:4]\n",
    "    DIS     = improve_auc[:,4:5]\n",
    "    KDE     = improve_auc[:,5:6]\n",
    "    SVM05   = improve_auc[:,6:7]\n",
    "    SVM01   = improve_auc[:,7:8]\n",
    "\n",
    "    plt.figure(figsize=(8, 4.5))\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "\n",
    "    plt.ylim([-0.4, 0.4])\n",
    "    plt.xlim([-0.02, max(spa_score[:,1])+0.01])\n",
    "    plt.xticks(spa_score[:,1],spa_score[:,1], rotation=90)\n",
    "\n",
    "    #'b', 'g', 'r', 'c', 'm', 'y', 'k', 'w'\n",
    "    plt.plot(spa_score[:,1], LOF,   'b-s', ms=4, mec=\"b\", label ='LOF', markevery = 1)\n",
    "    plt.plot(spa_score[:,1], CEN,   'r-p', ms=4, mec=\"r\", label ='CEN', markevery = 1)\n",
    "    plt.plot(spa_score[:,1], DIS,   'g-^', ms=4, mec=\"g\", label= 'NDIS', markevery = 1)\n",
    "    plt.plot(spa_score[:,1], KDE,   'y-d', ms=4, mec=\"y\", label ='KDE', markevery = 1)\n",
    "    plt.plot(spa_score[:,1], SVM05, 'm-o', ms=4, mec=\"m\", label =r'$\\mathrm{SVM}_{\\nu = 0.5}$', markevery = 1)\n",
    "    plt.plot(spa_score[:,1], SVM01, 'c-x', ms=4, mec=\"c\", label= r'$\\mathrm{SVM}_{\\nu = 0.1}$', markevery = 1)\n",
    "\n",
    "    #xx-small, x-small, small, medium, large, x-large, xx-large\n",
    "    ax.legend(bbox_to_anchor=(0.47, 1.0), ncol=3, fontsize = 'medium')\n",
    "\n",
    "    plt.xlabel('Sparsity of data', fontsize=14)\n",
    "    plt.ylabel('($\\mathrm{AUC}_{\\mathrm{hidden}}$' + '-' + '$\\mathrm{AUC}_{\\mathrm{input}}$)', fontsize=14)\n",
    "    plt.yticks(fontsize=12)\n",
    "    ax.yaxis.grid(True)\n",
    "\n",
    "    ax.twiny()\n",
    "    plt.xlim([-0.02, max(spa_score[:,1])+0.01])\n",
    "    plt.xticks(spa_score[:,1], labels, rotation=90)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path + \"auc_sparsity_\" + method + \"_line.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def plot_dimension_auc(data, improve_auc, spa_dim, method, path):\n",
    "\n",
    "    #Using CTU13-13 to demonstrate for four CTU13 datasets, [6,7,8,9]\n",
    "    id_data = [0, 1, 2, 4, 5, 9, 10, 11, 12, 13]\n",
    "    improve_auc = improve_auc[id_data]\n",
    "    spa_dim     = spa_dim[id_data]        #sparsity score\n",
    "\n",
    "    improve_auc = np.insert(improve_auc, [0], spa_dim, axis=1)\n",
    "\n",
    "    #improve_auc = sorted(improve_auc, key=lambda a_entry: a_entry[2])\n",
    "    improve_auc = improve_auc[improve_auc[:,2].argsort()]\n",
    "    dim     = np.asanyarray(improve_auc[:,2], dtype = int)\n",
    "    idx     = np.asanyarray(improve_auc[:,0], dtype = int)\n",
    "    labels1 = []\n",
    "    for d in idx:\n",
    "      labels1 = np.append(labels1, data[d])\n",
    "\n",
    "                     #name of datasets\n",
    "\n",
    "    LOF     = improve_auc[:,5:6]\n",
    "    CEN     = improve_auc[:,6:7]\n",
    "    DIS     = improve_auc[:,7:8]\n",
    "    KDE     = improve_auc[:,8:9]\n",
    "    SVM05   = improve_auc[:,9:10]\n",
    "    SVM01   = improve_auc[:,10:11]\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "    log_dim = np.round(np.log(dim+1-9), 2)\n",
    "\n",
    "    plt.ylim([-0.45, 0.45])\n",
    "    plt.xlim([-0.1, max(log_dim)+0.1])\n",
    "    plt.xticks(log_dim, dim, rotation=90)\n",
    "\n",
    "    #'b', 'g', 'r', 'c', 'm', 'y', 'k', 'w'\n",
    "    plt.plot(log_dim, LOF,   'b-s', ms=4, mec=\"b\", label ='LOF', markevery = 1)\n",
    "    plt.plot(log_dim, CEN,   'r-p', ms=4, mec=\"r\", label ='CEN', markevery = 1)\n",
    "    plt.plot(log_dim, DIS,   'g-^', ms=4, mec=\"g\", label= 'NDIS', markevery = 1)\n",
    "    plt.plot(log_dim, KDE,   'y-d', ms=4, mec=\"y\", label ='KDE', markevery = 1)\n",
    "    plt.plot(log_dim, SVM05, 'm-o', ms=4, mec=\"m\", label =r'$\\mathrm{SVM}_{\\nu = 0.5}$', markevery = 1)\n",
    "    plt.plot(log_dim, SVM01, 'c-x', ms=4, mec=\"c\", label= r'$\\mathrm{SVM}_{\\nu = 0.1}$', markevery = 1)\n",
    "\n",
    "    #xx-small, x-small, small, medium, large, x-large, xx-large\n",
    "    ax.legend(bbox_to_anchor=(0.47, 1.0), ncol=3, fontsize = 'medium')\n",
    "\n",
    "    plt.xlabel('Dimension in log scale', fontsize=14)\n",
    "    plt.ylabel('($\\mathrm{AUC}_{\\mathrm{hidden}}$' + '-' + '$\\mathrm{AUC}_{\\mathrm{input}}$)', fontsize=14)\n",
    "    plt.yticks(fontsize=12)\n",
    "    ax.yaxis.grid(True)\n",
    "\n",
    "    ax.twiny()\n",
    "    plt.xlim([-0.1, max(log_dim)+0.1])\n",
    "    plt.xticks(log_dim, labels1, rotation=90)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path + \"auc_dimension_\" + method + \".pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\"****************** Plot histogram of z_mu, z_var and z **********************\"\n",
    "def histogram_z(x, name, alpha, epoch, path):\n",
    "\n",
    "    mu    = np.mean(x)\n",
    "    sigma = np.std(x)\n",
    "\n",
    "    # the histogram of the data\n",
    "    n, bins, patches = plt.hist(x, 20, normed=1, facecolor='green', alpha=0.5)\n",
    "    # add a 'best fit' line\n",
    "    y = mlab.normpdf( bins, mu, sigma)\n",
    "    plt.plot(bins, y, 'r--', linewidth=1)\n",
    "\n",
    "    if (name == 'mu'):\n",
    "        title = r'$\\mathrm{Histogram\\ of\\ \\mu}_{\\mathrm{z}}\\ (\\mathrm{\\alpha\\ = ' + str(alpha) + ',}\\ \\mathrm{epoch\\ = }'+ str(epoch) + ')$'\n",
    "        xlabel = r'$\\mathrm{\\mu}_{\\mathrm{z}}$'\n",
    "    elif (name == 'var'):\n",
    "        title = r'$\\mathrm{Histogram\\ of\\ \\sigma}_{\\mathrm{z}}\\ (\\mathrm{\\alpha\\ = ' + str(alpha) + ',}\\ \\mathrm{epoch\\ = }'+ str(epoch) + ')$'\n",
    "        xlabel = r'$\\mathrm{\\sigma}_{\\mathrm{z}}$'\n",
    "    else:\n",
    "        title = r'$\\mathrm{Histogram\\ of\\ z}\\ (\\mathrm{\\alpha\\ = ' + str(alpha) + ',}\\ \\mathrm{epoch\\ = }'+ str(epoch) + ')$'\n",
    "        xlabel = r'$\\mathrm{z}$'\n",
    "\n",
    "    plt.xlabel(xlabel, fontsize=18)\n",
    "    plt.ylabel('Probability', fontsize=14)\n",
    "\n",
    "    plt.title(title, fontsize=18)\n",
    "    plt.axis([-3, 3, 0, max(y)+ 0.1*max(y)])\n",
    "    plt.grid(True)\n",
    "    plt.savefig(path + \"Visualize_histogram/\" + \"his_\" + name + \"_\" + str(alpha) + \"_\" +  str(epoch) + \".pdf\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Nov 28 09:41:21 2016\n",
    "\n",
    "@author: VANLOI\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def auc_density(training_set, testing_set, actual, scale):\n",
    "    \"\"\"Compute AUC for density-based methods: Centroid, Negative Mean Distances,\n",
    "    Kernel Density Estimation and One-class Support Vector Machine, and LOF.\n",
    "    \"\"\"\n",
    "    #gamma = 1/2bw^2 = 1/n_feautes -> bw = (n_features/2)^0.5\n",
    "    #h_default = (d/2.0)**0.5\n",
    "    bw = (training_set.shape[1]/2.0)**0.5        #default value in One-class SVM\n",
    "    gamma = 1/(2*bw*bw)\n",
    "\n",
    "    \"*************** Centroid AE - Hidden layer **************\"\n",
    "    CEN = CentroidBasedOneClassClassifier()\n",
    "    CEN.fit(training_set)\n",
    "    predictions_cen = -CEN.get_density(testing_set)\n",
    "    FPR_cen, TPR_cen, thresholds_cen = roc_curve(actual, predictions_cen)\n",
    "    cen = auc(FPR_cen, TPR_cen)\n",
    "\n",
    "\n",
    "    \"****************** Negative Distance - Hidden layer **********************\"\n",
    "    clf_dis = DensityBasedOneClassClassifier(bandwidth = bw,\n",
    "                                             kernel=\"really_linear\",\n",
    "                                             metric=\"euclidean\",\n",
    "                                             scale = scale)\n",
    "    clf_dis.fit(training_set)\n",
    "    predictions_dis  = clf_dis.get_density(testing_set)\n",
    "    FPR_dis, TPR_dis, thresholds_dis = roc_curve(actual, predictions_dis)\n",
    "    dis = auc(FPR_dis, TPR_dis)\n",
    "\n",
    "\n",
    "    \"****************** KDE AE - Hidden layer*****************\"\n",
    "    #  ['gaussian'|'tophat'|'epanechnikov'|'exponential'|'linear'|'cosine']\n",
    "    KDE = DensityBasedOneClassClassifier(bandwidth = bw,\n",
    "                                         kernel=\"gaussian\",\n",
    "                                         metric=\"euclidean\",\n",
    "                                         scale = scale)\n",
    "    KDE.fit(training_set)\n",
    "    predictions_kde = KDE.get_density(testing_set)\n",
    "    FPR_kde, TPR_kde, thresholds_kde = roc_curve(actual, predictions_kde)\n",
    "    kde = auc(FPR_kde, TPR_kde)\n",
    "\n",
    "\n",
    "    \"********************* 1-SVM Hidden layer ***************************\"\n",
    "    training_set, testing_set =  normalize_data(training_set, testing_set, scale)\n",
    "\n",
    "    clf_05 = svm.OneClassSVM(nu=0.5, kernel=\"rbf\", gamma=gamma)\n",
    "    clf_05.fit(training_set)\n",
    "    #n_support_vectors =  len(clf.support_vectors_)\n",
    "    predictions_svm  = clf_05.decision_function(testing_set)\n",
    "    FPR_svm, TPR_svm, thresholds_svm = roc_curve(actual, predictions_svm)\n",
    "    svm_05 = auc(FPR_svm, TPR_svm)\n",
    "\n",
    "    \"nu = 0.1\"\n",
    "    clf_01 = svm.OneClassSVM( nu=0.1, kernel=\"rbf\", gamma=gamma)\n",
    "    clf_01.fit(training_set)\n",
    "    #num_01 =  len(clf_01.support_vectors_)\n",
    "    predictions_svm_01  = clf_01.decision_function(testing_set)\n",
    "    FPR_svm_01, TPR_svm_01, thresholds_svm_01 = roc_curve(actual, predictions_svm_01)\n",
    "    svm_01 = auc(FPR_svm_01, TPR_svm_01)\n",
    "\n",
    "    \"******************************* LOF **********************************\"\n",
    "    neighbors = (int)(len(training_set)*0.1)\n",
    "    clf_lof = LocalOutlierFactor(n_neighbors=neighbors)\n",
    "    clf_lof.fit(training_set)\n",
    "    predict = clf_lof._decision_function(testing_set)\n",
    "    FPR, TPR, thresholds = roc_curve(actual, predict)\n",
    "    lof = auc(FPR, TPR)\n",
    "\n",
    "    return lof, cen, dis, kde, svm_05, svm_01\n",
    "\n",
    "\n",
    "def auc_AEbased(test_X, output_test, actual):\n",
    "\n",
    "    \"******************* Testing Output layer ***************\"\n",
    "    OF = -(((test_X - output_test)**2).mean(1))\n",
    "    \"\"\"Classification decision will be based on the error (MAE or RMSE) between\n",
    "    output and input. The higher error value a example has, the stronger decision\n",
    "    the example belongs to anomaly class.\n",
    "    Because we set normal class is positive class, so we put minus \"-\" to MSE to\n",
    "    make OF of normal examples are large while those from anomaly examples are small\n",
    "    \"\"\"\n",
    "    predictions_auto = OF\n",
    "    FPR_auto, TPR_auto, thresholds_auto = roc_curve(actual, predictions_auto)\n",
    "    auc_auto = auc(FPR_auto, TPR_auto)\n",
    "\n",
    "    return auc_auto"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Aug 15 19:05:51 2017\n",
    "\n",
    "@author: VANLOI\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "path = \"Results/\"\n",
    "\n",
    "\"Check whether weights matrix is updated or not\"\n",
    "\n",
    "\n",
    "def check_weight_update(sda):\n",
    "    np.set_printoptions(precision=4, suppress=True)\n",
    "    \"Check whether weights matrix is updated or not\"\n",
    "    for i in range(sda.n_layers):\n",
    "        print(\"\\n %d\" % i)\n",
    "        print(sda.Autoencoder_layers[i].W.get_value(borrow=True))\n",
    "\n",
    "    for j in range(sda.n_layers, 2 * sda.n_layers):\n",
    "        print(\"\\n %d\" % j)\n",
    "        print(sda.Autoencoder_layers[j].W.eval())\n",
    "\n",
    "    print(\"\\n ************************************ \")\n",
    "\n",
    "\n",
    "class SdA(object):\n",
    "\n",
    "    def __init__(self, numpy_rng, theano_rng=None, n_ins=100, hidden_layers_sizes=[50, 30]):\n",
    "\n",
    "        self.encoder = []\n",
    "        self.decoder = []\n",
    "        self.params = []\n",
    "        self.n_layers = len(hidden_layers_sizes)\n",
    "\n",
    "        \"\"\"set seed one time per dataset, rng will randomly generate different\n",
    "        number in each mini_batch_size and, different in each epoch. This is\n",
    "        repetive the same number when we create new SdA object. See theano_rondom.py\"\"\"\n",
    "        self.rng = theano.tensor.shared_randomstreams.RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "\n",
    "        assert self.n_layers > 0\n",
    "        #        if not theano_rng:\n",
    "        #            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "\n",
    "        # allocate symbolic variables for the data\n",
    "        # the data is presented as rasterized images\n",
    "        self.x = T.matrix('x')\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            # the size of the input is either the number of hidden units of\n",
    "            # the layer below or the input size if we are on the first layer\n",
    "            if (i == 0):\n",
    "                input_size = n_ins\n",
    "                layer_input = self.x\n",
    "            else:\n",
    "                input_size = hidden_layers_sizes[i - 1]\n",
    "                layer_input = self.encoder[-1].output\n",
    "\n",
    "            \"Not the middle hidden layer\"\n",
    "            if (i < self.n_layers - 1):\n",
    "                encoder_layer = HiddenLayer(rng=numpy_rng,\n",
    "                                            input=layer_input,\n",
    "                                            n_in=input_size,\n",
    "                                            n_out=hidden_layers_sizes[i],\n",
    "                                            activation=T.tanh)\n",
    "                self.encoder.append(encoder_layer)\n",
    "                self.params.extend(encoder_layer.params)\n",
    "\n",
    "            else:\n",
    "                \"The middle hidden layer, linear\"\n",
    "                encoder_mu = HiddenLayer(rng=numpy_rng,\n",
    "                                         input=self.encoder[i - 1].output,\n",
    "                                         n_in=input_size,\n",
    "                                         n_out=hidden_layers_sizes[i])\n",
    "\n",
    "                encoder_var = HiddenLayer(rng=numpy_rng,\n",
    "                                          input=self.encoder[i - 1].output,\n",
    "                                          n_in=input_size,\n",
    "                                          n_out=hidden_layers_sizes[i])\n",
    "\n",
    "                self.encoder.append(encoder_mu)\n",
    "                self.params.extend(encoder_mu.params)\n",
    "\n",
    "                self.encoder.append(encoder_var)\n",
    "                self.params.extend(encoder_var.params)\n",
    "\n",
    "        \"*************** Sample z **************\"\n",
    "        mu = self.encoder[-2].output\n",
    "        log_var = self.encoder[-1].output\n",
    "        sample_z = self.sample_z(mu, log_var)\n",
    "\n",
    "        \"*************** Decoder ***************\"\n",
    "        i = self.n_layers - 1\n",
    "        while (i >= 0):\n",
    "            input_size = hidden_layers_sizes[i]\n",
    "            if (i > 0):\n",
    "                output_size = hidden_layers_sizes[i - 1]\n",
    "            else:\n",
    "                output_size = n_ins\n",
    "\n",
    "            if (i == self.n_layers - 1):  # the first layer in decoder\n",
    "                layer_input = sample_z\n",
    "                decoder_layer = HiddenLayer(rng=numpy_rng,\n",
    "                                            input=layer_input,\n",
    "                                            n_in=input_size,\n",
    "                                            n_out=output_size,\n",
    "                                            activation=T.tanh)  # may be linear\n",
    "                self.decoder.append(decoder_layer)\n",
    "                self.params.extend(decoder_layer.params)\n",
    "            else:\n",
    "                layer_input = self.decoder[-1].output\n",
    "                decoder_layer = HiddenLayer(rng=numpy_rng,\n",
    "                                            input=layer_input,\n",
    "                                            n_in=input_size,\n",
    "                                            n_out=output_size,\n",
    "                                            activation=T.tanh,\n",
    "                                            W=self.encoder[i].W.T)\n",
    "\n",
    "                self.decoder.append(decoder_layer)\n",
    "                self.params.append(decoder_layer.b)\n",
    "            i = i - 1\n",
    "\n",
    "        \"******************* End To End Cost function ************************\"\n",
    "        z_mu = self.encoder[-2].output\n",
    "        z_var = self.encoder[-1].output\n",
    "        y = self.decoder[-1].output\n",
    "\n",
    "        self.alpha = 1e-8\n",
    "        self.lamda = 0.05\n",
    "\n",
    "        self.recon = (((self.x - y) ** 2).mean(1)).mean()\n",
    "        \"\"\"When compute a constant together with theano variable, it will be converted\n",
    "        into the same shape as the theano variable. We may compute mean over features\n",
    "        of each example instead of sum. This is to avoid the difference in dimension of\n",
    "        each data. Default lamda = 0.05, alpha = 1e-8\"\"\"\n",
    "\n",
    "        alpha = self.alpha\n",
    "        self.KL = T.mean(\n",
    "            (0.5 / alpha) * T.mean(T.exp(z_var) + z_mu ** 2 - alpha - alpha * z_var + alpha * T.log(alpha), 1))\n",
    "        self.end2end_cost = self.recon + self.lamda * T.log10(self.KL + 1)\n",
    "\n",
    "        # Experiment: lamda = 0.05; alpha = 1e-8    1\n",
    "        # mean(1) is within example, mean(0) is within each feature\n",
    "\n",
    "    \"**************************** Sample z ***********************************\"\n",
    "\n",
    "    def sample_z(self, mu, log_var):\n",
    "        eps = self.rng.normal(mu.shape, 0.0, 1.0, dtype=theano.config.floatX)\n",
    "        sample_z = mu + T.exp(log_var / 2) * eps\n",
    "        return sample_z\n",
    "\n",
    "    \"********************** Compute KL and Recon Loss *************************\"\n",
    "\n",
    "    #    def Recon_KL_Loss(self, data_set):\n",
    "    #        data_size = data_set.get_value().shape[0]\n",
    "    #        index = T.lscalar('index')\n",
    "    #        KL1 = self.lamda*T.log10(self.KL+1)\n",
    "    #        Loss = theano.function([index],\n",
    "    #                               outputs = [self.recon, KL1],\n",
    "    #                               givens={self.x: data_set[index : data_size]})\n",
    "    #        return Loss(0)\n",
    "\n",
    "    def Recon_KL_loss_batch(self, train_x, batch_size):\n",
    "\n",
    "        index = T.lscalar('index')\n",
    "        # begining of a batch, given `index`\n",
    "        batch_begin = index * batch_size\n",
    "        # ending of a batch given `index`\n",
    "        batch_end = batch_begin + batch_size\n",
    "        KL1 = self.lamda * T.log10(self.KL + 1)\n",
    "        loss_com = theano.function([index],\n",
    "                                   outputs=[self.recon, KL1],\n",
    "                                   givens={self.x: train_x[batch_begin: batch_end]})\n",
    "        return loss_com\n",
    "\n",
    "    def Recon_KL_Loss(self, train_x, batch_size):\n",
    "        n_train = train_x.get_value().shape[0]\n",
    "        n_batches = (int)(n_train / batch_size)\n",
    "        loss_com = self.Recon_KL_loss_batch(train_x, batch_size)\n",
    "        loss = np.empty([0, 2])\n",
    "        for batch_index in range(n_batches):\n",
    "            l = loss_com(index=batch_index)\n",
    "            loss = np.append(loss, [l[0], l[1]])\n",
    "        loss = np.reshape(loss, (-1, 2))\n",
    "\n",
    "        return (loss.mean(0))\n",
    "\n",
    "    \"************************** Get Mu and Log_var ****************************\"\n",
    "\n",
    "    def get_mu_logvar(self, data_set):\n",
    "        data_size = data_set.get_value().shape[0]\n",
    "        index = T.lscalar('index')\n",
    "        mu = self.encoder[-2].output\n",
    "        log_var = self.encoder[-1].output\n",
    "        mu_logvar = theano.function([index],\n",
    "                                    outputs=[mu, log_var],\n",
    "                                    givens={self.x: data_set[index: data_size]})\n",
    "        return mu_logvar(0)\n",
    "\n",
    "    \"****** Error on train_x and valid_x before optimization process **********\"\n",
    "\n",
    "    def Loss_train_valid(self, train_x, valid_x):\n",
    "        index = T.lscalar('index')\n",
    "\n",
    "        train_size = train_x.get_value().shape[0]\n",
    "        tm = theano.function([index],\n",
    "                             outputs=self.end2end_cost,\n",
    "                             givens={self.x: train_x[index: train_size]})\n",
    "\n",
    "        valid_size = valid_x.get_value().shape[0]\n",
    "        vm = theano.function([index],\n",
    "                             outputs=self.end2end_cost,\n",
    "                             givens={self.x: valid_x[index: valid_size]})\n",
    "\n",
    "        return tm(0), vm(0)\n",
    "\n",
    "    \"**************************** Get hidden data z **************************\"\n",
    "\n",
    "    def get_hidden_data(self, data_set):\n",
    "        data_size = data_set.get_value().shape[0]\n",
    "        index = T.lscalar('index')\n",
    "        mu = self.encoder[-2].output\n",
    "        log_var = self.encoder[-1].output\n",
    "        z = self.sample_z(mu, log_var)\n",
    "        hidden_data = theano.function([index],\n",
    "                                      outputs=z,\n",
    "                                      givens={self.x: data_set[index: data_size]})\n",
    "        return hidden_data(0)\n",
    "\n",
    "    \"************get data from the output of Autoencoder**********************\"\n",
    "\n",
    "    def get_output_data(self, data_set):\n",
    "        data_size = data_set.get_value().shape[0]\n",
    "        index = T.lscalar('index')\n",
    "        y_data = theano.function([index],\n",
    "                                 outputs=self.decoder[-1].output,\n",
    "                                 givens={self.x: data_set[index: data_size]})\n",
    "        return y_data(0)\n",
    "\n",
    "    \"******************** Histogram z, z_mu and z_var ************************\"\n",
    "\n",
    "    def Plot_histogram_z(self, train_set, test_set, actual, epoch, path):\n",
    "        z_train = self.get_hidden_data(train_set)\n",
    "        mu, logvar = self.get_mu_logvar(train_set)\n",
    "\n",
    "        np.savetxt(path + \"Visualize_histogram/\" + \"z_train_\" + str(epoch) + \"_\" + str(self.alpha) + \".csv\", z_train,\n",
    "                   delimiter=\",\", fmt='%f')\n",
    "        #        np.savetxt(path + \"Visualize_histogram/\" + \"z_mu\" + str(epoch) + \".csv\",  mu, delimiter=\",\", fmt='%f' )\n",
    "        #        np.savetxt(path + \"Visualize_histogram/\" + \"z_var\" + str(epoch) + \".csv\", np.exp(logvar), delimiter=\",\", fmt='%f' )\n",
    "\n",
    "        #        histogram_z(mu[:,0],             'mu' , self.alpha, epoch, path)\n",
    "        #        histogram_z(np.exp(logvar[:,0]), 'var', self.alpha, epoch, path)\n",
    "        histogram_z(z_train[:, 0], 'z', self.alpha, epoch, path)\n",
    "\n",
    "    \"********************** Standard deviation of z ***************************\"\n",
    "\n",
    "    def Compute_Std(self, train_set, test_set, actual, data_name, path):\n",
    "        z_train = self.get_hidden_data(train_set)\n",
    "        z_test = self.get_hidden_data(test_set)\n",
    "\n",
    "        visualize_hidden1(z_train, z_test, actual, data_name, path)\n",
    "        # std on each feature over data\n",
    "        std = np.std(z_train, axis=0)\n",
    "        np.set_printoptions(precision=6, suppress=True)\n",
    "        print(\"\\n+ Standard Deviation of Hidden data:\")\n",
    "        print(std)\n",
    "\n",
    "    \"********************* Compute AUC on hidden data *************************\"\n",
    "\n",
    "    def Compute_AUC_Hidden(self, train_set, test_set, actual, norm, data_name):\n",
    "        z_train = self.get_hidden_data(train_set)  # get hidden values\n",
    "        z_test = self.get_hidden_data(test_set)  # get hidden values\n",
    "        y_test = self.get_output_data(test_set)  # get prediction values\n",
    "        \"Compute performance of classifiers on latent data\"\n",
    "        lof, cen, dis, kde, svm05, svm01 = auc_density(z_train, z_test, actual, norm)\n",
    "        ae = auc_AEbased(test_set.get_value(), y_test, actual)\n",
    "        return lof, cen, dis, kde, svm05, svm01, ae\n",
    "\n",
    "    \"**************************************************************************\"\n",
    "\n",
    "    def Save_Hidden_Data(self, train_set, test_set, data_name, path):\n",
    "        z_train = self.get_hidden_data(train_set)  # get hidden values\n",
    "        z_test = self.get_hidden_data(test_set)  # get hidden values\n",
    "        np.savetxt(path + data_name + \"_train_z.csv\", z_train, delimiter=\",\", fmt='%f')\n",
    "        np.savetxt(path + data_name + \"_test_z.csv\", z_test, delimiter=\",\", fmt='%f')\n",
    "\n",
    "    \"**************************************************************************\"\n",
    "\n",
    "    def Save_Hidden_Data_Size(self, train_set, test_set, data_name, size, path):\n",
    "        z_train = self.get_hidden_data(train_set)  # get hidden values\n",
    "        z_test = self.get_hidden_data(test_set)  # get hidden values\n",
    "        np.savetxt(path + \"data/\" + data_name + \"_train_z_\" + str(size) + \".csv\", z_train, delimiter=\",\", fmt='%f')\n",
    "        np.savetxt(path + \"data/\" + data_name + \"_test_z_\" + str(size) + \".csv\", z_test, delimiter=\",\", fmt='%f')\n",
    "\n",
    "    \"******** Training End-to-End Early-stopping by Downhill Package *********\"\n",
    "\n",
    "    def End2end_Early_stopping(self, numpy_rng, dataset, n_validate, data_name,\n",
    "                               batch_size, end2end_lr, algo, norm, patience, validation):\n",
    "\n",
    "        train_X, test_X, actual = dataset\n",
    "        valid_x = train_X.get_value()[:n_validate]\n",
    "        train_x = train_X.get_value()[n_validate:]\n",
    "        \"for compute tm and vm before optimization process\"\n",
    "        t = theano.shared(numpy.asarray(train_x, dtype=theano.config.floatX), borrow=True)\n",
    "        v = theano.shared(numpy.asarray(valid_x, dtype=theano.config.floatX), borrow=True)\n",
    "\n",
    "        \"Training network by downhill\"\n",
    "        # 'adadelta' 'adagrad (default 0.01)' 'adam''esgd' 'nag''rmsprop' 'rprop' 'sgd'\n",
    "        opt = downhill.build(algo=algo, params=self.params, loss=self.end2end_cost, inputs=[self.x])\n",
    "        train = downhill.Dataset(train_x, batch_size=batch_size, rng=numpy_rng)\n",
    "        valid = downhill.Dataset(valid_x, batch_size=len(valid_x), rng=numpy_rng)\n",
    "\n",
    "        \"***** Monitor before optimization *****\"\n",
    "        stop_ep = 0\n",
    "        RE = np.empty([0, 3])\n",
    "        monitor = np.empty([0, 8])\n",
    "        #        LOSS = np.empty([0,3])\n",
    "        #        self.Plot_histogram_z(train_X, test_X, actual, 0, path)\n",
    "        # AUC before optimization\n",
    "        lof, cen, dis, kde, svm05, svm01, ae = self.Compute_AUC_Hidden(train_X, test_X, actual, norm, data_name)\n",
    "        a = np.column_stack([0, lof, cen, dis, kde, svm05, svm01, ae])\n",
    "        monitor = np.append(monitor, a)\n",
    "        # Loss components before optimization\n",
    "\n",
    "        #        loss = self.Recon_KL_Loss(t, batch_size)\n",
    "        #        LOSS = np.append(LOSS,[stop_ep, loss[0], loss[1]])\n",
    "        # Error before optimization\n",
    "        tm1, vm1 = self.Loss_train_valid(t, v)\n",
    "        RE = np.append(RE, np.column_stack([stop_ep, vm1, tm1]))\n",
    "\n",
    "        for tm1, vm1 in opt.iterate(train,  # 5, 5, 1e-2, 0.9\n",
    "                                    valid,\n",
    "                                    patience=patience,  # 10\n",
    "                                    validate_every=validation,  # 5\n",
    "                                    min_improvement=1e-3,  # 1e-3\n",
    "                                    # learning_rate =  end2end_lr,  # 1e-4\n",
    "                                    momentum=0.0,\n",
    "                                    nesterov=False):\n",
    "            stop_ep = stop_ep + 1\n",
    "            #            loss = self.Recon_KL_Loss(t, batch_size)\n",
    "            #            LOSS = np.append(LOSS,[stop_ep, loss[0], loss[1]])\n",
    "            #            \"******* Monitor optimization ******\"\n",
    "            if ((stop_ep % 200 == 0) and (stop_ep > 0)):\n",
    "                # self.Plot_histogram_z(train_X, test_X, actual, stop_ep, path)\n",
    "                lof, cen, dis, kde, svm05, svm01, ae = self.Compute_AUC_Hidden(train_X, test_X, actual, norm, data_name)\n",
    "                a = np.column_stack([stop_ep, lof, cen, dis, kde, svm05, svm01, ae])\n",
    "\n",
    "            monitor = np.append(monitor, a)\n",
    "            re = np.column_stack([stop_ep, vm1['loss'], tm1['loss']])\n",
    "            RE = np.append(RE, re)\n",
    "\n",
    "            if (stop_ep >= 1000):\n",
    "                break\n",
    "\n",
    "        # Plotting AUC and save to csv file\n",
    "        monitor = np.reshape(monitor, (-1, 8))\n",
    "        #        Plotting_Monitor(monitor, 0.4, 1.0, data_name, path)\n",
    "        #        np.savetxt(path + data_name + \"_monitor_auc1.csv\", monitor, delimiter=\",\", fmt='%f' )\n",
    "\n",
    "        #        LOSS = np.reshape(LOSS, (-1,3))\n",
    "        #        Plotting_Loss_Component(LOSS, RE, 0.0, 0.5, data_name, path)\n",
    "        #        np.savetxt(path + data_name + \"_loss_component.csv\", LOSS, delimiter=\",\", fmt='%f' )\n",
    "\n",
    "        RE = np.reshape(RE, (-1, 3))\n",
    "        #        Plotting_End2End_RE(RE, stop_ep, 0.0, 0.4, data_name, path)\n",
    "        #        np.savetxt(path +  data_name + \"_training_error1.csv\", RE, delimiter=\",\", fmt='%f' )\n",
    "\n",
    "        np.set_printoptions(precision=6, suppress=True)\n",
    "        print(\"\\n \", RE[stop_ep])\n",
    "\n",
    "        return RE[stop_ep]\n",
    "\n",
    "\n",
    "def test_SdA(pre_lr=0.01, end2end_lr=1e-4, algo='sgd',\n",
    "             dataset=[], data_name=\"WBC\", n_validate=0, norm=\"maxabs\",\n",
    "             batch_size=10, hidden_sizes=[1, 1, 1], corruptions=[0.0, 0.0, 0.0],\n",
    "             patience=1, validation=1):\n",
    "    numpy_rng = numpy.random.RandomState(89677)  # numpy random generator 89677\n",
    "    train_X, test_X, actual = dataset  # dataset is already normalised\n",
    "\n",
    "    input_size = train_X.get_value().shape[1]  # input size = dimension\n",
    "    train_x = train_X.get_value()[n_validate:]  # 80% for pre-training, 20% for validation\n",
    "    n_train_batches = train_x.shape[0]\n",
    "    n_train_batches //= batch_size  # number of batches for pre-training\n",
    "\n",
    "    # construct the stacked denoising autoencoder class\n",
    "    sda = SdA(numpy_rng=numpy_rng, n_ins=input_size,\n",
    "              hidden_layers_sizes=hidden_sizes)\n",
    "\n",
    "    RE = sda.End2end_Early_stopping(numpy_rng, dataset, n_validate, data_name,\n",
    "                                    batch_size, end2end_lr, algo, norm, patience, validation)\n",
    "    return sda, RE\n",
    "\n",
    "\n",
    "def Main_Test():\n",
    "    '''    list_data = [\"PageBlocks\", \"WPBC\", \"PenDigits\", \"GLASS\", \"Shuttle\", \"Arrhythmia\", \\\n",
    "                     \"CTU13_10\", \"CTU13_08\", \"CTU13_09\", \"CTU13_13\", \\\n",
    "                     \"Spambase\", \"UNSW\", \"NSLKDD\", \"InternetAds\"]'''\n",
    "    list_data=[\"UNSW\"]\n",
    "    norm = \"maxabs\"  # standard, maxabs[-1,1] or minmax[0,1]\n",
    "    corruptions = [0.1, 0.1, 0.1]\n",
    "\n",
    "    print(\"+ VAE: 0.05, Group\")\n",
    "    print(\"+ Data: \", list_data)\n",
    "    print(\"+ Scaler: \", norm)\n",
    "\n",
    "    AUC_Hidden = np.empty([0, 10])  # store auc of all hidden data\n",
    "    num = 0  # a counter\n",
    "    for data in list_data:\n",
    "        num = num + 1\n",
    "\n",
    "        h_sizes = hyper_parameters(data)  # Load hyper-parameters\n",
    "        train_set, test_set, actual = load_data(data)  # load original data\n",
    "\n",
    "        train_X, test_X = normalize_data(train_set, test_set, norm)  # Normalize data\n",
    "\n",
    "        train_X = theano.shared(numpy.asarray(train_X, dtype=theano.config.floatX), borrow=True)\n",
    "        test_X = theano.shared(numpy.asarray(test_X, dtype=theano.config.floatX), borrow=True)\n",
    "\n",
    "        datasets = [(train_X), (test_X), (actual)]  # Pack data for training AE\n",
    "\n",
    "        in_dim = train_set.shape[1]  # dimension of input data\n",
    "        n_vali = (int)(train_set.shape[0] / 5)  # size of validation set\n",
    "        n_train = len(train_set) - n_vali  # size of training set\n",
    "        # batch    = int(n_train/20)                           #Training set will be split training set into 20 batches\n",
    "        # print data information\n",
    "        pat, val, batch, n_batch = stopping_para_vae(n_train)\n",
    "\n",
    "        print(\"\\n\" + str(num) + \".\", data, \"...\")\n",
    "        print(\" + Hidden Sizes: \", in_dim, h_sizes, \"- Batch_sizes:\", batch)\n",
    "        print(\" + Data: %d (%d train, %d vali) - %d normal, %d anomaly\" \\\n",
    "              % (len(train_set), n_train, n_vali, \\\n",
    "                 len(test_set[(actual == 1)]), len(test_set[(actual == 0)])))\n",
    "\n",
    "        print(\" + Patience: %5.0d, Validate: %5.0d,  \\n + Batch size: %5.0d, n batch:%5.0d\" \\\n",
    "              % (pat, val, batch, n_batch))\n",
    "\n",
    "        AUC_RE = np.empty([0, 10])\n",
    "        # adadelta, 'adagrad' 'adam''esgd' 'nag''rmsprop' 'rprop' 'sgd'\n",
    "        # if (num==1):\n",
    "        sda, re = test_SdA(pre_lr=1e-2,  # re = [stop_ep, vm, tm]\n",
    "                           end2end_lr=1e-4,\n",
    "                           algo='adadelta',\n",
    "                           dataset=datasets,\n",
    "                           data_name=data,\n",
    "                           n_validate=n_vali,\n",
    "                           norm=norm,\n",
    "                           batch_size=batch,\n",
    "                           hidden_sizes=h_sizes,\n",
    "                           corruptions=corruptions,\n",
    "                           patience=pat,\n",
    "                           validation=val)\n",
    "\n",
    "        # Computer AUC on hidden data\n",
    "        lof, cen, dis, kde, svm05, svm01, ae = sda.Compute_AUC_Hidden(train_X, test_X, actual, norm, data)\n",
    "        auc_hidden = np.column_stack([batch, re[0], lof, cen, dis, kde, svm05, svm01, ae, 100 * re[2]])\n",
    "        AUC_Hidden = np.append(AUC_Hidden, auc_hidden)\n",
    "\n",
    "        # compute standard deviation of z\n",
    "        # sda.Compute_Std(train_X, test_X, actual, data, path)\n",
    "        # save hidden data to files\n",
    "        #        sda.Save_Hidden_Data(train_X, test_X, data, path)\n",
    "\n",
    "        # store AUC_input AUC_hidden and RE to AUC_RE for each data\n",
    "        #        AUC_RE   = np.append(AUC_RE, auc_hidden)\n",
    "        #        AUC_RE   = np.reshape(AUC_RE,(-1,10))\n",
    "        #\n",
    "        #        print(\"\\n+ AUC input, AUC hidden:\")\n",
    "        #        np.set_printoptions(precision=3, suppress=True)\n",
    "        #        column_list = [2,3,4,5,6,7,8,9]\n",
    "        #        print (AUC_RE[:,column_list])\n",
    "        #\n",
    "        #        AUC_Hidden = np.append(AUC_Hidden, auc_hidden)\n",
    "        AUC_Hidden = np.reshape(AUC_Hidden, (-1, 10))\n",
    "        np.set_printoptions(precision=3, suppress=True)\n",
    "        column_list = [2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        print(\"    LOF    CEN    MDIS   KDE   SVM5    SVM1    AE    RE*100\")\n",
    "        print(AUC_Hidden[:, column_list])\n",
    "        print(\"\\n\")\n",
    "\n",
    "    AUC_Hidden = np.reshape(AUC_Hidden, (-1, 10))\n",
    "    np.set_printoptions(precision=3, suppress=True)\n",
    "    column_list = [2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    print(\"    LOF    CEN    MDIS   KDE   SVM5    SVM1    AE    RE*100\")\n",
    "    print(AUC_Hidden[:, column_list])\n",
    "\n",
    "\n",
    "#    #store AUC_input and AUC_hidden to AUC_Input, AUC_Hidden\n",
    "#    AUC_Hidden  =  np.reshape(AUC_Hidden, (-1, 10))\n",
    "#    np.savetxt(path +  \"AUC_Hidden.csv\", AUC_Hidden, delimiter=\",\", fmt='%f' )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Main_Test()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}