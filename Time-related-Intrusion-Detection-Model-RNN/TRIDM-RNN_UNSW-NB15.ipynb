{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Dropout, Input\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.regularizers import Regularizer\n",
    "from tensorflow.python.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Default values.\n",
    "    train_set = 'data/UNSW_NB15_training-set.csv'\n",
    "    test_set = 'data/UNSW_NB15_testing-set.csv'\n",
    "    train = pd.read_csv(train_set, index_col='id')  # 指定“id”这一列数据作为行索引\n",
    "    test = pd.read_csv(test_set, index_col='id')  # 指定“id”这一列数据作为行索引\n",
    "\n",
    "    # 二分类数据\n",
    "    training_label = train['label'].values  # 将train的“label”这一列的值单独取出来\n",
    "    testing_label = test['label'].values  # 将test的“label”这一列的值单独取出来\n",
    "    temp_train = training_label\n",
    "    temp_test = testing_label\n",
    "\n",
    "    # Creates new dummy columns from each unique string in a particular feature 创建新的虚拟列\n",
    "    unsw = pd.concat([train, test])  # 将train和test拼接在一起\n",
    "    unsw = pd.get_dummies(data=unsw,\n",
    "                          columns=['proto', 'service', 'state'])  # 将'proto', 'service', 'state'这三列使用one-hot-encoder转变\n",
    "    # Normalising all numerical features:\n",
    "    unsw.drop(['label', 'attack_cat'], axis=1,\n",
    "              inplace=True)  # 删除'label', 'attack_cat'这两列，其中(inplace=True)是直接对原dataFrame进行操作\n",
    "    unsw_value = unsw.values\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))  # 初始化MinMaxScaler\n",
    "    unsw_value = scaler.fit_transform(unsw_value)  # 将待处理数据矩阵进行归一化\n",
    "    train_set = unsw_value[:len(train), :]  # 分离出train集\n",
    "    test_set = unsw_value[len(train):, :]  # 分离出test集\n",
    "\n",
    "    # return train_set, training_label, test_set, testing_label\n",
    "    return train_set, temp_train, test_set, temp_test\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Sparse(Regularizer):\n",
    "    def __init__(self, rho=0.005, alpha=10):\n",
    "        self.rho = rho\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, x):\n",
    "        rho_hat = K.mean(x)\n",
    "        regularization = self.rho * K.log(self.rho / rho_hat) + (1 - self.rho) * K.log((1 - self.rho) / (1 - rho_hat))\n",
    "        return self.alpha * regularization\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'rho:': float(self.rho),\n",
    "                'alpha': float(self.alpha)\n",
    "                }\n",
    "\n",
    "def build_SAE(rho=0.05):\n",
    "\n",
    "    # first step is to define a sSAE and pre-training\n",
    "    # Layer 1\n",
    "    input_traffic = Input(shape=(196, ))\n",
    "    distorted_input1 = Dropout(0.1)(input_traffic)\n",
    "    encoded_1 = Dense(128, activation='relu', activity_regularizer=Sparse(rho))(distorted_input1)\n",
    "    encoded_1_bn = BatchNormalization()(encoded_1)\n",
    "    decoded_1 = Dense(196, activation='relu', activity_regularizer=Sparse(rho))(encoded_1_bn)\n",
    "\n",
    "    autoendoder_1 = Model(inputs=input_traffic, outputs=decoded_1)\n",
    "    encoder_1 = Model(inputs=input_traffic, outputs=encoded_1_bn)\n",
    "\n",
    "    # Layer 2\n",
    "    encoded1_input = Input(shape=(128, ))\n",
    "    distorted_input2 = Dropout(0.1)(encoded1_input)\n",
    "    encoded_2 = Dense(32, activation='relu', activity_regularizer=Sparse(rho))(distorted_input2)\n",
    "    encoded_2_bn = BatchNormalization()(encoded_2)\n",
    "    decoded_2 = Dense(128, activation='relu', activity_regularizer=Sparse(rho))(encoded_2_bn)\n",
    "\n",
    "    autoendoder_2 = Model(inputs=encoded1_input, outputs=decoded_2)\n",
    "    encoder_2 = Model(inputs=encoded1_input, outputs=encoded_2_bn)\n",
    "\n",
    "    # Layer 3\n",
    "    encoded2_input = Input(shape=(32, ))\n",
    "    distorted_input3 = Dropout(0.1)(encoded2_input)\n",
    "    encoded_3 = Dense(32, activation='relu', activity_regularizer=Sparse(rho))(distorted_input3)\n",
    "    encoded_3_bn = BatchNormalization()(encoded_3)\n",
    "    decoded_3 = Dense(32, activation='relu', activity_regularizer=Sparse(rho))(encoded_3_bn)\n",
    "\n",
    "    autoendoder_3 = Model(inputs=encoded2_input, outputs=decoded_3)\n",
    "    encoder_3 = Model(inputs=encoded2_input, outputs=encoded_3_bn)\n",
    "\n",
    "    optimize_1 = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "    autoendoder_1.compile(loss='mse', optimizer=optimize_1)\n",
    "    encoder_1.compile(loss='mse', optimizer=optimize_1)\n",
    "\n",
    "    optimize_2 = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "    autoendoder_2.compile(loss='mse', optimizer=optimize_2)\n",
    "    encoder_2.compile(loss='mse', optimizer=optimize_2)\n",
    "\n",
    "    optimize_3 = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "    autoendoder_3.compile(loss='mse', optimizer=optimize_3)\n",
    "    encoder_3.compile(loss='mse', optimizer=optimize_3)\n",
    "\n",
    "    model_input = Input(shape=(196,))\n",
    "    model_encoded_1 = Dense(128, activation='relu')(model_input)\n",
    "    model_encoded1_bn = BatchNormalization()(model_encoded_1)\n",
    "    model_encoded_2 = Dense(32, activation='relu')(model_encoded1_bn)\n",
    "    model_encoded2__bn = BatchNormalization()(model_encoded_2)\n",
    "    model_encoded_3 = Dense(32, activation='relu')(model_encoded2__bn)\n",
    "    model_encoded3__bn = BatchNormalization()(model_encoded_3)\n",
    "\n",
    "    model_decoded_3 = Dense(32, activation='relu')(model_encoded3__bn)\n",
    "    model_decoded_2 = Dense(128, activation='relu')(model_decoded_3)\n",
    "    model_decoded_1 = Dense(196, activation='relu')(model_decoded_2)\n",
    "\n",
    "    ae_model = Model(inputs=model_input, outputs=model_decoded_1)\n",
    "    ae_encoder = Model(inputs=model_input, outputs=model_encoded3__bn)\n",
    "    optimize = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "    ae_model.compile(loss='mse', optimizer=optimize)\n",
    "\n",
    "    # second step is to define a classifier and fine-tuning\n",
    "\n",
    "\n",
    "    return autoendoder_1, encoder_1, autoendoder_2, encoder_2, autoendoder_3, encoder_3, ae_model, ae_encoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load data\n",
    "print(\"Load data...\")\n",
    "train, train_label, test, test_label = load_data()\n",
    "print(\"train shape: \", train.shape)\n",
    "train_label = train_label.reshape((-1, 1))\n",
    "test_label = test_label.reshape((-1, 1))\n",
    "print(\"train_label shape: \", train_label.shape)\n",
    "\n",
    "# build model\n",
    "print(\"Build AE model\")\n",
    "autoencoder_1, encoder_1, autoencoder_2, encoder_2, autoencoder_3, encoder_3, sSAE, sSAE_encoder = build_SAE(rho=0.04)\n",
    "\n",
    "print(\"Start pre-training....\")\n",
    "\n",
    "# fit the first layer, 在此处添加validation_data=test，加上callbacks，记录的是val_loss，取最小的那个\n",
    "print(\"First layer training....\")\n",
    "AE_1_dir = os.path.join(os.getcwd(), 'saved_ae_1')\n",
    "ae_1_filepath=\"best_ae_1.hdf5\"\n",
    "ae_1_point = ModelCheckpoint(os.path.join(AE_1_dir, ae_1_filepath), monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "ae_1_stops = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
    "autoencoder_1.fit(train, train, epochs=100, batch_size=1024, validation_data=(test, test), verbose=0, shuffle=True, callbacks=[ae_1_point, ae_1_stops])\n",
    "\n",
    "autoencoder_1.load_weights('./saved_ae_1/best_ae_1.hdf5')\n",
    "first_layer_output = encoder_1.predict(train)  # 在此使用loss最小的那个模型\n",
    "test_first_out = encoder_1.predict(test)\n",
    "print(\"The shape of first layer output is: \", first_layer_output.shape)\n",
    "\n",
    "# fit the second layer\n",
    "print(\"Second layer training....\")\n",
    "AE_2_dir = os.path.join(os.getcwd(), 'saved_ae_2')\n",
    "ae_2_filepath=\"best_ae_2.hdf5\"\n",
    "ae_2_point = ModelCheckpoint(os.path.join(AE_2_dir, ae_2_filepath), monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "ae_2_stops = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
    "autoencoder_2.fit(first_layer_output, first_layer_output, epochs=100, batch_size=512, verbose=0, validation_data=(test_first_out, test_first_out), shuffle=True, callbacks=[ae_2_point, ae_2_stops])\n",
    "\n",
    "autoencoder_2.load_weights('./saved_ae_2/best_ae_2.hdf5')\n",
    "second_layer_output = encoder_2.predict(first_layer_output)\n",
    "test_second_out = encoder_2.predict(test_first_out)\n",
    "print(\"The shape of second layer output is: \", second_layer_output.shape)\n",
    "\n",
    "# fit the third layer\n",
    "print(\"Third layer training....\")\n",
    "AE_3_dir = os.path.join(os.getcwd(), 'saved_ae_3')\n",
    "ae_3_filepath=\"best_ae_3.hdf5\"\n",
    "ae_3_point = ModelCheckpoint(os.path.join(AE_3_dir, ae_3_filepath), monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "ae_3_stops = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
    "autoencoder_3.fit(second_layer_output, second_layer_output, epochs=100, batch_size=512, verbose=0, validation_data=(test_second_out, test_second_out), shuffle=True, callbacks=[ae_3_point, ae_3_stops])\n",
    "autoencoder_3.load_weights('./saved_ae_3/best_ae_3.hdf5')\n",
    "\n",
    "print(\"Pass the weights to sSAE_encoder...\")\n",
    "sSAE_encoder.layers[1].set_weights(autoencoder_1.layers[2].get_weights())  # first Dense\n",
    "sSAE_encoder.layers[2].set_weights(autoencoder_1.layers[3].get_weights())  # first BN\n",
    "sSAE_encoder.layers[3].set_weights(autoencoder_2.layers[2].get_weights())  # second Dense\n",
    "sSAE_encoder.layers[4].set_weights(autoencoder_2.layers[3].get_weights())  # second BN\n",
    "sSAE_encoder.layers[5].set_weights(autoencoder_3.layers[2].get_weights())  # third Dense\n",
    "sSAE_encoder.layers[6].set_weights(autoencoder_3.layers[3].get_weights())  # third BN\n",
    "\n",
    "encoded_train = sSAE_encoder.predict(train)\n",
    "encoded_test = sSAE_encoder.predict(test)\n",
    "\n",
    "np.save('data/encoded_train.npy', encoded_train)\n",
    "np.save('data/train_label.npy', train_label)\n",
    "np.save('data/encoded_test.npy', encoded_test)\n",
    "np.save('data/test_label.npy', test_label)\n",
    "\n",
    "# 级联两层Dense 最后加一个softmax\n",
    "mlp0 = Dense(units=32, activation='relu')(sSAE_encoder.output)\n",
    "lstm_reshape = Reshape((1, 32))(mlp0)\n",
    "\n",
    "lstm = LSTM(units=16, activation='tanh', return_sequences=False)(lstm_reshape)\n",
    "lstm_drop = Dropout(0.3)(lstm)\n",
    "\n",
    "mlp = Dense(units=10, activation='relu')(lstm_drop)\n",
    "mlp2 = Dense(units=1, activation='sigmoid')(mlp)\n",
    "\n",
    "\n",
    "classifier = Model(sSAE_encoder.input, mlp2)\n",
    "optimize = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "classifier.compile(optimizer=optimize, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models_temp')\n",
    "filepath=\"best_model.hdf5\"\n",
    "checkpoint = ModelCheckpoint(os.path.join(save_dir, filepath), monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_grads=True,\n",
    "                         write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "reduc_lr = ReduceLROnPlateau(monitor='val_accuracy', patience=10, mode='max', factor=0.2, epsilon=0.0001)\n",
    "\n",
    "history = classifier.fit(train, train_label, epochs=100, batch_size=1024, validation_data=(test, test_label), callbacks=[checkpoint, tbCallBack, reduc_lr], verbose=2)\n",
    "classifier.load_weights('saved_models_temp/best_model.hdf5')\n",
    "\n",
    "# 保存下最好的模型，然后重新构建一个模型，毕竟这里只是一个预训练的过程。\n",
    "# 然后，在后面的过程中，首先使用的AE，并且对结果进行TimesereisGenerator\n",
    "# 最后，输入到LSTM中实现分类。\n",
    "train_y = classifier.predict(train)\n",
    "train_pred = train_y > 0.5\n",
    "\n",
    "test_y = classifier.predict(test)\n",
    "test_pred = test_y > 0.5\n",
    "\n",
    "print(confusion_matrix(train_label, train_pred))\n",
    "print(confusion_matrix(test_label, test_pred))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# 加载数据\n",
    "\n",
    "train_all = np.load('data/encoded_train.npy')  # (175341, 32)\n",
    "train_all_label = np.load('data/train_label.npy')  # (175341, 1)\n",
    "test_all = np.load('data/encoded_test.npy')\n",
    "test_all_label = np.load('data/test_label.npy')\n",
    "\n",
    "# 利用TimesereisGenerator生成序列数据\n",
    "time_steps = 8\n",
    "batch_size = 1024\n",
    "\n",
    "# 先把训练集划分出一部分作为验证集\n",
    "train = train_all[:(172032 + time_steps), :]  # 4096 * 42 = 172032\n",
    "train_label = train_all_label[:(172032 + time_steps), :]\n",
    "test = test_all[:(81920 + time_steps), :]  # 4096 * 20 = 81920\n",
    "test_label = test_all_label[:(81920 + time_steps), :]\n",
    "# val_data = train_all[int(len(train_all)* 0.7):, :]\n",
    "# val_label = train_all_label[int(len(train_all)* 0.7):, :]\n",
    "# print(train.shape[0])\n",
    "# print(val_data.shape[0])\n",
    "# 数据集生成器\n",
    "train_label_ = np.insert(train_label, 0, 0, axis=0)\n",
    "test_label_ = np.insert(test_label, 0, 0, axis=0)\n",
    "# val_label_ = np.insert(val_label, 0, 0)\n",
    "train_generator = TimeseriesGenerator(train, train_label_[:-1], length=time_steps, sampling_rate=1,\n",
    "                                      batch_size=batch_size)\n",
    "test_generator = TimeseriesGenerator(test, test_label_[:-1], length=time_steps, sampling_rate=1, batch_size=batch_size)\n",
    "# val_generator = TimeseriesGenerator(val_data, val_label_[:-1], length=time_steps, sampling_rate=1, batch_size=batch_size)\n",
    "\n",
    "# 构造模型\n",
    "# input_traffic = Input((time_steps, 32))\n",
    "input_traffic = Input(shape=(time_steps, 32))\n",
    "# 1 lstm layer, stateful=True\n",
    "\n",
    "# GRU/LSTM\n",
    "'''GRU1=Bidirectional(GRU(units=24, activation='tanh',\n",
    "                           return_sequences=True, recurrent_dropout=0.1))(input_traffic)\n",
    "GRU_drop1 = Dropout(0.5)(GRU1)\n",
    "\n",
    "GRU2=Bidirectional(GRU(units=12, activation='tanh',\n",
    "                           return_sequences=False, recurrent_dropout=0.1))(GRU_drop1)\n",
    "GRU_drop2 = Dropout(0.5)(GRU2)'''\n",
    "lstm1 = Bidirectional(LSTM(units=24, activation='tanh',\n",
    "                           return_sequences=True, recurrent_dropout=0.1))(input_traffic)\n",
    "lstm_drop1 = Dropout(0.5)(lstm1)\n",
    "# 2 lstm layer, stateful=True\n",
    "lstm2 = Bidirectional(LSTM(units=12, activation='tanh', return_sequences=False,\n",
    "                           recurrent_dropout=0.1))(lstm_drop1)\n",
    "lstm_drop2 = Dropout(0.5)(lstm2)\n",
    "# lstm3 = Bidirectional(LSTM(units=8, activation='tanh', return_sequences=False,\n",
    "#                            recurrent_dropout=0.1))(lstm_drop2)\n",
    "# lstm_drop2 = Dropout(0.5)(lstm_drop1)\n",
    "# mlp\n",
    "mlp = Dense(units=6, activation='relu')(lstm_drop2)\n",
    "mlp2 = Dense(units=1, activation='sigmoid')(mlp)\n",
    "classifier = Model(input_traffic, mlp2)\n",
    "optimize = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "classifier.compile(optimizer=optimize, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 设置一些callbacks\n",
    "save_dir = os.path.join(os.getcwd(), 'models')\n",
    "filepath = \"best_model.hdf5\"\n",
    "checkpoint = ModelCheckpoint(os.path.join(save_dir, filepath), monitor='val_accuracy', verbose=1, save_best_only=True,\n",
    "                             mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_grads=True,\n",
    "                         write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "reduc_lr = ReduceLROnPlateau(monitor='val_accuracy', patience=10, mode='max', factor=0.2, min_delta=0.0001)\n",
    "\n",
    "# 拟合及预测\n",
    "history = classifier.fit_generator(train_generator, epochs=40, verbose=2, steps_per_epoch=168,\n",
    "                                   callbacks=[checkpoint, tbCallBack, reduc_lr],\n",
    "                                   validation_data=test_generator, shuffle=0, validation_steps=80)\n",
    "\n",
    "classifier.load_weights('./models/best_model.hdf5')\n",
    "train_probabilities = classifier.predict_generator(train_generator, verbose=1)\n",
    "\n",
    "train_pred = train_probabilities > 0.5\n",
    "train_label_original = train_label_[(time_steps - 1):-2, :]\n",
    "\n",
    "test_probabilities = classifier.predict_generator(test_generator, verbose=1)\n",
    "test_pred = test_probabilities > 0.5\n",
    "test_label_original = test_label_[(time_steps - 1):-2, ]\n",
    "np.save('data/plot_prediction.npy', test_pred)\n",
    "np.save('data/plot_original.npy', test_label_original)\n",
    "# tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print('Trainset Confusion Matrix')\n",
    "print(confusion_matrix(train_label_original, train_pred))\n",
    "print('Testset Confusion Matrix')\n",
    "print(confusion_matrix(test_label_original, test_pred))\n",
    "print('Classification Report')\n",
    "\n",
    "print(classification_report(test_label_original, test_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 本程序是用于绘制出测试数据的判决情况，并采用一个相似度来衡量\n",
    "# 参考网址：https://matplotlib.org/gallery/recipes/transparent_legends.html#sphx-glr-gallery-recipes-transparent-legends-py\n",
    "# https://matplotlib.org/gallery/lines_bars_and_markers/cohere.html#sphx-glr-gallery-lines-bars-and-markers-cohere-py\n",
    "# 用蓝色表示原始的数据，用橙色表示预测的数据，其中预测若出现失误，则对应点采用的是红色的，即共有三种颜色。\n",
    "# 第二幅图画出的是预测错误的累计个数\n",
    "\n",
    "\n",
    "# Two signals with a coherent part at 10Hz and a random part\n",
    "s1 = np.load('data/plot_original.npy')\n",
    "s2 = np.load('data/plot_prediction.npy')\n",
    "test_pred = s2 > 0.5\n",
    "index_same = np.argwhere(s1 == test_pred)\n",
    "index_diff = np.argwhere(s1 != test_pred)\n",
    "\n",
    "print(index_same.shape)\n",
    "print(index_diff.shape)\n",
    "dt = 1.0\n",
    "t = np.arange(0, len(s1), dt)\n",
    "s3 = np.ones(len(s1)) * 0.5\n",
    "fig = plt.figure(1)\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "font1 = {'family': 'Times New Roman',\n",
    "         'weight': 'normal',\n",
    "         'size': 14,\n",
    "         }\n",
    "font2 = {'family': 'Times New Roman',\n",
    "         'weight': 'normal',\n",
    "         'size': 14,\n",
    "         }\n",
    "ax1.plot(t, s1, markersize=8, label='True Label', color='blue', marker='o', linestyle='-')\n",
    "ax1.plot(t[index_same[:, 0]], s2[index_same[:, 0]], markersize=2, label='Correct Prediction', color='orange',\n",
    "         marker='s', linestyle='')\n",
    "ax1.plot(t[index_diff[:, 0]], s2[index_diff[:, 0]], markersize=0.5, label='Wrong Prediction', color='red', marker='s',\n",
    "         linestyle='')\n",
    "ax1.plot(t, s3, 'r-')\n",
    "ax1.set_ylim(-0.3, 1.5)\n",
    "ax1.set_xlabel('samples', font2)\n",
    "ax1.set_ylabel('Probability of Each Sample', font2)\n",
    "ax1.legend(loc='upper right', prop=font2)\n",
    "plt.title('Comparsion of Prediction and True Labels', font1)\n",
    "ax1.grid(True)\n",
    "plt.show()\n",
    "# 画出累计\n",
    "fig2 = plt.figure(1)\n",
    "ax2 = fig2.add_subplot(111)\n",
    "count_line = np.zeros(len(s1))\n",
    "index_low = 0\n",
    "index_high = 0\n",
    "for i, index in enumerate(index_diff):\n",
    "    index_high = index[0]\n",
    "    count_line[index_low:index_high] = i\n",
    "    index_low = index_high\n",
    "count_line[81918:] = 1506\n",
    "ax2.plot(t, count_line)\n",
    "plt.title('Cumulative Amount of Incorrect Predictions', font1)\n",
    "ax2.set_xlabel('samples', font2)\n",
    "ax2.set_ylabel('Number of Incorrect Predictions', font2)\n",
    "\n",
    "# plt.subplots_adjust(wspace=0., hspace =0.3)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}