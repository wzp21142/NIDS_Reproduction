{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and\n",
    "# Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\n",
    "# Copyright (C) 2017  Abien Fred Agarap\n",
    "#\n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU Affero General Public License as published\n",
    "# by the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU Affero General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU Affero General Public License\n",
    "# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "\"\"\"Implementation of the GRU+SVM model [http://arxiv.org/abs/1709.03082] by A.F. Agarap\"\"\"\n",
    "\n",
    "__version__ = \"0.3.11\"\n",
    "__author__ = \"Abien Fred Agarap\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "\n",
    "class GruSvm:\n",
    "    \"\"\"Implementation of the GRU+SVM model using TensorFlow\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha,\n",
    "        batch_size,\n",
    "        cell_size,\n",
    "        dropout_rate,\n",
    "        num_classes,\n",
    "        sequence_length,\n",
    "        svm_c,\n",
    "    ):\n",
    "        \"\"\"Initialize the GRU+SVM class\n",
    "\n",
    "        Parameter\n",
    "        ---------\n",
    "        alpha : float\n",
    "          The learning rate for the GRU+Softmax model.\n",
    "        batch_size : int\n",
    "          The number of batches to use for training/validation/testing.\n",
    "        cell_size : int\n",
    "          The size of cell state.\n",
    "        dropout_rate : float\n",
    "          The dropout rate to be used.\n",
    "        num_classes : int\n",
    "          The number of classes in a dataset.\n",
    "        sequence_length : int\n",
    "          The number of features in a dataset.\n",
    "        svm_c : float\n",
    "          The SVM penalty parameter C.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        self.cell_size = cell_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_classes = num_classes\n",
    "        self.sequence_length = sequence_length\n",
    "        self.svm_c = svm_c\n",
    "\n",
    "        def __graph__():\n",
    "            \"\"\"Build the inference graph\"\"\"\n",
    "            with tf.name_scope(\"input\"):\n",
    "                # [BATCH_SIZE, SEQUENCE_LENGTH]\n",
    "                x_input = tf.placeholder(\n",
    "                    dtype=tf.uint8, shape=[None, self.sequence_length], name=\"x_input\"\n",
    "                )\n",
    "\n",
    "                # [BATCH_SIZE, SEQUENCE_LENGTH, 10]\n",
    "                x_onehot = tf.one_hot(\n",
    "                    indices=x_input,\n",
    "                    depth=10,\n",
    "                    on_value=1.0,\n",
    "                    off_value=0.0,\n",
    "                    name=\"x_onehot\",\n",
    "                )\n",
    "\n",
    "                # [BATCH_SIZE]\n",
    "                y_input = tf.placeholder(dtype=tf.uint8, shape=[None], name=\"y_input\")\n",
    "\n",
    "                # [BATCH_SIZE, N_CLASSES]\n",
    "                y_onehot = tf.one_hot(\n",
    "                    indices=y_input,\n",
    "                    depth=self.num_classes,\n",
    "                    on_value=1.0,\n",
    "                    off_value=-1.0,\n",
    "                    name=\"y_onehot\",\n",
    "                )\n",
    "\n",
    "            state = tf.placeholder(\n",
    "                dtype=tf.float32, shape=[None, self.cell_size], name=\"initial_state\"\n",
    "            )\n",
    "\n",
    "            p_keep = tf.placeholder(dtype=tf.float32, name=\"p_keep\")\n",
    "            learning_rate = tf.placeholder(dtype=tf.float32, name=\"learning_rate\")\n",
    "\n",
    "            cell = tf.contrib.rnn.GRUCell(self.cell_size)\n",
    "            drop_cell = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=p_keep)\n",
    "\n",
    "            # outputs: [BATCH_SIZE, SEQUENCE_LENGTH, CELL_SIZE]\n",
    "            # states: [BATCH_SIZE, CELL_SIZE]\n",
    "            outputs, states = tf.nn.dynamic_rnn(\n",
    "                drop_cell, x_onehot, initial_state=state, dtype=tf.float32\n",
    "            )\n",
    "\n",
    "            states = tf.identity(states, name=\"H\")\n",
    "\n",
    "            with tf.name_scope(\"final_training_ops\"):\n",
    "                with tf.name_scope(\"weights\"):\n",
    "                    weight = tf.get_variable(\n",
    "                        \"weights\",\n",
    "                        initializer=tf.random_normal(\n",
    "                            [self.cell_size, self.num_classes], stddev=0.01\n",
    "                        ),\n",
    "                    )\n",
    "                    self.variable_summaries(weight)\n",
    "                with tf.name_scope(\"biases\"):\n",
    "                    bias = tf.get_variable(\n",
    "                        \"biases\", initializer=tf.constant(0.1, shape=[self.num_classes])\n",
    "                    )\n",
    "                    self.variable_summaries(bias)\n",
    "                hf = tf.transpose(outputs, [1, 0, 2])\n",
    "                last = tf.gather(hf, int(hf.get_shape()[0]) - 1)\n",
    "                with tf.name_scope(\"Wx_plus_b\"):\n",
    "                    output = tf.matmul(last, weight) + bias\n",
    "                    tf.summary.histogram(\"pre-activations\", output)\n",
    "\n",
    "            # L2-SVM\n",
    "            with tf.name_scope(\"svm\"):\n",
    "                regularization_loss = 0.5 * tf.reduce_sum(tf.square(weight))\n",
    "                hinge_loss = tf.reduce_sum(\n",
    "                    tf.square(\n",
    "                        tf.maximum(\n",
    "                            tf.zeros([self.batch_size, self.num_classes]),\n",
    "                            1 - y_onehot * output,\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                with tf.name_scope(\"loss\"):\n",
    "                    loss = regularization_loss + self.svm_c * hinge_loss\n",
    "            tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(\n",
    "                loss\n",
    "            )\n",
    "\n",
    "            with tf.name_scope(\"accuracy\"):\n",
    "                predicted_class = tf.sign(output)\n",
    "                predicted_class = tf.identity(predicted_class, name=\"prediction\")\n",
    "                with tf.name_scope(\"correct_prediction\"):\n",
    "                    correct = tf.equal(\n",
    "                        tf.argmax(predicted_class, 1), tf.argmax(y_onehot, 1)\n",
    "                    )\n",
    "                with tf.name_scope(\"accuracy\"):\n",
    "                    accuracy = tf.reduce_mean(tf.cast(correct, \"float\"))\n",
    "            tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "            # merge all the summaries collected from the TF graph\n",
    "            merged = tf.summary.merge_all()\n",
    "\n",
    "            # set class properties\n",
    "            self.x_input = x_input\n",
    "            self.y_input = y_input\n",
    "            self.y_onehot = y_onehot\n",
    "            self.p_keep = p_keep\n",
    "            self.loss = loss\n",
    "            self.optimizer = optimizer\n",
    "            self.state = state\n",
    "            self.states = states\n",
    "            self.learning_rate = learning_rate\n",
    "            self.predicted_class = predicted_class\n",
    "            self.accuracy = accuracy\n",
    "            self.merged = merged\n",
    "\n",
    "        sys.stdout.write(\"\\n<log> Building Graph...\")\n",
    "        __graph__()\n",
    "        sys.stdout.write(\"</log>\\n\")\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        checkpoint_path,\n",
    "        log_path,\n",
    "        model_name,\n",
    "        epochs,\n",
    "        train_data,\n",
    "        train_size,\n",
    "        validation_data,\n",
    "        validation_size,\n",
    "        result_path,\n",
    "    ):\n",
    "        \"\"\"Trains the model\n",
    "\n",
    "        Parameter\n",
    "        ---------\n",
    "        checkpoint_path : str\n",
    "          The path where to save the trained model.\n",
    "        log_path : str\n",
    "          The path where to save the TensorBoard summaries.\n",
    "        model_name : str\n",
    "          The filename for the trained model.\n",
    "        epochs : int\n",
    "          The number of passes through the whole dataset.\n",
    "        train_data : numpy.ndarray\n",
    "          The NumPy array training dataset.\n",
    "        train_size : int\n",
    "          The size of `train_data`.\n",
    "        validation_data : numpy.ndarray\n",
    "          The NumPy array testing dataset.\n",
    "        validation_size : int\n",
    "          The size of `validation_data`.\n",
    "        result_path : str\n",
    "          The path where to save the actual and predicted classes array.\n",
    "        \"\"\"\n",
    "\n",
    "        if not os.path.exists(path=checkpoint_path):\n",
    "            os.mkdir(path=checkpoint_path)\n",
    "\n",
    "        saver = tf.train.Saver(max_to_keep=1000)\n",
    "\n",
    "        # initialize H (current_state) with values of zeros\n",
    "        current_state = np.zeros([self.batch_size, self.cell_size])\n",
    "\n",
    "        # variables initializer\n",
    "        init_op = tf.group(\n",
    "            tf.global_variables_initializer(), tf.local_variables_initializer()\n",
    "        )\n",
    "\n",
    "        # get the time tuple\n",
    "        timestamp = str(time.asctime())\n",
    "\n",
    "        train_writer = tf.summary.FileWriter(\n",
    "            logdir=os.path.join(log_path, (timestamp + \"-training\").replace(' ','_').replace(':','_')),\n",
    "            graph=tf.get_default_graph(),\n",
    "        )\n",
    "        validation_writer = tf.summary.FileWriter(\n",
    "            logdir=os.path.join(log_path, (timestamp + \"-validation\").replace(' ','_').replace(':','_')),\n",
    "            graph=tf.get_default_graph(),\n",
    "        )\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_op)\n",
    "\n",
    "            checkpoint = tf.train.get_checkpoint_state(checkpoint_path)\n",
    "\n",
    "            if checkpoint and checkpoint.model_checkpoint_path:\n",
    "                saver = tf.train.import_meta_graph(\n",
    "                    checkpoint.model_checkpoint_path + \".meta\"\n",
    "                )\n",
    "                saver.restore(sess, tf.train.latest_checkpoint(checkpoint_path))\n",
    "\n",
    "            try:\n",
    "                for step in range(epochs * train_size // self.batch_size):\n",
    "\n",
    "                    # set the value for slicing\n",
    "                    # e.g. step = 0, batch_size = 256, train_size = 1898240\n",
    "                    # (0 * 256) % 1898240 = 0\n",
    "                    # [offset:(offset + batch_size)] = [0:256]\n",
    "                    offset = (step * self.batch_size) % train_size\n",
    "                    train_example_batch = train_data[0][\n",
    "                        offset : (offset + self.batch_size)\n",
    "                    ]\n",
    "                    train_label_batch = train_data[1][\n",
    "                        offset : (offset + self.batch_size)\n",
    "                    ]\n",
    "\n",
    "                    # dictionary for key-value pair input for training\n",
    "                    feed_dict = {\n",
    "                        self.x_input: train_example_batch,\n",
    "                        self.y_input: train_label_batch,\n",
    "                        self.state: current_state,\n",
    "                        self.learning_rate: self.alpha,\n",
    "                        self.p_keep: self.dropout_rate,\n",
    "                    }\n",
    "\n",
    "                    train_summary, _, predictions, actual, next_state = sess.run(\n",
    "                        [\n",
    "                            self.merged,\n",
    "                            self.optimizer,\n",
    "                            self.predicted_class,\n",
    "                            self.y_onehot,\n",
    "                            self.states,\n",
    "                        ],\n",
    "                        feed_dict=feed_dict,\n",
    "                    )\n",
    "\n",
    "                    # Display training loss and accuracy every 100 steps and at step 0\n",
    "                    if step % 100 == 0:\n",
    "                        # get train loss and accuracy\n",
    "                        train_loss, train_accuracy = sess.run(\n",
    "                            [self.loss, self.accuracy], feed_dict=feed_dict\n",
    "                        )\n",
    "\n",
    "                        # display train loss and accuracy\n",
    "                        print(\n",
    "                            \"step [{}] train -- loss : {}, accuracy : {}\".format(\n",
    "                                step, train_loss, train_accuracy\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                        # write the train summary\n",
    "                        train_writer.add_summary(train_summary, step)\n",
    "\n",
    "                        # save the model at current step\n",
    "                        saver.save(\n",
    "                            sess=sess,\n",
    "                            save_path=os.path.join(checkpoint_path, model_name),\n",
    "                            global_step=step,\n",
    "                        )\n",
    "\n",
    "                    current_state = next_state\n",
    "\n",
    "                    self.save_labels(\n",
    "                        predictions=predictions,\n",
    "                        actual=actual,\n",
    "                        result_path=result_path,\n",
    "                        step=step,\n",
    "                        phase=\"training\",\n",
    "                    )\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Training interrupted at {}\".format(step))\n",
    "                os._exit(1)\n",
    "            finally:\n",
    "                print(\"EOF -- Training done at step {}\".format(step))\n",
    "\n",
    "                for step in range(epochs * validation_size // self.batch_size):\n",
    "\n",
    "                    offset = (step * self.batch_size) % validation_size\n",
    "                    test_example_batch = validation_data[0][\n",
    "                        offset : (offset + self.batch_size)\n",
    "                    ]\n",
    "                    test_label_batch = validation_data[1][\n",
    "                        offset : (offset + self.batch_size)\n",
    "                    ]\n",
    "\n",
    "                    # dictionary for key-value pair input for validation\n",
    "                    feed_dict = {\n",
    "                        self.x_input: test_example_batch,\n",
    "                        self.y_input: test_label_batch,\n",
    "                        self.state: np.zeros([self.batch_size, self.cell_size]),\n",
    "                        self.p_keep: 1.0,\n",
    "                    }\n",
    "\n",
    "                    (\n",
    "                        validation_summary,\n",
    "                        predictions,\n",
    "                        actual,\n",
    "                        validation_loss,\n",
    "                        validation_accuracy,\n",
    "                    ) = sess.run(\n",
    "                        [\n",
    "                            self.merged,\n",
    "                            self.predicted_class,\n",
    "                            self.y_onehot,\n",
    "                            self.loss,\n",
    "                            self.accuracy,\n",
    "                        ],\n",
    "                        feed_dict=feed_dict,\n",
    "                    )\n",
    "\n",
    "                    # Display validation loss and accuracy every 100 steps\n",
    "                    if step % 100 == 0 and step > 0:\n",
    "\n",
    "                        # add the validation summary\n",
    "                        validation_writer.add_summary(validation_summary, step)\n",
    "\n",
    "                        # display validation loss and accuracy\n",
    "                        print(\n",
    "                            \"step [{}] validation -- loss : {}, accuracy : {}\".format(\n",
    "                                step, validation_loss, validation_accuracy\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                    self.save_labels(\n",
    "                        predictions=predictions,\n",
    "                        actual=actual,\n",
    "                        result_path=result_path,\n",
    "                        step=step,\n",
    "                        phase=\"validation\",\n",
    "                    )\n",
    "\n",
    "                print(\"EOF -- Testing done at step {}\".format(step))\n",
    "\n",
    "    @staticmethod\n",
    "    def predict(\n",
    "        batch_size,\n",
    "        cell_size,\n",
    "        dropout_rate,\n",
    "        num_classes,\n",
    "        test_data,\n",
    "        test_size,\n",
    "        checkpoint_path,\n",
    "        result_path,\n",
    "    ):\n",
    "        \"\"\"Classifies the data whether there is an intrusion or none\n",
    "\n",
    "        Parameter\n",
    "        ---------\n",
    "        batch_size : int\n",
    "          The number of batches to use for training/validation/testing.\n",
    "        cell_size : int\n",
    "          The size of cell state.\n",
    "        dropout_rate : float\n",
    "          The dropout rate to be used.\n",
    "        num_classes : int\n",
    "          The number of classes in a dataset.\n",
    "        test_data : numpy.ndarray\n",
    "          The NumPy array testing dataset.\n",
    "        test_size : int\n",
    "          The size of `test_data`.\n",
    "        checkpoint_path : str\n",
    "          The path where to save the trained model.\n",
    "        result_path : str\n",
    "          The path where to save the actual and predicted classes array.\n",
    "        \"\"\"\n",
    "\n",
    "        # create initial RNN state array, filled with zeros\n",
    "        initial_state = np.zeros([batch_size, cell_size])\n",
    "\n",
    "        # cast the array to float32\n",
    "        initial_state = initial_state.astype(np.float32)\n",
    "\n",
    "        # variables initializer\n",
    "        init_op = tf.group(\n",
    "            tf.global_variables_initializer(), tf.local_variables_initializer()\n",
    "        )\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_op)\n",
    "\n",
    "            # get the checkpoint file\n",
    "            checkpoint = tf.train.get_checkpoint_state(checkpoint_path)\n",
    "\n",
    "            if checkpoint and checkpoint.model_checkpoint_path:\n",
    "                # if checkpoint file exists, load the saved meta graph\n",
    "                saver = tf.train.import_meta_graph(\n",
    "                    checkpoint.model_checkpoint_path + \".meta\"\n",
    "                )\n",
    "                # and restore previously saved variables\n",
    "                saver.restore(sess, tf.train.latest_checkpoint(checkpoint_path))\n",
    "                print(\n",
    "                    \"Loaded model from {}\".format(\n",
    "                        tf.train.latest_checkpoint(checkpoint_path)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                for step in range(test_size // batch_size):\n",
    "\n",
    "                    offset = (step * batch_size) % test_size\n",
    "                    test_features_batch = test_data[0][offset : (offset + batch_size)]\n",
    "                    test_labels_batch = test_data[1][offset : (offset + batch_size)]\n",
    "\n",
    "                    # one-hot encode labels according to NUM_CLASSES\n",
    "                    label_onehot = tf.one_hot(test_labels_batch, num_classes, 1.0, -1.0)\n",
    "                    y_onehot = sess.run(label_onehot)\n",
    "\n",
    "                    # dictionary for input values for the tensors\n",
    "                    feed_dict = {\n",
    "                        \"input/x_input:0\": test_features_batch,\n",
    "                        \"initial_state:0\": initial_state,\n",
    "                        \"p_keep:0\": dropout_rate,\n",
    "                    }\n",
    "\n",
    "                    # get the tensor for classification\n",
    "                    prediction_tensor = sess.graph.get_tensor_by_name(\n",
    "                        \"accuracy/prediction:0\"\n",
    "                    )\n",
    "                    predictions = sess.run(prediction_tensor, feed_dict=feed_dict)\n",
    "\n",
    "                    # add key, value pair for labels\n",
    "                    feed_dict[\"input/y_input:0\"] = test_labels_batch\n",
    "\n",
    "                    # get the tensor for calculating the classification accuracy\n",
    "                    accuracy_tensor = sess.graph.get_tensor_by_name(\n",
    "                        \"accuracy/accuracy/Mean:0\"\n",
    "                    )\n",
    "                    accuracy = sess.run(accuracy_tensor, feed_dict=feed_dict)\n",
    "\n",
    "                    if step % 100 == 0 and step > 0:\n",
    "                        print(\"step [{}] test -- accuracy : {}\".format(step, accuracy))\n",
    "\n",
    "                    GruSvm.save_labels(\n",
    "                        predictions=predictions,\n",
    "                        actual=y_onehot,\n",
    "                        result_path=result_path,\n",
    "                        phase=\"testing\",\n",
    "                        step=step,\n",
    "                    )\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"KeyboardInterrupt at step {}\".format(step))\n",
    "            finally:\n",
    "                print(\"Done classifying at step {}\".format(step))\n",
    "\n",
    "    @staticmethod\n",
    "    def variable_summaries(var):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.scalar(\"mean\", mean)\n",
    "            with tf.name_scope(\"stddev\"):\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar(\"stddev\", stddev)\n",
    "            tf.summary.scalar(\"max\", tf.reduce_max(var))\n",
    "            tf.summary.scalar(\"min\", tf.reduce_min(var))\n",
    "            tf.summary.histogram(\"histogram\", var)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_labels(predictions, actual, result_path, step, phase):\n",
    "        \"\"\"Saves the actual and predicted labels to a NPY file\n",
    "\n",
    "        Parameter\n",
    "        ---------\n",
    "        predictions : numpy.ndarray\n",
    "          The NumPy array containing the predicted labels.\n",
    "        actual : numpy.ndarray\n",
    "          The NumPy array containing the actual labels.\n",
    "        result_path : str\n",
    "          The path where to save the concatenated actual and predicted labels.\n",
    "        step : int\n",
    "          The time step for the NumPy arrays.\n",
    "        phase : str\n",
    "          The phase for which the predictions is, i.e. training/validation/testing.\n",
    "        \"\"\"\n",
    "\n",
    "        # Concatenate the predicted and actual labels\n",
    "        labels = np.concatenate((predictions, actual), axis=1)\n",
    "\n",
    "        # Create the result_path directory if it does not exist\n",
    "        if not os.path.exists(path=result_path):\n",
    "            os.mkdir(path=result_path)\n",
    "\n",
    "        # Save the labels array to NPY file\n",
    "        np.save(\n",
    "            file=os.path.join(result_path, \"{}-gru_svm-{}.npy\".format(phase, step)),\n",
    "            arr=labels,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "__version__ = \"0.1.1\"\n",
    "__author__ = \"Abien Fred Agarap\"\n",
    "\n",
    "import argparse\n",
    "from utils import data\n",
    "# hyper-parameters for the model\n",
    "BATCH_SIZE = 256\n",
    "CELL_SIZE = 256\n",
    "DROPOUT_P_KEEP = 0.85\n",
    "HM_EPOCHS = 1\n",
    "LEARNING_RATE = 1e-5\n",
    "N_CLASSES = 2\n",
    "SEQUENCE_LENGTH = 21\n",
    "SVM_C = 0.5\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"GRU+SVM for Intrusion Detection\")\n",
    "    group = parser.add_argument_group(\"Arguments\")\n",
    "    group.add_argument(\n",
    "        \"-o\",\n",
    "        \"--operation\",\n",
    "        required=True,\n",
    "        type=str,\n",
    "        help='the operation to perform: \"train\" or \"test\"',\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"-t\",\n",
    "        \"--train_dataset\",\n",
    "        required=False,\n",
    "        type=str,\n",
    "        help=\"the NumPy array training dataset (*.npy) to be used\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"-v\",\n",
    "        \"--validation_dataset\",\n",
    "        required=True,\n",
    "        type=str,\n",
    "        help=\"the NumPy array validation dataset (*.npy) to be used\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"-c\",\n",
    "        \"--checkpoint_path\",\n",
    "        required=True,\n",
    "        type=str,\n",
    "        help=\"path where to save the trained model\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"-l\",\n",
    "        \"--log_path\",\n",
    "        required=False,\n",
    "        type=str,\n",
    "        help=\"path where to save the TensorBoard logs\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"-m\",\n",
    "        \"--model_name\",\n",
    "        required=False,\n",
    "        type=str,\n",
    "        help=\"filename for the trained model\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"-r\",\n",
    "        \"--result_path\",\n",
    "        required=True,\n",
    "        type=str,\n",
    "        help=\"path where to save the actual and predicted labels\",\n",
    "    )\n",
    "    arguments = parser.parse_args()\n",
    "    return arguments\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "\n",
    "    if argv.operation == \"train\":\n",
    "        # get the train data\n",
    "        # features: train_data[0], labels: train_data[1]\n",
    "        train_features, train_labels = data.load_data(dataset=argv.train_dataset)\n",
    "\n",
    "        # get the validation data\n",
    "        # features: validation_data[0], labels: validation_data[1]\n",
    "        validation_features, validation_labels = data.load_data(\n",
    "            dataset=argv.validation_dataset\n",
    "        )\n",
    "\n",
    "        # get the size of the dataset for slicing\n",
    "        train_size = train_features.shape[0]\n",
    "        validation_size = validation_features.shape[0]\n",
    "\n",
    "        # slice the dataset to be exact as per the batch size\n",
    "        # e.g. train_size = 1898322, batch_size = 256\n",
    "        # [:1898322-(1898322%256)] = [:1898240]\n",
    "        # 1898322 // 256 = 7415; 7415 * 256 = 1898240\n",
    "        train_features = train_features[: train_size - (train_size % BATCH_SIZE)]\n",
    "        train_labels = train_labels[: train_size - (train_size % BATCH_SIZE)]\n",
    "\n",
    "        # modify the size of the dataset to be passed on model.train()\n",
    "        train_size = train_features.shape[0]\n",
    "\n",
    "        # slice the dataset to be exact as per the batch size\n",
    "        validation_features = validation_features[\n",
    "            : validation_size - (validation_size % BATCH_SIZE)\n",
    "        ]\n",
    "        validation_labels = validation_labels[\n",
    "            : validation_size - (validation_size % BATCH_SIZE)\n",
    "        ]\n",
    "\n",
    "        # modify the size of the dataset to be passed on model.train()\n",
    "        validation_size = validation_features.shape[0]\n",
    "\n",
    "        # instantiate the model\n",
    "        model = GruSvm(\n",
    "            alpha=LEARNING_RATE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            cell_size=CELL_SIZE,\n",
    "            dropout_rate=DROPOUT_P_KEEP,\n",
    "            num_classes=N_CLASSES,\n",
    "            sequence_length=SEQUENCE_LENGTH,\n",
    "            svm_c=SVM_C,\n",
    "        )\n",
    "\n",
    "        # train the model\n",
    "        model.train(\n",
    "            checkpoint_path=argv.checkpoint_path,\n",
    "            log_path=argv.log_path,\n",
    "            model_name=argv.model_name,\n",
    "            epochs=HM_EPOCHS,\n",
    "            train_data=[train_features, train_labels],\n",
    "            train_size=train_size,\n",
    "            validation_data=[validation_features, validation_labels],\n",
    "            validation_size=validation_size,\n",
    "            result_path=argv.result_path,\n",
    "        )\n",
    "    elif argv.operation == \"test\":\n",
    "        test_features, test_labels = data.load_data(dataset=argv.validation_dataset)\n",
    "\n",
    "        test_size = test_features.shape[0]\n",
    "\n",
    "        test_features = test_features[: test_size - (test_size % BATCH_SIZE)]\n",
    "        test_labels = test_labels[: test_size - (test_size % BATCH_SIZE)]\n",
    "\n",
    "        test_size = test_features.shape[0]\n",
    "\n",
    "        GruSvm.predict(\n",
    "            batch_size=BATCH_SIZE,\n",
    "            cell_size=CELL_SIZE,\n",
    "            dropout_rate=DROPOUT_P_KEEP,\n",
    "            num_classes=N_CLASSES,\n",
    "            test_data=[test_features, test_labels],\n",
    "            test_size=test_size,\n",
    "            checkpoint_path=argv.checkpoint_path,\n",
    "            result_path=argv.result_path,\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: The plistlib.Dict class is deprecated, use builtin dict instead\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from plistlib import Dict\n",
    "def dict2obj(dictObj):\n",
    "    if not isinstance(dictObj, dict):\n",
    "        return dictObj\n",
    "    d = Dict()\n",
    "    for k, v in dictObj.items():\n",
    "        d[k] = dict2obj(v)\n",
    "    return d\n",
    "argstrain= {'operation':'train','train_dataset':'dataset/train/train_data.npy',\n",
    "       'validation_dataset':'dataset/test/test_data.npy',\n",
    "       'checkpoint_path':'models/checkpoint/gru_svm','model_name':'gru_svm.ckpt',\n",
    "       'log_path':'models/logs/gru_svm',\n",
    "        'result_path':'results/gru_svm'}\n",
    "args=dict2obj(argstrain)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:75: DeprecationWarning: Attribute access from plist dicts is deprecated, use d[key] notation instead\n",
      "d:\\anaconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:78: DeprecationWarning: Attribute access from plist dicts is deprecated, use d[key] notation instead\n",
      "d:\\anaconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:83: DeprecationWarning: Attribute access from plist dicts is deprecated, use d[key] notation instead\n",
      "d:\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:98: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "d:\\anaconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:124: DeprecationWarning: Attribute access from plist dicts is deprecated, use d[key] notation instead\n",
      "d:\\anaconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:125: DeprecationWarning: Attribute access from plist dicts is deprecated, use d[key] notation instead\n",
      "d:\\anaconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:126: DeprecationWarning: Attribute access from plist dicts is deprecated, use d[key] notation instead\n",
      "d:\\anaconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:132: DeprecationWarning: Attribute access from plist dicts is deprecated, use d[key] notation instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<log> Building Graph...</log>\n",
      "INFO:tensorflow:Restoring parameters from models/checkpoint/gru_svm\\gru_svm.ckpt-12900\n",
      "step [0] train -- loss : 177.81179809570312, accuracy : 0.76171875\n",
      "step [100] train -- loss : 48.817962646484375, accuracy : 0.95703125\n",
      "step [200] train -- loss : 49.84575271606445, accuracy : 0.9453125\n",
      "step [300] train -- loss : 56.65222930908203, accuracy : 0.93359375\n",
      "step [400] train -- loss : 144.38323974609375, accuracy : 0.8046875\n",
      "step [500] train -- loss : 87.08695220947266, accuracy : 0.8828125\n",
      "step [600] train -- loss : 181.8019561767578, accuracy : 0.73828125\n",
      "step [700] train -- loss : 104.31031036376953, accuracy : 0.87109375\n",
      "step [800] train -- loss : 136.45654296875, accuracy : 0.79296875\n",
      "step [900] train -- loss : 124.87150573730469, accuracy : 0.83203125\n",
      "step [1000] train -- loss : 154.5930938720703, accuracy : 0.765625\n",
      "step [1100] train -- loss : 82.90081787109375, accuracy : 0.90625\n",
      "step [1200] train -- loss : 100.6135025024414, accuracy : 0.87890625\n",
      "step [1300] train -- loss : 91.99685668945312, accuracy : 0.88671875\n",
      "step [1400] train -- loss : 87.06642150878906, accuracy : 0.86328125\n",
      "step [1500] train -- loss : 114.73143005371094, accuracy : 0.8359375\n",
      "step [1600] train -- loss : 97.04052734375, accuracy : 0.875\n",
      "step [1700] train -- loss : 133.44691467285156, accuracy : 0.828125\n",
      "step [1800] train -- loss : 174.02090454101562, accuracy : 0.765625\n",
      "step [1900] train -- loss : 107.344482421875, accuracy : 0.8671875\n",
      "step [2000] train -- loss : 96.0007553100586, accuracy : 0.85546875\n",
      "step [2100] train -- loss : 150.73179626464844, accuracy : 0.796875\n",
      "step [2200] train -- loss : 95.36182403564453, accuracy : 0.86328125\n",
      "step [2300] train -- loss : 101.49301147460938, accuracy : 0.87890625\n",
      "step [2400] train -- loss : 48.50977325439453, accuracy : 0.9375\n",
      "step [2500] train -- loss : 112.0766830444336, accuracy : 0.81640625\n",
      "step [2600] train -- loss : 86.31466674804688, accuracy : 0.91015625\n",
      "step [2700] train -- loss : 127.32305908203125, accuracy : 0.8046875\n",
      "step [2800] train -- loss : 125.80841827392578, accuracy : 0.81640625\n",
      "step [2900] train -- loss : 91.57572174072266, accuracy : 0.87890625\n",
      "step [3000] train -- loss : 130.36537170410156, accuracy : 0.85546875\n",
      "step [3100] train -- loss : 82.79180908203125, accuracy : 0.87109375\n",
      "step [3200] train -- loss : 120.61540985107422, accuracy : 0.8203125\n",
      "step [3300] train -- loss : 92.99482727050781, accuracy : 0.8515625\n",
      "step [3400] train -- loss : 70.80219268798828, accuracy : 0.8984375\n",
      "step [3500] train -- loss : 75.92086029052734, accuracy : 0.90234375\n",
      "step [3600] train -- loss : 72.85093688964844, accuracy : 0.90234375\n",
      "step [3700] train -- loss : 90.75818634033203, accuracy : 0.87109375\n",
      "step [3800] train -- loss : 94.18780517578125, accuracy : 0.89453125\n",
      "step [3900] train -- loss : 95.25664520263672, accuracy : 0.88671875\n",
      "step [4000] train -- loss : 88.29547882080078, accuracy : 0.87109375\n",
      "step [4100] train -- loss : 105.74120330810547, accuracy : 0.85546875\n",
      "step [4200] train -- loss : 162.2609100341797, accuracy : 0.79296875\n",
      "step [4300] train -- loss : 105.03331756591797, accuracy : 0.85546875\n",
      "step [4400] train -- loss : 168.3857421875, accuracy : 0.73828125\n",
      "step [4500] train -- loss : 148.00289916992188, accuracy : 0.80078125\n",
      "step [4600] train -- loss : 47.47825622558594, accuracy : 0.94921875\n",
      "step [4700] train -- loss : 129.8563995361328, accuracy : 0.80078125\n",
      "step [4800] train -- loss : 101.29736328125, accuracy : 0.8671875\n",
      "step [4900] train -- loss : 70.14989471435547, accuracy : 0.91015625\n",
      "step [5000] train -- loss : 107.22409057617188, accuracy : 0.83203125\n",
      "step [5100] train -- loss : 177.2659149169922, accuracy : 0.7265625\n",
      "step [5200] train -- loss : 159.90237426757812, accuracy : 0.76953125\n",
      "step [5300] train -- loss : 48.73881149291992, accuracy : 0.9375\n",
      "step [5400] train -- loss : 81.5623550415039, accuracy : 0.8984375\n",
      "step [5500] train -- loss : 154.44915771484375, accuracy : 0.79296875\n",
      "step [5600] train -- loss : 120.94841766357422, accuracy : 0.8671875\n",
      "step [5700] train -- loss : 137.3813018798828, accuracy : 0.81640625\n",
      "step [5800] train -- loss : 188.7654266357422, accuracy : 0.7109375\n",
      "step [5900] train -- loss : 140.42652893066406, accuracy : 0.85546875\n",
      "step [6000] train -- loss : 130.09934997558594, accuracy : 0.84375\n",
      "step [6100] train -- loss : 112.18502044677734, accuracy : 0.8515625\n",
      "step [6200] train -- loss : 114.80230712890625, accuracy : 0.8359375\n",
      "step [6300] train -- loss : 74.77886962890625, accuracy : 0.89453125\n",
      "step [6400] train -- loss : 118.6807632446289, accuracy : 0.8515625\n",
      "step [6500] train -- loss : 69.87052917480469, accuracy : 0.92578125\n",
      "step [6600] train -- loss : 69.0800552368164, accuracy : 0.9296875\n",
      "step [6700] train -- loss : 66.64717102050781, accuracy : 0.92578125\n",
      "step [6800] train -- loss : 232.06478881835938, accuracy : 0.66796875\n",
      "step [6900] train -- loss : 229.7908172607422, accuracy : 0.60546875\n",
      "step [7000] train -- loss : 132.1740264892578, accuracy : 0.8359375\n",
      "step [7100] train -- loss : 62.484188079833984, accuracy : 0.9140625\n",
      "step [7200] train -- loss : 90.19786834716797, accuracy : 0.88671875\n",
      "step [7300] train -- loss : 151.4584197998047, accuracy : 0.79296875\n",
      "step [7400] train -- loss : 113.09739685058594, accuracy : 0.8515625\n",
      "EOF -- Training done at step 7414\n",
      "step [100] validation -- loss : 150.75949096679688, accuracy : 0.81640625\n",
      "step [200] validation -- loss : 111.35363006591797, accuracy : 0.81640625\n",
      "step [300] validation -- loss : 136.55953979492188, accuracy : 0.84375\n",
      "step [400] validation -- loss : 175.58074951171875, accuracy : 0.76171875\n",
      "step [500] validation -- loss : 179.6343231201172, accuracy : 0.8203125\n",
      "step [600] validation -- loss : 173.44993591308594, accuracy : 0.79296875\n",
      "step [700] validation -- loss : 103.87996673583984, accuracy : 0.890625\n",
      "step [800] validation -- loss : 130.75743103027344, accuracy : 0.85546875\n",
      "step [900] validation -- loss : 71.70336151123047, accuracy : 0.94921875\n",
      "step [1000] validation -- loss : 153.14039611816406, accuracy : 0.8046875\n",
      "step [1100] validation -- loss : 94.17206573486328, accuracy : 0.87890625\n",
      "step [1200] validation -- loss : 192.1533966064453, accuracy : 0.77734375\n",
      "step [1300] validation -- loss : 109.80712127685547, accuracy : 0.87890625\n",
      "step [1400] validation -- loss : 143.79588317871094, accuracy : 0.8125\n",
      "step [1500] validation -- loss : 192.49261474609375, accuracy : 0.70703125\n",
      "step [1600] validation -- loss : 124.00154113769531, accuracy : 0.8828125\n",
      "EOF -- Testing done at step 1642\n"
     ]
    }
   ],
   "source": [
    "main(argv=args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: The plistlib.Dict class is deprecated, use builtin dict instead\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "argstest= {'operation':'test','train_dataset':'dataset/train/train_data.npy',\n",
    "       'validation_dataset':'dataset/test/test_data.npy',\n",
    "       'checkpoint_path':'models/checkpoint/gru_svm',\n",
    "        'result_path':'results/gru_svm'}\n",
    "args=dict2obj(argstest)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:75: DeprecationWarning: Attribute access from plist dicts is deprecated, use d[key] notation instead\n",
      "d:\\anaconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:134: DeprecationWarning: Attribute access from plist dicts is deprecated, use d[key] notation instead\n",
      "d:\\anaconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:135: DeprecationWarning: Attribute access from plist dicts is deprecated, use d[key] notation instead\n",
      "d:\\anaconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:151: DeprecationWarning: Attribute access from plist dicts is deprecated, use d[key] notation instead\n",
      "d:\\anaconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:152: DeprecationWarning: Attribute access from plist dicts is deprecated, use d[key] notation instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/checkpoint/gru_svm\\gru_svm.ckpt-7400\n",
      "Loaded model from models/checkpoint/gru_svm\\gru_svm.ckpt-7400\n",
      "step [100] test -- accuracy : 0.78125\n",
      "step [200] test -- accuracy : 0.8046875\n",
      "step [300] test -- accuracy : 0.79296875\n",
      "step [400] test -- accuracy : 0.71484375\n",
      "step [500] test -- accuracy : 0.78515625\n",
      "step [600] test -- accuracy : 0.81640625\n",
      "step [700] test -- accuracy : 0.87890625\n",
      "step [800] test -- accuracy : 0.85546875\n",
      "step [900] test -- accuracy : 0.90625\n",
      "step [1000] test -- accuracy : 0.77734375\n",
      "step [1100] test -- accuracy : 0.85546875\n",
      "step [1200] test -- accuracy : 0.73828125\n",
      "step [1300] test -- accuracy : 0.875\n",
      "step [1400] test -- accuracy : 0.76953125\n",
      "step [1500] test -- accuracy : 0.6953125\n",
      "step [1600] test -- accuracy : 0.84765625\n",
      "Done classifying at step 1642\n"
     ]
    }
   ],
   "source": [
    "main(argv=args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}