{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NFWTUL3GCHsG"
   },
   "outputs": [],
   "source": [
    "# Load the top modules that are used in multiple places\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rrnTamUHDzxI"
   },
   "outputs": [],
   "source": [
    "# Some global variables to drive the script\n",
    "# data_url is the location of the data\n",
    "# Data is not loaded from a local file\n",
    "# Data is loaded from a prepocessed dataset\n",
    "data_url=\"MachineLearningCVE/processed/bal-cicids2017.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bYEbuZO0MHyf"
   },
   "outputs": [],
   "source": [
    "# label names (YY) in the data and their\n",
    "# mapping to numerical values\n",
    "label_map = {\n",
    " 'BENIGN' : 0,\n",
    " 'FTP-Patator' : 1,\n",
    " 'SSH-Patator' : 2,\n",
    " 'DoS slowloris' : 3,\n",
    " 'DoS Slowhttptest': 4,\n",
    " 'DoS Hulk' : 5,\n",
    " 'DoS GoldenEye' : 6,\n",
    " 'Heartbleed' : 7,\n",
    " 'Web Attack � Brute Force' : 8,\n",
    " 'Web Attack � XSS' : 8,\n",
    " 'Web Attack � Sql Injection' : 8,\n",
    " 'Infiltration' : 9,\n",
    " 'Bot' : 10,\n",
    " 'PortScan' : 11,\n",
    " 'DDoS' : 12,\n",
    "}\n",
    "\n",
    "num_ids_features = 76\n",
    "num_ids_classes = 13\n",
    "ids_classes = [ 'BENIGN', 'FTP-Patator', 'SSH-Patator', 'DoS slowloris', 'DoS Slowhttptest', 'DoS Hulk', 'DoS GoldenEye', 'Heartbleed', 'Brute Force', 'XSS', 'Sql Injection', 'Infiltration', 'Bot', 'PortScan', 'DDoS',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "usYPD_l1Bimz"
   },
   "outputs": [],
   "source": [
    "# Utility functions used by classifiers\n",
    "# In particular to load and split data and output results\n",
    "def ids_load_df_from_csv():\n",
    "    \"\"\"\n",
    "    Load dataframe from csv file\n",
    "    Input:\n",
    "        None\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(data_url)\n",
    "\n",
    "    print (\"load Dataframe shape\", df.shape)\n",
    "\n",
    "    return df\n",
    "\n",
    "def ids_split(df):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        Dataframe that has columns of covariates followed by a column of labels\n",
    "    Returns:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test as numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    numcols = len(df.columns)\n",
    "    print(\"df.shape\", df.shape)\n",
    "\n",
    "    X = df.iloc[:, 0:numcols-1]\n",
    "    y = df.loc[:, 'YY']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "    print (\"X_train.shape\", X_train.shape, \"y_train.shape\", y_train.shape)\n",
    "    print (\"X_val.shape\", X_val.shape, \"y_val.shape\", y_val.shape)\n",
    "    print (\"X_test.shape\", X_test.shape, \"y_test.shape\", y_test.shape)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    y_train = y_train.values\n",
    "    y_val = y_val.values\n",
    "    y_test = y_test.values\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "def ids_accuracy (y_actual, y_pred):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        Numpy arrays with actual and predicted labels\n",
    "    Returns:\n",
    "        multiclass accuracy and f1 scores; two class accuracy and f1 scores\n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    # modify labels to get results for two class classification\n",
    "    y_actual_2 = (y_actual > 0).astype(int)\n",
    "    y_pred_2 = (y_pred > 0).astype(int)\n",
    "\n",
    "    acc = accuracy_score (y_actual, y_pred)\n",
    "    f1 = f1_score(y_actual, y_pred, average='macro')\n",
    "    acc_2 = accuracy_score (y_actual_2, y_pred_2)\n",
    "    f1_2 = f1_score(y_actual_2, y_pred_2)\n",
    "    \n",
    "    return acc, f1, acc_2, f1_2\n",
    "    \n",
    "\n",
    "def ids_metrics(y_actual, y_pred):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        Numpy arrays with actual and predicted labels\n",
    "    Returns:\n",
    "        None\n",
    "    Print: various classification metrics\n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm = confusion_matrix (y_actual, y_pred)\n",
    "    print (cm)\n",
    "\n",
    "    acc, f1, acc_2, f1_2 = ids_accuracy (y_actual, y_pred)\n",
    "    print('Classifier accuracy : {:.4f}'.format(acc), 'F1 score: {:.4f}'.format(f1))\n",
    "    print('Two class classifier accuracy : {:.4f}'.format(acc_2), 'F1 score: {:.4f}'.format(f1_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BsSD3PeyoH3b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load Dataframe shape (136000, 77)\n",
      "df.shape (136000, 77)\n",
      "X_train.shape (98260, 76) y_train.shape (98260,)\n",
      "X_val.shape (17340, 76) y_val.shape (17340,)\n",
      "X_test.shape (20400, 76) y_test.shape (20400,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "ids_input (InputLayer)       (None, 76)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 76)                5852      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 76)                5852      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 13)                1001      \n",
      "=================================================================\n",
      "Total params: 12,705\n",
      "Trainable params: 12,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 98260 samples, validate on 17340 samples\n",
      "Epoch 1/3\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.004000000189989805.\n",
      " - 11s - loss: 0.2073 - acc: 0.9372 - val_loss: 0.1549 - val_acc: 0.9456\n",
      "Epoch 2/3\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.003994673958045744.\n",
      " - 10s - loss: 0.1196 - acc: 0.9625 - val_loss: 0.1198 - val_acc: 0.9630\n",
      "Epoch 3/3\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.003984049823323384.\n",
      " - 11s - loss: 0.1121 - acc: 0.9653 - val_loss: 0.1069 - val_acc: 0.9668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d68276bba8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FCNN model developed using the deeplizard tutorial\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import TensorBoard\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "# For reproducible results\n",
    "import random as rn\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "np.random.seed(42)\n",
    "rn.seed(42)\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "df = ids_load_df_from_csv ()\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = ids_split(df)\n",
    "\n",
    "# To use sparse_categorical_crossentropy as the loss function\n",
    "#   use softmax as the activation function in the output layer\n",
    "inputs = keras.Input(shape=(num_ids_features,), name=\"ids_input\")\n",
    "hl1 = Dense(num_ids_features, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "hl2 = Dense(num_ids_features, activation=\"relu\", name=\"dense_2\")(hl1)\n",
    "outputs = Dense(num_ids_classes, activation=\"softmax\", name=\"output\")(hl2)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "initial_learning_rate = 0.004\n",
    "epochs = 3\n",
    "decay = initial_learning_rate / epochs\n",
    "\n",
    "# learning scheduler 1\n",
    "def lr_time_based_decay(epoch, lr):\n",
    "    return lr * 1 / (1 + decay * epoch)\n",
    "\n",
    "# learning scheuler 2\n",
    "def lr_step_decay(epoch, lr):\n",
    "    drop_rate = 0.75\n",
    "    epochs_drop = 2\n",
    "    return initial_learning_rate * math.pow(drop_rate, math.floor(epoch/epochs_drop))\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tb_cbk = TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(lr=initial_learning_rate),\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x=X_train, \n",
    "    y=y_train, \n",
    "    batch_size=64, \n",
    "    shuffle=True,\n",
    "    epochs=epochs, \n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[LearningRateScheduler(lr_time_based_decay, verbose=1), tb_cbk],\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "itR5eQDXIyLk"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'notebook'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-8-2dcc045af1ba>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mget_ipython\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrun_line_magic\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'reload_ext'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'tensorboard'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorboard\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mnotebook\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mnotebook\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# View open TensorBoard instances\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;31m# Control TensorBoard display. If no port is provided,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'notebook'"
     ]
    }
   ],
   "source": [
    "'''from tensorboard import notebook\n",
    "notebook.list() # View open TensorBoard instances\n",
    "\n",
    "# Control TensorBoard display. If no port is provided,\n",
    "# the most recently launched TensorBoard is used\n",
    "# notebook.display(port=6006, height=1000)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs\n",
    "notebook.display(port=6006, height=1000)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xYH7gu-tNshX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4842   15    6    1    3  100   10    0   24    9   85   82    1]\n",
      " [   0 1034    1   11    0    1    0    0    0    0    0    0    0]\n",
      " [   5    2 1030    0    0    1    3    0    0    0    0    0    0]\n",
      " [   0    0    2  983   14    0    1    0    0    0    0    0    0]\n",
      " [   1    0    2    3  973    0    4    0    0    0    0    0    0]\n",
      " [   6    0    0    0    0 1043    0    0    1    0    0    0    2]\n",
      " [   9    0    0    0    2    1 1025    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0 1039    0    0    0    0    0]\n",
      " [   1    0   76    0    0   12    7    0  928    0    0    0    0]\n",
      " [  28    0    0    0    0    0    0    0    0 1009    0    0    0]\n",
      " [  17    0    0    0    0    0    0    0    0    0  957    0    0]\n",
      " [   6    0    0    0    0    1    0    0    0    0    0  995    0]\n",
      " [  19    0    0    0    0    0    0    0    0    0    0    0  907]]\n",
      "Classifier accuracy : 0.9668 F1 score: 0.9699\n",
      "Two class classifier accuracy : 0.9753 F1 score: 0.9826\n"
     ]
    }
   ],
   "source": [
    "# prediction step and metrics similar to logistic and knn classifiers\n",
    "predictions = model.predict(\n",
    "    x=X_val,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    ") \n",
    "\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "ids_metrics(y_val, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNctWZ2L/yVZy9tEokgnbuh",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "ids_keras_tf.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PyCharm (数据挖掘)",
   "language": "python",
   "name": "pycharm-43d6ef98"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}