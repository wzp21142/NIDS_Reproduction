{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "from pyspark import SQLContext, SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and labels\n",
    "x=np.load('Bx.npy')\n",
    "y=np.load('By.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "sc = SparkContext(conf = conf)\n",
    "spark = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the rank of each feature\n",
    "R=[]\n",
    "for h in range(x.shape[1]):\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=np.unique(y).shape[0], n_init=10)\n",
    "    ff=kmeans.fit_predict(x[:,h].reshape(-1,1))\n",
    "    r=metrics.homogeneity_score(y,ff) #Use the homogeneity score as a rank of the feature\n",
    "    R.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arrange feature accroding to thier ranks\n",
    "Rnk=np.argsort(np.array(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate the cross-validation splitter\n",
    "kfolds=StratifiedKFold(n_splits=5,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Per each set of ranks, use cross-validation to calculate accuracy.\n",
    "smr=[]\n",
    "for j in range(Rnk.shape[0]):\n",
    "    fd=x[:,Rnk[j:]]\n",
    "    pp=0\n",
    "    lpa=np.zeros((0,2))\n",
    "    for train,test in kfolds.split(fd,y):\n",
    "        dff = map(lambda x: (int(float(x[-1])), Vectors.dense(x[:-1])),np.hstack((fd[train],y[train].reshape(-1,1))))\n",
    "        TrD = spark.createDataFrame(dff,schema=[\"label\", \"features\"])\n",
    "        dff = map(lambda x: (int(float(x[-1])), Vectors.dense(x[:-1])),np.hstack((fd[test],y[test].reshape(-1,1))))\n",
    "        TsD = spark.createDataFrame(dff,schema=[\"label\", \"features\"])\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128,input_dim=fd.shape[1],activation='relu',use_bias=True))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(64,activation='relu',use_bias=True))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(32,activation='relu',use_bias=True))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(1,activation='sigmoid',use_bias=True)) #The number of neurons is equal to the number of classes\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        trainer = SingleTrainer(keras_model=model, worker_optimizer='adam', loss='binary_crossentropy', num_epoch=1000)\n",
    "        trained_model = trainer.train(TrD)\n",
    "        st = time.time()\n",
    "        predictor = ModelPredictor(keras_model=trained_model)\n",
    "        et=time.time()-st\n",
    "        ff=predictor.predict(TsD)\n",
    "        ts=np.array(map(lambda x: x[0],ff.select('prediction').collect())).reshape(-1,1)\n",
    "        pp=pp+metrics.accuracy_score(y[test].reshape(-1,1),(ts>=0.5).astype(int))\n",
    "        lpa=np.vstack((lpa,np.hstack((y[test].reshape(-1,1),ts))))\n",
    "    pp=pp/kfolds.n_splits\n",
    "    np.savetxt('F%d.csv'%j,lpa,delimiter=',')\n",
    "    smr.append([j, pp, et*1000000/x.shape[0]]) #Calculate the time required to predict a label per each object in uS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[43.0, 0.991675, 1.433292376],\n",
       " [42.0, 0.991898, 1.353315116],\n",
       " [41.0, 0.991903, 1.35947327],\n",
       " [40.0, 0.991466, 1.359475238],\n",
       " [39.0, 0.991416, 1.353318265],\n",
       " [38.0, 0.991899, 1.377923716],\n",
       " [37.0, 0.991851, 1.375533602],\n",
       " [36.0, 0.991544, 1.322565291],\n",
       " [35.0, 0.991434, 1.414835237],\n",
       " [34.0, 0.991404, 1.345726674],\n",
       " [33.0, 0.991377, 1.33066711],\n",
       " [32.0, 0.99169, 1.353318265],\n",
       " [31.0, 0.991562, 1.356685526],\n",
       " [30.0, 0.991196, 1.304102247],\n",
       " [29.0, 0.991627, 1.316402019],\n",
       " [28.0, 0.991339, 1.377897732],\n",
       " [27.0, 0.991492, 1.353291494],\n",
       " [26.0, 0.991355, 1.341018493],\n",
       " [25.0, 0.991169, 1.334840655],\n",
       " [24.0, 0.991322, 1.43329277],\n",
       " [23.0, 0.99138, 1.291778459],\n",
       " [22.0, 0.991193, 1.297958266],\n",
       " [21.0, 0.991334, 1.316420129],\n",
       " [20.0, 0.990726, 1.285655738],\n",
       " [19.0, 0.990726, 1.310253314],\n",
       " [18.0, 0.990866, 1.297949605],\n",
       " [17.0, 0.990612, 1.285645108],\n",
       " [16.0, 0.99086, 1.310232448],\n",
       " [15.0, 0.990418, 1.273341792],\n",
       " [14.0, 0.990551, 1.261048713],\n",
       " [13.0, 0.989806, 1.304109727],\n",
       " [12.0, 0.989716, 1.291810348],\n",
       " [11.0, 0.989766, 1.27947475],\n",
       " [10.0, 0.989693, 1.291806411],\n",
       " [9.0, 0.989474, 1.273352029],\n",
       " [8.0, 0.989497, 1.267198599],\n",
       " [7.0, 0.989376, 1.273348485],\n",
       " [6.0, 0.989578, 1.427111782],\n",
       " [5.0, 0.989227, 1.285646289],\n",
       " [4.0, 0.988241, 1.396382823],\n",
       " [3.0, 0.98782, 1.3040802],\n",
       " [2.0, 0.986622, 1.322534977]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-8bba76b",
   "language": "python",
   "display_name": "PyCharm (SGM-CNN)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}